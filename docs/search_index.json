[["index.html", "Statistiques avec R ", " Statistiques avec R Bienvenue sur le site du cours SCI 1018! L’entièreté du contenu du cours est disponible en accès libre sur ce site. Les évaluations sont disponibles sur le site web de la TÉLUQ aux étudiantes et aux étudiants inscrits dans ce cours. Avant de débuter le cours, familiarisez-vous avec la structure de ce site en utilisant le menu vertical à gauche. Lisez attentivement la section Présentation avant de vous plonger dans les apprentissages. "],["à-propos-du-cours.html", "À propos du cours", " À propos du cours Présentation Lorem ipsum dolor sit amet, an homero populo has. Ei petentium laboramus aliquando duo, an ius omnesque laboramus, vel in reque dicant impedit. Id magna quaestio vim, eu has possit verear. Nec tation repudiare ea, autem quodsi te vel, democritum definitiones ut quo. Dico posse ea sed, ridens iuvaret duo ea, nibh sonet sapientem vim at. Pourquoi suivre ce cours ? Lorem ipsum dolor sit amet, an homero populo has. Ei petentium laboramus aliquando duo, an ius omnesque laboramus, vel in reque dicant impedit. Id magna quaestio vim, eu has possit verear. Nec tation repudiare ea, autem quodsi te vel, democritum definitiones ut quo. Dico posse ea sed, ridens iuvaret duo ea, nibh sonet sapientem vim at. Objectifs Lorem ipsum dolor sit amet, an homero populo has. Ei petentium laboramus aliquando duo, an ius omnesque laboramus, vel in reque dicant impedit. Id magna quaestio vim, eu has possit verear. Nec tation repudiare ea, autem quodsi te vel, democritum definitiones ut quo. Dico posse ea sed, ridens iuvaret duo ea, nibh sonet sapientem vim at. Feuille de route Lorem ipsum dolor sit amet, an homero populo has. Ei petentium laboramus aliquando duo, an ius omnesque laboramus, vel in reque dicant impedit. Id magna quaestio vim, eu has possit verear. Nec tation repudiare ea, autem quodsi te vel, democritum definitiones ut quo. Dico posse ea sed, ridens iuvaret duo ea, nibh sonet sapientem vim at. Crédits Lorem ipsum dolor sit amet, an homero populo has. Ei petentium laboramus aliquando duo, an ius omnesque laboramus, vel in reque dicant impedit. Id magna quaestio vim, eu has possit verear. Nec tation repudiare ea, autem quodsi te vel, democritum definitiones ut quo. Dico posse ea sed, ridens iuvaret duo ea, nibh sonet sapientem vim at. Contact Élise Filotas Je suis professeure au Département Science et technologie de la TELUQ en écologie quantitative et je suis responsable du cours SCI 1018. Vous pouvez me contacter à l’adresse suivante: elise.filotas@teluq.ca Consultez mon site web pour en savoir plus sur ma recherche. "],["ressources.html", "Ressources", " Ressources Travaux notés Lorem ipsum dolor sit amet, an homero populo has. Ei petentium laboramus aliquando duo, an ius omnesque laboramus, vel in reque dicant impedit. Id magna quaestio vim, eu has possit verear. Nec tation repudiare ea, autem quodsi te vel, democritum definitiones ut quo. Dico posse ea sed, ridens iuvaret duo ea, nibh sonet sapientem vim at. Données Lorem ipsum dolor sit amet, an homero populo has. Ei petentium laboramus aliquando duo, an ius omnesque laboramus, vel in reque dicant impedit. Id magna quaestio vim, eu has possit verear. Nec tation repudiare ea, autem quodsi te vel, democritum definitiones ut quo. Dico posse ea sed, ridens iuvaret duo ea, nibh sonet sapientem vim at. Ressources R Lorem ipsum dolor sit amet, an homero populo has. Ei petentium laboramus aliquando duo, an ius omnesque laboramus, vel in reque dicant impedit. Id magna quaestio vim, eu has possit verear. Nec tation repudiare ea, autem quodsi te vel, democritum definitiones ut quo. Dico posse ea sed, ridens iuvaret duo ea, nibh sonet sapientem vim at. Références Lorem ipsum dolor sit amet, an homero populo has. Ei petentium laboramus aliquando duo, an ius omnesque laboramus, vel in reque dicant impedit. Id magna quaestio vim, eu has possit verear. Nec tation repudiare ea, autem quodsi te vel, democritum definitiones ut quo. Dico posse ea sed, ridens iuvaret duo ea, nibh sonet sapientem vim at. "],["installation-de-r.html", "Installation de R", " Installation de R Figure 0.1: L’allure du site web de R. Où se procurer ? Vous trouverez le site web principal du projet R se trouve à l’adresse (http://cran.r-project.org/) (Figure 0.1). Afin d’accélérer la vitesse de téléchargement et d’éviter de surcharger le site principal, accédez au site miroir situé le plus près de chez-vous (Figure 0.2). Les sites miroirs sont répartis à travers la planète et sont simplement des copies exactes du site principal. Figure 0.2: Certains des sites miroirs de R. Sur le site miroir choisi, sélectionnez le système d’exploitation sur lequel vous désirez une version de R et suivez les instructions du téléchargement. L’installation de R diffère légèrement d’un système d’exploitation à un autre. Installation de R sous MS-Windows À partir de la page principale, cliquez sur le lien pour accéder à la page de téléchargement de la distribution pour MS-Windows, puis cliquez sur le lien pour initier le téléchargement. Le fichier à télécharger pour les systèmes MS-Windows est typiquement du genre R-version.exe (Figure 0.3). Le téléchargement inclut la version 64 bits et la version 32 bits. La version 32 bits convient à la plupart des utilisateurs de systèmes MS-Windows. Figure 0.3: Installation pour systèmes MS-Windows. Après avoir téléchargé le fichier, exécutez-le en double-cliquant sur son icône. L’assistant d’installation du logiciel s’affichera à l’écran et vous guidera étape par étape jusqu’à la fin de l’installation. À noter qu’on peut choisir la langue de l’assistant d’installation de R. De plus, certains des menus et messages d’erreurs de R seront dans la même langue que celle du système d’exploitation. Pour les novices, il suffit d’accepter les options d’installation par défaut à une exception près. Avec les dernières moutures de MS-Windows, telles que MS-Windows 7, il est préférable d’installer le logiciel dans un répertoire autre que Programmes ou Program files car la sécurité accrue de MS-Windows ne permet pas la mise à jour ou l’installation d’extensions de R plus tard. Ainsi, l’installation de R dans un répertoire tel que C:\\Utilisateurs\\votre.nom\\R (où votre.nom correspond à votre nom d’utilisateur lorsque vous démarrez MS-Windows) convient parfaitement. Installation de R sous Mac Pour les utilisateurs de Mac OS X (\\(&gt; 10.6\\)), il suffit de cliquer sur le lien pour accéder à la page de la distribution de R pour systèmes Mac. Initiez le téléchargement en cliquant sur le lien approprié (Figure 0.4). Le fichier à télécharger pour les systèmes Mac est typiquement du genre R-version.pkg. Figure 0.4: Installation pour systèmes Mac. Après avoir téléchargé le fichier, double-cliquez sur son icône pour installer le logiciel et suivez les instructions à l’écran. Installation de R sous GNU/Linux L’installation de R sur les plate-formes GNU/Linux est légèrement plus complexe, mais procure beaucoup plus de flexibilité. Cliquez sur le lien vers les systèmes GNU/Linux et sélectionnez la distribution de GNU/Linux (appelée communément distro) de votre choix (Figure 0.5). Les quatre principales distributions de GNU/Linux sont soutenues: Debian, Red Hat, Suse, et Ubuntu. À noter que R est également compatible avec d’autres distributions qui découlent des quatre distributions de GNU/Linux mentionnées précédemment (p. ex., OpenSUSE, Fedora, Linux Mint). Pour Ubuntu et Debian, par exemple, on peut ajouter l’adresse d’un site miroir de R dans le fichier /etc/apt/sources.list. Figure 0.5: Installation pour systèmes GNU/Linux. Pour la page d’installation, vous trouverez également les informations nécessaires afin d’ajouter la clé d’authentification du site (un des avantages de la sécurité GNU/Linux). Ainsi, à chaque fois qu’une mise à jour de R apparaîtra, elle sera automatiquement installée. Il en va de même pour certaines banques de fonctions appelées communément packages. Afin de compiler les packages, il est aussi recommandé d’installer les outils de compilation de R à l’aide de la commande sudo apt-get install r-base-dev dans le terminal BASH ou celui de votre choix. "],["pour-débuter-avec-r.html", "Pour débuter avec R", " Pour débuter avec R Une des particularités de R est qu’il ne contient qu’une interface graphique minimale, aussi connue sous le nom de GUI (Graphical User Interface, Figure 0.6). Pour GNU/Linux, il n’y a aucune interface graphique avec l’installation de base. On doit communiquer avec R sous forme de commandes. Plusieurs utilisateurs sont déconcertés à la vue d’un curseur clignotant dans un terminal en cette ère des fenêtres graphiques élaborées dans la plupart des logiciels grand public. On peut taper des commandes directement au terminal, mais cette pratique est peu utile si on veut reproduire l’analyse plus tard. Nous suggérons plutôt d’écrire les commandes dans un éditeur afin de pouvoir les sauvegarder, les annoter abondamment de commentaires et de les modifier au besoin. Un fichier de code rédigé dans certains langages informatiques est appelé communément script . Ainsi, nous appellerons script le fichier de code R. Figure 0.6: Interface graphique minimale sous MS-Windows. Pour un projet donné, on peut créer un script R contenant le code qui va de l’importation des données jusqu’aux graphiques qui illustrent les résultats. L’avantage de rédiger un script est non négligeable: on peut reproduire exactement et rapidement l’importation d’un jeu de données, les manipulations, les analyses statistiques et les graphiques tels qu’ils ont été exécutés à l’origine. Les mêmes étapes sont plus laborieuses et enclines à l’erreur avec un logiciel statistique où il faut répéter exactement une séquence de clics de souris. Bien qu’il existe quelques interfaces graphiques pour exécuter des analyses classiques en R telles que R Commander (http://socserv.mcmaster.ca/jfox/Misc/Rcmdr/), nous ne recommandons pas leur utilisation si vous désirez adopter R à long terme. De plus, les interfaces graphiques ne peuvent couvrir qu’un éventail très restreint des analyses possibles avec R. Un script de code R est une feuille de route des progrès réalisés dans un projet d’analyse que l’on peut partager avec des collaborateurs ou recycler au besoin pour des projets ultérieurs. La flexibilité et la transparence qu’apporte l’usage de scripts justifient amplement l’apprentissage du langage R. "],["choisir-son-éditeur.html", "Choisir son éditeur", " Choisir son éditeur Plusieurs éditeurs sont disponibles sous toutes les déclinaisons, du plus complexe au plus simple. Sous sa forme la plus simple, l’éditeur est un logiciel permettant d’écrire du code et de le sauvegarder. D’autres éditeurs, plus sophistiqués, reconnaissent plusieurs langages de programmation comme C, C++, Java, html, R, surlignent certaines commandes du langage et identifient les paires de parenthèses d’ouverture et de fermeture. Les meilleurs éditeurs permettent d’envoyer le code directement à R. Certains éditeurs fonctionnent sur plusieurs plate-formes alors que d’autres sont spécifiques à certains systèmes d’exploitation. À noter que nous recommandons l’utilisation de l’éditeur RStudio, puisqu’il s’intègre parfaitement avec R, est disponible sur plusieurs systèmes d’exploitation (MS-Windows, Mac et Linux) et s’installe facilement. RStudio est un nouveau venu sur la scène des éditeurs intelligents développés spécifiquement pour R. Son allure attrayante plaira à certains utilisateurs. Il reconnaît la syntaxe de R, permet d’envoyer le code à R, divise la fenêtre en différentes sections, notamment une section pour écrire les scripts, une deuxième pour le terminal R, une troisième pour la gestion des jeux de données et des packages, et une dernière pour les graphiques. On peut se procurer cet éditeur à l’adresse suivante: (http://www.rstudio.org). Si RStudio ne vous convient pas, vous trouverez en annexe une liste d’éditeurs intelligents (Intelligent Development Editor, IDE) disponibles gratuitement pour différents systèmes d’exploitation. Optimiser son éditeur Prenez le temps d’explorer quelques unes des fonctionnalités de votre éditeur. Certains éditeurs permettent d’afficher la numérotation des lignes et des colonnes, de modifier la taille des caractères affichés, de créer des touches rapides (hot keys), et d’utiliser des options de recherche et de remplacement. La plupart des éditeurs intelligents reconnaîtront la syntaxe de R si le fichier à ouvrir possède une extension .r ou .R, activant ainsi une multitude d’options spécifiques de R. Si vous créez un nouveau fichier à partir de l’éditeur, il est préférable de lui donner les extensions mentionnées précédemment afin que votre éditeur l’associe à R automatiquement. Il est toujours utile d’ajouter des commentaires aux scripts. En langage R, le symbole # est réservé aux commentaires: dès qu’il est rencontré, R passe à la prochaine ligne. Bien qu’un seul # suffise, par convention, on utilise ## au début d’une ligne, alors qu’on utilise # à la fin d’une ligne. C’est d’ailleurs la convention que nous utiliserons tout au long de la session. ##un commentaire en début de phrase 1 + 1 #c&#39;est un commentaire en fin de ligne ## [1] 2 Vous trouverez des exemples détaillés de la syntaxe de R pour réaliser plusieurs tâches communes, telles que la création de variables, l’importation de fichiers de données, la sélection de sous-ensembles, et le tri d’un jeu de données, dans le document Programmation avec R – notions générales. Divers éditeurs disponibles À titre indicatif, nous présentons ci-dessous divers éditeurs que nous avons essayés pour rédiger du code R. Toutefois, veuillez noter qu’en cas de difficultés avec les éditeurs autres que Rstudio, il sera de votre responsabilité de résoudre les problèmes rencontrés. Éditeurs fonctionnant sous plusieurs systèmes d’exploitation Rstudio est un nouveau venu sur la scène des éditeurs intelligents développés spécifiquement pour R. Son allure attrayante plaira à certains utilisateurs. Il reconnaît la syntaxe de R, permet d’envoyer le code à R, divise la fenêtre en différentes sections, notamment une section pour écrire les scripts, une deuxième pour le terminal R, une troisième pour la gestion des jeux de données et packages, et une dernière pour les graphiques. On peut se procurer cet éditeur à l’adresse suivante: (http://www.rstudio.org). Emacs est un logiciel à multiples usages. Entre autres, c’est un éditeur pour GNU/Linux et MS-Windows qui reconnaît la syntaxe de plusieurs langages. En téléchargeant et en installant le package ESS (Emacs Speaks Statistics), Emacs reconnaît la syntaxe de R et permet d’envoyer le code à R. Vous pouvez vous procurer cet éditeur au (http://www.gnu.org/software/emacs/). Nous recommandons cet éditeur aux utilisateurs de GNU/Linux ou aux programmeurs avertis. JGR est un éditeur créé par les utilisateurs de R pour les utilisateurs de R. Il reconnaît la syntaxe de R et interagit avec R. On peut le télécharger au (http://www.rforge.net/JGR). Éditeurs spécifiques à MS-Windows Tinn-Rreconnaît la syntaxe de R et interagit avec R. Nous recommandons ce logiciel aux utilisateurs de MS-Windows. À noter qu’il fonctionne très bien sous XP, mais, sous Vista, certains problèmes d’interactivité avec R peuvent survenir. On peut se procurer cet éditeur au (https://sourceforge.net/projects/tinn-r/). WinEdt reconnaît la syntaxe de R et peut être téléchargé au (http://www.winedt.com/). Il peut communiquer avec R avec l’aide du package R RWinEdt. Éditeurs spécifiques à Mac Aquamacs est en fait la version de Emacs pour Mac. Tout comme Emacs, il reconnaît la syntaxe de R et permet d’interagir avec R. Nous recommandons cet éditeur aux utilisateurs de Mac. Vous pouvez vous le procurer au (http://aquamacs.org/). TextWrangler reconnaît la syntaxe de R et, à l’aide d’un script, interagit directement avec R. Il est disponible au (http://www.barebones.com/products/textwrangler/). Éditeurs spécifiques à GNU/Linux Kate reconnaît la syntaxe de R et interagit avec R. Il se trouve dans les dépôts de plusieurs distributions de GNU/Linux ou au (http://kate-editor.org/). gedit est un petit éditeur qui reconnaît la syntaxe de R et qui vient souvent par défaut avec les installations de GNU/Linux. On peut le trouver dans les dépôts de plusieurs distributions de GNU/Linux ou au (http://projects.gnome.org/gedit/). "],["programmation-avec-r.html", "Programmation avec R", " Programmation avec R Le contenu théorique de chaque leçon est présenté dans un document comme celui-ci et intègre souvent du code en langage de programmation R. Nous sommes conscient que la convention en français est d’utiliser la virgule pour indiquer la décimale. Toutefois, nous utilisons systématiquement le point pour désigner la décimale dans le texte. Ce choix vient du fait que la syntaxe de R utilise le point comme décimale – à la fois pour la saisie de valeurs numériques et pour l’affichage des sorties d’analyses. Ainsi, l’usage du point uniformisera le texte et nous sommes d’avis que cette décision facilitera sa compréhension. "],["notions-générales.html", "Notions générales", " Notions générales Ce document a pour but d’illustrer les bases nécessaires afin de se familiariser avec R. Pour les personnes qui n’ont jamais rencontré cet environnement, nous recommandons très fortement de répéter les exemples afin de bien comprendre la syntaxe et les subtilités de la programmation en ce qui est devenu la lingua franca des statistiques. Dans ce document, le code R est distingué du reste du texte en utilisant une police monospace comme celle-ci. Aussi, vous remarquerez que le symbole &gt; précède toujours les commandes illustrées dans les exemples du document. Ce symbole indique que les commandes pourraient être saisies dans la console R ou dans l’éditeur. Le projet R R est un langage de programmation ainsi qu’un logiciel développés spécifiquement pour les analyses statistiques et les graphiques. Il est le résultat des travaux de deux statisticiens-programmeurs de la Nouvelle-Zélande, Ross Ihaka et Robert Gentleman qui se sont inspirés des langages S et Scheme (http://fr.wikipedia.org/wiki/Scheme). D’ailleurs, ils ont nommé le langage R en référence à leur prénom (Ross et Robert), mais aussi en guise de clin d’oeil à son prédecesseur S (http://fr.wikipedia.org/wiki/S_(langage_de_programmation)). Dès les débuts, Ihaka et Gentleman ont rendu le code source de leur langage disponible gratuitement à tous, mettant en pratique la philosophie que la qualité des graphiques et des analyses executés ne doivent pas être une fonction de l’épaisseur du portefeuille. Quiconque peut ainsi faire des analyses de très haut niveau peu importe sa situation financière. Le code source du langage étant ouvert, quiconque peut aller voir, s’il le désire, comment les fonctions ont été codées, les modifier pour son propre usage ou en créer de nouvelles pour des applications spécifiques, puis les rendre disponibles à tous par l’intermédiaire de banques de fonctions. Cette transparence et cet esprit de partage expliquent en partie le succès fulgurant de ce langage et la formation d’une large communauté d’utilisateurs dans le monde. L’initiative des deux Néo-Zélandais a pris la forme d’un projet global auquel s’est greffée une équipe de programmeurs et de statisticiens qui mettent à jour régulièrement le logiciel, y apportent des améliorations et rendent le tout disponible gratuitement à partir de plus de 75 sites répartis sur les cinq continents. Un aspect important de R est qu’il est disponible sous différents systèmes d’exploitation, incluant MS-Windows, Mac, et GNU/Linux. La syntaxe R est un langage de programmation orienté objet et interprété, ce qui signifie que toute information est gardée en mémoire sous forme d’objet pendant une session R tant que ce dernier n’est pas modifié ou éliminé (“orienté objet”) et qu’on n’a pas à compiler le programme avant de l’exécuter (“interprété”). On peut créer un objet en lui assignant un nom et de l’information. Une fois créé, cet objet existe dans la mémoire de R et peut prendre différentes formes comme une valeur numérique, du texte, un vecteur, une matrice, une fonction, un jeu de données ou le résultat d’une analyse . Contrairement à d’autres logiciels, lorsqu’on crée un élément, il demeure accessible durant toute la session. Par exemple, après avoir réalisé une analyse et stocké les résultats dans un objet, on peut décider d’extraire les résidus du modèle, des valeurs prédites, un \\(R^2\\) ou une autre valeur sans avoir à faire rouler à nouveau l’analyse. En effet, R garde en mémoire le résultat de l’analyse pour extraire les valeurs nécessaires. Conceptuellement, un objet est l’équivalent d’un contenant utilisé pour conserver des aliments dans un réfrigérateur dans votre milieu de travail – on peut mettre des étiquettes avec le nom du propriétaire sur le contenant afin de distinguer les différents contenants dans le réfrigérateur. C’est le même principe utilisé par R: on stocke de l’information dans un objet (le contenant) et le nom de cet objet (l’étiquette) permet à R de gérer l’information qu’on lui soumet. Toutes les commandes ou les fonctions en R suivent la même stratégie: un nom suivi de parenthèses. Par exemple, la fonction mean( ), étonnamment, permet de calculer la moyenne arithmétique. La plupart des fonctions possèdent des arguments séparés par des virgules. Dans notre exemple, la fonction mean( ) possède un argument 'x = ' que nous utilisons pour spécifier la variable sur laquelle nous voulons calculer la moyenne. Toutefois, avant de continuer avec des exemples, voyons comment accéder à l’aide disponible dans R. À l’aide Il existe plusieurs ressources pour assister les utilisateurs de R. Par exemple, la commande help.start( ) génère une page html à partir de laquelle nous pouvons naviguer vers différents thèmes au moyen d’hyperliens (Figure 0.7). Figure 0.7: Aide dans R avec r help.start( ). La commande help.search( ) permet de faire une recherche spécifique dans les fichiers d’aide de R. L’argument apropos permet de spécifier le sujet de la recherche. Par exemple, la commande : help.search(apropos = &quot;generalized linear models&quot;) retournera l’ensemble des fonctions qui traitent des modèles linéaires généralisés parmi les fonctions de R. Afin d’accéder directement à l’aide d’une fonction, on peut utiliser la commande ? suivie du nom de la fonction. Toute la documentation des fonctions est fondée sur la même structure: la présentation de la syntaxe, des arguments, et des détails sur l’usage et le résultat de l’opération, de l’auteur de la fonction et des références. La plupart des fonctions finissent par des exemples qu’on peut reproduire soi-même: il suffit de copier-coller le code dans la console. C’est un excellent moyen de se familiariser avec une fonction et son utilisation. ?mean #retourne l&#39;aide de la fonction mean( ) Un autre outil pratique est la fonction RSiteSearch( ) avec l’objet de la recherche entre guillements. Cette fonction cherche dans les pages d’aide de toutes les fonctions de R du site de R et n’est pas limité aux fonctions installées sur votre ordinateur. Après avoir exécuté la commande, une fenêtre de votre fureteur internet préféré s’ouvrira pour accéder aux résultats. Bien sûr, il est nécessaire d’avoir une connexion internet lors de l’exécution de cette fonction. Par exemple, RSiteSearch(&quot;error bars&quot;) présentera comme résultat les pages de fonctions faisant mention de barres d’erreurs sur des graphiques. On peut ensuite modifier la recherche ou l’étendre à partir du moteur de recherche directement sur la page web. Plusieurs ressources électroniques sont également disponibles sur une multitude de sites internet de chercheurs ou d’utilisateurs de R. Le site (http://www.rseek.org) peut faciliter cette recherche. Vecteurs et opérations de base R stocke l’information sous forme d’objets et ces derniers peuvent prendre différentes formes. Le premier type d’objet que nous verrons est le vecteur[^1]. Les éléments contenus dans un vecteur sont d’un seul type ou mode. Parmi les modes les plus courants, on compte, le mode numérique (numeric), entier (integer), caractère (character), facteur (factor) ou logique (logical). Créer un vecteur Le plus simple vecteur dans R consiste en une seule valeur (un scalaire). Comme premier exemple, créons un objet pour stocker le nombre de pistes (10) sur l’album Folklore du groupe 16 Horsepower1. Créons un vecteur que nous nommerons Nbre.pistes et qui prendra la valeur 10. Nbre.pistes &lt;- 10 Notons le symbole &lt;- (symbole inférieur suivi d’un tiret) qui est un opérateur d’assignation. En d’autres mots, la valeur de 10 est envoyée dans l’objet dénommé Nbre.pistes. Le symbole = peut aussi être utilisé pour assigner des valeurs, mais cela n’a pas toujours été le cas. Dans ce cours, nous utilisons le symbole classique d’assignation &lt;-. Rappelons aussi que l’on créé simplement en lui donnant un nom. Ce nom doit commencer par une lettre, laquelle peut être suivie de chiffres ou d’autres lettres. Par ailleurs, le nom de l’objet ne peut contenir d’espace; on peut le remplacer au besoin par un point ou un trait souligné. C’est une bonne pratique de garder le nom des objets court et informatif (éviter toto1, toto2) et d’éviter d’attribuer des noms déjà utilisés par des fonctions2. Pour reprendre à l’analogie des contenants dans un réfrigérateur, nous avons créé un contenant qui stocke la valeur 10, et avons placé l’étiquette Nbre.pistes sur ce contenant afin de pouvoir facilement l’identifier par la suite. Pour visualiser le contenu de l’objet, nous pouvons simplement saisir son nom dans la console (ou, préférablement, l’envoyer à R à partir de notre éditeur). Nbre.pistes ## [1] 10 Remarquons aussi que R fait la distinction entre les minuscules et les majuscules. Si nous interrogeons R à propos de l’objet nbre.Pistes : nbre.Pistes ## Error in eval(expr, envir, enclos): object &#39;nbre.Pistes&#39; not found Il ne reconnaît pas l’objet. À noter aussi qu’on peut créer un objet en lui assignant les valeurs d’un autre objet. Par exemple, si nous désirons créer un nouvel objet qui prendra la même valeur que Nbre.pistes, nous pouvons écrire : le.nombre.de.pistes &lt;- Nbre.pistes Nbre.pistes #l&#39;objet original le.nombre.de.pistes #le nouvel objet créé Si au lieu d’assigner une seule valeur à l’objet, nous désirons stocker le nombre de pistes de toute la discographie du groupe 16 Horsepower3, nous pouvons procéder ainsi : Nbre.pistes &lt;- c(6, 14, 14, 11, 11, 10, 20) L’opérateur d’assignation &lt;- indique que nous créons un objet, ici une variable numérique constituée des 7 valeurs à droite de l’opérateur. La fonction c()[^1] est une fonction qui combine les valeurs en un vecteur. Ces valeurs doivent être séparées par des virgules. La fonction c()[^1] est très souvent utilisée dans R pour combiner des éléments. Maintenant, observons ce que R retourne avec la commande Nbre.pistes. Nbre.pistes ## [1] 6 14 14 11 11 10 20 Nous voyons le nombre de pistes de chaque album: le vecteur initial a été remplacé par les nouvelles valeurs que nous avons spécifiées. On remarque que les valeurs sont précédées par [1]. Le chiffre entre [ ] donne l’indice de l’observation qui la succède (la valeur 6 est la première). Pour des vecteurs beaucoup plus longs, R retournera l’indice à chaque nouvelle ligne. Le même principe s’applique pour créer une chaîne de caractères. Dans le même ordre d’idées, nous pourrions créer un objet pour stocker les noms des albums de 16 Horsepower auxquels nous faisions référence dans Nbre.pistes. Albums &lt;- c(&quot;16 Horsepower&quot;, &quot;Sackcloth &#39;n&#39; Ashes&quot;, &quot;Low Estate&quot;, &quot;Secret South&quot;, &quot;Hoarse&quot;, &quot;Folklore&quot;, &quot;Olden&quot;) Albums ## [1] &quot;16 Horsepower&quot; &quot;Sackcloth &#39;n&#39; Ashes&quot; &quot;Low Estate&quot; &quot;Secret South&quot; ## [5] &quot;Hoarse&quot; &quot;Folklore&quot; &quot;Olden&quot; Sélectionner des valeurs On peut sélectionner certaines valeurs à partir d’un vecteur à l’aide de [][^1]. Par exemple, pour choisir la deuxième valeur du vecteur Nbre.pistes, on peut écrire : Nbre.pistes[2] ## [1] 14 Pour extraire les trois premières valeurs du vecteur, on peut utiliser : Nbre.pistes[1:3] ## [1] 6 14 14 L’opérateur : crée une séquence, ici de 1 à 3. On peut aussi sélectionner des valeurs non-consécutives en utilisant la fonction c() : Nbre.pistes[c(1, 3, 5)] ## [1] 6 14 11 Dans l’exemple ci-dessus, on a extrait les première, troisième et cinquième valeurs du vecteur. Il est aussi possible d’exclure des valeurs en précédant les indices du symbole - à l’intérieur des []. Nbre.pistes[-1] ## [1] 14 14 11 11 10 20 Nbre.pistes[-c(1,3,6)] ## [1] 14 11 11 20 Répéter des valeurs La fonction rep() permet de répéter une ou plusieurs valeurs. Par exemple, pour répéter la valeur 5 vingt fois : y20 &lt;- rep(x = 5, times = 20) y20 ## [1] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 La fonction rep() inclut deux arguments : x spécifie la valeur à répéter, tandis que times indique le nombre de répétitions. On peut aussi répéter des caractères. y20.caracteres &lt;- rep(x = c(&quot;neuf&quot;, &quot;vieux&quot;), times = 20) y20.caracteres ## [1] &quot;neuf&quot; &quot;vieux&quot; &quot;neuf&quot; &quot;vieux&quot; &quot;neuf&quot; &quot;vieux&quot; &quot;neuf&quot; &quot;vieux&quot; &quot;neuf&quot; &quot;vieux&quot; &quot;neuf&quot; &quot;vieux&quot; &quot;neuf&quot; ## [14] &quot;vieux&quot; &quot;neuf&quot; &quot;vieux&quot; &quot;neuf&quot; &quot;vieux&quot; &quot;neuf&quot; &quot;vieux&quot; &quot;neuf&quot; &quot;vieux&quot; &quot;neuf&quot; &quot;vieux&quot; &quot;neuf&quot; &quot;vieux&quot; ## [27] &quot;neuf&quot; &quot;vieux&quot; &quot;neuf&quot; &quot;vieux&quot; &quot;neuf&quot; &quot;vieux&quot; &quot;neuf&quot; &quot;vieux&quot; &quot;neuf&quot; &quot;vieux&quot; &quot;neuf&quot; &quot;vieux&quot; &quot;neuf&quot; ## [40] &quot;vieux&quot; Plusieurs fonctions de base peuvent être appliquées à une multitude de scénarios, et c’est l’un des attraits de la programmation en R. Ici, nous avons utilisé à nouveau la fonction c() pour créer un vecteur à 2 valeurs, neuf et vieux, et nous avons répété ce vecteur 20 fois. Il est donc important d’apprendre quelques fonctions de base qui reviendront souvent dans le cours. Créer une série La création d’une série ou séquence de valeurs avec un pas spécifique est réalisée avec la fonction seq(). y.seq &lt;- seq(from = 1, to = 10, by = 1) y.seq ## [1] 1 2 3 4 5 6 7 8 9 10 Les arguments de la fonction sont assez explicites : from indique le départ de la série, to indique la fin, et by spécifie le pas. On peut facilement créer une séquence qui décline. y.seq.decl &lt;- seq(from = 10, to = 1, by = -1) La valeur de l’argument by n’est pas limitée aux entiers. y.seq.01 &lt;- seq(from = 1, to = 5, by = 0.1) y.seq.01 ## [1] 1.0 1.1 1.2 1.3 1.4 1.5 1.6 1.7 1.8 1.9 2.0 2.1 2.2 2.3 2.4 2.5 2.6 2.7 2.8 2.9 3.0 3.1 3.2 3.3 3.4 3.5 ## [27] 3.6 3.7 3.8 3.9 4.0 4.1 4.2 4.3 4.4 4.5 4.6 4.7 4.8 4.9 5.0 La fonction seq() possède également un argument length.out qui permet d’obtenir le nombre de valeurs sans indiquer le pas. y.seq.length &lt;- seq(from = 1, to = 100, length.out = 12) y.seq.length ## [1] 1 10 19 28 37 46 55 64 73 82 91 100 Connaître les caractéristiques d’un vecteur Afin de s’informer sur la nature d’un objet, on peut utiliser la fonction class(). class(Nbre.pistes) ## [1] &quot;numeric&quot; class(Albums) ## [1] &quot;character&quot; On remarque que R identifie Nbre.pistes comme un vecteur numérique alors que Albums est identifié comme un vecteur de caractères. R possède aussi plusieurs fonctions logiques nous dévoilant les caractéristiques d’un vecteur. is.numeric(Nbre.pistes) ## [1] TRUE is.character(Albums) ## [1] TRUE La fonction length() est particulièrement utile pour déterminer le nombre d’observations . length(Nbre.pistes) ## [1] 7 length(y.seq.01) ## [1] 41 Opérateurs mathématiques R utilise les opérateurs mathématiques conventionnels tels que l’addition (+) , la soustraction (-), la multiplication (*) et la division (/), et respecte les conventions de priorité des opérations. Ainsi, dans une équation, la multiplication et la division ont priorité sur l’addition et la soustraction. 30 + 10 - 2 * 3 ## [1] 34 Ceci étant dit, c’est une bonne idée de regrouper certains termes lorsque les calculs sont particulièrement longs pour éviter les problèmes. Lors d’opérations mathématiques impliquant deux vecteurs de différentes tailles, les valeurs du vecteur le plus court sont recyclées ou réutilisées. Ce comportement est un aspect important de la programmation en R. Nbre.pistes + Nbre.pistes ## [1] 12 28 28 22 22 20 40 Les deux vecteurs ci-dessus sont de la même taille et l’opération s’effectue un élément à la fois. Toutefois, dans l’exemple suivant, on calcule la différence entre Nbre.pistes (7 valeurs) et 24 (1 valeur). Nbre.pistes - 24 ## [1] -18 -10 -10 -13 -13 -14 -4 Ici, la valeur 24 est recyclée et est soustraite de chaque valeur du vecteur Nbre.pistes. On observe le même comportement pour la multiplication de Nbre.pistes et y.seq créé plus tôt. Nbre.pistes * y.seq ## Warning in Nbre.pistes * y.seq: longer object length is not a multiple of shorter object length ## [1] 6 28 42 44 55 60 140 48 126 140 Puisque Nbre.pistes comporte 7 valeurs et que y.seq en compte 10, les premières valeurs de Nbre.pistes seront réutilisées avec les dernières valeurs de y.seq pour compléter le calcul. On remarque le message d’avertissement de R à propos de l’objet le plus long qui n’est pas un multiple de l’objet le plus court (10 n’est pas un multiple de 7, mais est un multiple de 5 et de 2). Le tableau 0.1 montre plusieurs fonctions mathématiques de base. Les fonctions trigonométriques sont également disponibles en R (Tableau 0.2). Table 0.1: Fonctions mathématiques de base courantes en R. Fonction Description min(x) valeur minimale parmi les éléments de x max(x) valeur maximale parmi les éléments de x range(x) étendue des éléments de x rank(x) le rang des éléments de x (ième observation en ordre croissant) round(x, digits) arrondit les éléments de x à digits décimales sum(x) somme des éléments de x cumsum(x) renvoie le vecteur dont le ième élément est la somme de x[1] à x[i] prod(x) produit des éléments de x cumprod(x) renvoie le vecteur dont le ième élément est le produit de x[1] à x[i] abs(x) valeur absolue des éléments de x sqrt(x) racine carrée des éléments de x log2(x) logarithme en base 2 des éléments de x log10(x) logarithme en base 10 des éléments de x log(x, base) logarithme en base base des éléments de x log(x) logarithme naturel des éléments de x équivaut à log(x, base = exp(1)) exp(x) renvoie la valeur de e élevée à la puissance x x^y la valeur de x élevée à la puissance y Table 0.2: Fonctions trigonométriques courantes en R. Fonction Description cos(x) cosinus en radians des éléments de x sin(x) sinus en radians des éléments de x tan(x) tangente en radians des éléments de x acos(x) arc-cosinus en radians des éléments de x asin(x) arc-sinus en radians des éléments de x atan(x) arc-tangente en radians des éléments de x Tests logiques Une série d’opérateurs logiques sont disponibles en R, le résultat étant TRUE ou FALSE. Les opérateurs logiques les plus intuitifs sont &lt;, &lt;=, &gt;, et &gt;=. L’opérateur == teste si deux éléments sont exactement égaux, alors que != teste si deux éléments sont différents. Pour comparer deux vecteurs de valeurs logiques, on peut utiliser l’opérateur &amp; qui déterminera l’intersection des deux vecteurs, c’est-à-dire les cas où les tests sont vrais pour les deux simultanément. L’opérateur | (ou logique) permet de déterminer l’union des deux vecteurs (les cas où les tests sont vrais pour l’un ou l’autre). Nbre.pistes ## [1] 6 14 14 11 11 10 20 Nbre.pistes &gt;= 10 ## [1] FALSE TRUE TRUE TRUE TRUE TRUE TRUE Nbre.pistes != 10 ## [1] TRUE TRUE TRUE TRUE TRUE FALSE TRUE Albums == &quot;Olden&quot; #spécifier les caractères entre &quot; &quot; ## [1] FALSE FALSE FALSE FALSE FALSE FALSE TRUE vec.log1 &lt;- c(TRUE, FALSE, TRUE, TRUE, FALSE) vec.log2 &lt;- c(TRUE, TRUE, FALSE, FALSE, FALSE) vec.log1 &amp; vec.log2 #deux tests vrais pour la valeur 1 ## [1] TRUE FALSE FALSE FALSE FALSE vec.log1 | vec.log2 #test vrai pour valeurs 1, 2, 3, 4 ## [1] TRUE TRUE TRUE TRUE FALSE Les fonctions all() et any() déterminent si toutes ou au moins l’une des valeurs répondent au test logique, respectivement. Pour comparer deux vecteurs entre eux, on utilisera plutôt les fonctions all.equal() ou identical(). any(Nbre.pistes &gt; 11) ## [1] TRUE all(Nbre.pistes == 11) ## [1] FALSE all.equal(Nbre.pistes, Nbre.pistes) ## [1] TRUE Nbre &lt;- rep(10, 7) #on crée un autre vecteur Nbre ## [1] 10 10 10 10 10 10 10 all.equal(Nbre.pistes, Nbre) ## [1] &quot;Mean relative difference: 0.3157895&quot; identical(Nbre.pistes, Nbre) ## [1] FALSE Le dernier appel à all.equal() nous indique que les deux vecteurs ne sont pas identiques puisqu’il y a une différence moyenne entre les deux de 0.31579. La fonction which() est aussi apparentée aux tests logiques. Elle permet de déterminer lesquelles des valeurs répondent à une certaine condition énoncée par un test logique. Les variantes which.min() et which.max() permettent d’identifier les valeurs minimales et maximales, respectivement. which(Nbre.pistes &gt; 11) #retourne les indices ## [1] 2 3 7 Nbre.pistes[which(Nbre.pistes &gt; 11)] #retourne les valeurs ## [1] 14 14 20 which(Albums == &quot;Folklore&quot;) #retourne les indices ## [1] 6 which.max(Nbre.pistes) #indice de l&#39;observation maximale ## [1] 7 which.min(Nbre.pistes) #indice de l&#39;observation minimale ## [1] 1 La fonction ifelse() permet de faire un test conditionnel sur un vecteur. Par exemple, pour créer une variable binaire à partir de la variable Nbre.pistes (c.-à-d., &lt;= 10 ou &gt; 10), on procéderait ainsi : Bin10 &lt;- ifelse(Nbre.pistes &lt;= 10, 0, 1) Bin10 ## [1] 0 1 1 1 1 0 1 On procède de la même façon si le vecteur contient des caractères : y20.caracteres ## [1] &quot;neuf&quot; &quot;vieux&quot; &quot;neuf&quot; &quot;vieux&quot; &quot;neuf&quot; &quot;vieux&quot; &quot;neuf&quot; &quot;vieux&quot; &quot;neuf&quot; &quot;vieux&quot; &quot;neuf&quot; &quot;vieux&quot; &quot;neuf&quot; ## [14] &quot;vieux&quot; &quot;neuf&quot; &quot;vieux&quot; &quot;neuf&quot; &quot;vieux&quot; &quot;neuf&quot; &quot;vieux&quot; &quot;neuf&quot; &quot;vieux&quot; &quot;neuf&quot; &quot;vieux&quot; &quot;neuf&quot; &quot;vieux&quot; ## [27] &quot;neuf&quot; &quot;vieux&quot; &quot;neuf&quot; &quot;vieux&quot; &quot;neuf&quot; &quot;vieux&quot; &quot;neuf&quot; &quot;vieux&quot; &quot;neuf&quot; &quot;vieux&quot; &quot;neuf&quot; &quot;vieux&quot; &quot;neuf&quot; ## [40] &quot;vieux&quot; bin.y20 &lt;- ifelse(y20.caracteres == &quot;neuf&quot;, &quot;nouveau&quot;, &quot;ancien&quot;) bin.y20[1:5] ## [1] &quot;nouveau&quot; &quot;ancien&quot; &quot;nouveau&quot; &quot;ancien&quot; &quot;nouveau&quot; La fonction ifelse( ) est aussi pratique pour créer des classes (i.e., une variable catégorique) en imbriquant plusieurs tests sur le vecteur : Classe.pistes &lt;- ifelse(Nbre.pistes &lt; 10, &quot;&lt; 10&quot;, ifelse(Nbre.pistes &gt;= 10 &amp; Nbre.pistes &lt; 15, &quot;10 - 15&quot;, &quot;&gt; 15&quot;)) Le code ci-dessus est réparti sur plusieurs lignes pour en améliorer la lisibilité, mais nous aurions obtenu exactement le même résultat si tout le code s’était trouvé sur une même ligne. Lorsqu’on saisit une commande sur plusieurs lignes, R attend la parenthèse de fermeture avant de réaliser l’opération. On débute en déterminant si Nbre.pistes &lt; 10, et on attribue la valeur \"&lt; 10\" le cas échéant. Sinon, on teste si Nbre.pistes &gt;= 10 mais &lt; 15, et, si c’est le cas, on donne la valeur de \"10-15\" au vecteur. Si ce test est faux, on attribue une valeur de \"&gt; 15\" au vecteur. On peut utiliser plusieurs des opérateurs logiques pour faire des tests de conditions pour décider de l’exécution d’autres opérations. Par exemple, si le test est vrai, on effectue une opération particulière, et, si le test est faux, on effectue une opération différente. L’énoncé if appelle un test de condition spécifié entre parenthèses. L’opération à exécuter le cas échéant sera indiquée entre {}. Dans l’exemple ci-dessous, on teste si l’objet contient une valeur numérique ou une chaîne de caractères. Si la valeur est une chaîne de caractères, on imprime à l’écran \"des caractères\" à l’aide de la fonction paste(). Pour une valeur numérique, le test imprimera \"pas des caractères\" à l’écran. valeur &lt;- 200 #une valeur numérique if(is.character(valeur)) {paste(&quot;des caractères&quot;) } else {paste(&quot;pas des caractères&quot;)} ## [1] &quot;pas des caractères&quot; valeur &lt;- &quot;du texte&quot; #une chaîne de caractères if(is.character(valeur)) {paste(&quot;des caractères&quot;) } else {paste(&quot;pas des caractères&quot;)} ## [1] &quot;des caractères&quot; On remarque que, si le test est faux, l’opération à effectuer doit être spécifiée après l’énoncé else et doit être insérée entre {}. Il est possible d’imbriquer les tests de conditions. Pour les vecteurs plus haut, nous avons utilisé &amp; et | et le comportement était similaire à des opérateurs mathématiques : le calcul était exécuté pour chaque paire d’éléments des deux vecteurs comparés. Les homologues &amp;&amp; et || ont un usage légèrement différent. En effet, lorsqu’on utilise &amp;&amp;, le test à droite s’effectue sur le premier élément de l’objet seulement si le test sur le premier élément de gauche est vrai. Avec ||, le test à droite de l’opérateur sera évalué sur le premier élément seulement si le test de gauche sur le premier élément est faux. des.valeurs &lt;- 1:10 des.valeurs ## [1] 1 2 3 4 5 6 7 8 9 10 des.valeurs &gt; 0 ## [1] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE des.valeurs &lt; 0 ## [1] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE des.valeurs &gt; 0 &amp; des.valeurs &lt; 0 ## [1] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE des.valeurs &gt; 0 | des.valeurs &lt; 0 ## [1] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE des.valeurs &gt; 0 &amp;&amp; des.valeurs &lt; 0 # CODE ORIGINAL NE MARCHE PAS ## Error in des.valeurs &gt; 0 &amp;&amp; des.valeurs &lt; 0: &#39;length = 10&#39; in coercion to &#39;logical(1)&#39; des.valeurs &gt; 0 || des.valeurs &lt; 0 # CODE ORIGINAL NE MARCHE PAS ## Error in des.valeurs &gt; 0 || des.valeurs &lt; 0: &#39;length = 10&#39; in coercion to &#39;logical(1)&#39; # NOUVEAU CODE all_positive &lt;- all(des.valeurs &gt; 0) all_negative &lt;- all(des.valeurs &lt; 0) all_positive &amp;&amp; all_negative ## [1] FALSE all_positive || all_negative ## [1] TRUE Nous revisiterons les tests logiques à la section Créer des sous-ensembles pour créer des sous-ensembles de jeux de données. Matrices et calculs matriciels Créer une matrice La création et la manipulation de matrices sont simples dans R. La matrice est un autre type d’objet. Elle contient des éléments qui sont soit numériques ou des caractères, mais pas un mélange des deux. Par exemple, si la matrice contient au moins un seul caractère, tous les éléments seront convertis en caractères. Créons une matrice simple à l’aide de matrix() constituée d’une séquence de 1 à 9 avec un pas de 1 : mat &lt;- matrix(data = 1:12, nrow = 3, ncol = 4) mat ## [,1] [,2] [,3] [,4] ## [1,] 1 4 7 10 ## [2,] 2 5 8 11 ## [3,] 3 6 9 12 Les arguments nrow et ncol indiquent les dimensions de la matrice que nous désirons obtenir (une matrice de 3 rangées et de 4 colonnes). On remarque que les éléments sont insérés dans la matrice par colonnes : la première colonne est remplie, ensuite la deuxième et la troisième. L’argument byrow permet de changer la manière d’insérer les éléments dans la matrice. mat&lt;-matrix(data = 1:12, nrow = 3, ncol = 4, byrow = TRUE) mat ## [,1] [,2] [,3] [,4] ## [1,] 1 2 3 4 ## [2,] 5 6 7 8 ## [3,] 9 10 11 12 On obtient une nouvelle matrice dans laquelle les valeurs ont été insérées une rangée à la fois. Par défaut, l’argument byrow prend la valeur FALSE. Sélectionner des valeurs Tout comme pour le vecteur, on utilise [] pour extraire des éléments d’une matrice. Toutefois, on devra spécifier les deux dimensions [rangée, colonne]. Par exemple, mat[1, 3] ## [1] 3 mat[1, ] ## [1] 1 2 3 4 mat[, 2] ## [1] 2 6 10 mat[1:2, 1] ## [1] 1 5 mat[c(1, 3), 3] ## [1] 3 11 À noter que si on spécifie seulement l’indice de la rangée, toute la rangée sera extraite. De la même façon, si seul l’indice de la colonne est spécifié, toute la colonne sera extraite. Connaître les caractéristiques d’une matrice On peut vérifier les caractéristiques de la matrice à l’aide de la fonction class ou de tests logiques. class(mat) ## [1] &quot;matrix&quot; &quot;array&quot; is.numeric(mat) ## [1] TRUE is.vector(mat) ## [1] FALSE La fonction length() permet de déterminer le nombre d’éléments de la matrice. On peut utiliser dim(), nrow() ou ncol() pour connaître les dimensions de la matrice (rangées, colonnes). length(mat) ## [1] 12 dim(mat) ## [1] 3 4 nrow(mat) ## [1] 3 ncol(mat) ## [1] 4 Il est possible d’ajouter des étiquettes aux colonnes et aux rangées à l’aide de colnames() et rownames(), respectivement. colnames(mat) ## NULL colnames(mat) &lt;- c(&quot;C1&quot;, &quot;C2&quot;, &quot;C3&quot;, &quot;C4&quot;) colnames(mat) ## [1] &quot;C1&quot; &quot;C2&quot; &quot;C3&quot; &quot;C4&quot; rownames(mat) &lt;- c(&quot;R1&quot;, &quot;R2&quot;, &quot;R3&quot;) mat ## C1 C2 C3 C4 ## R1 1 2 3 4 ## R2 5 6 7 8 ## R3 9 10 11 12 Opérations et manipulation de matrices Plusieurs fonctions agissent sur les matrices. Par exemple, diag( permet d’extraire la diagonale de la matrice. diag(mat) ## [1] 1 6 11 On peut facilement transposer une matrice. t(mat) ## R1 R2 R3 ## C1 1 5 9 ## C2 2 6 10 ## C3 3 7 11 ## C4 4 8 12 Afin d’obtenir la somme ou la moyenne des colonnes d’une matrice, on peut utiliser les fonctions colSums() et colMeans(), respectivement. Il existe des fonctions similaires pour les rangées d’une matrice. colSums(mat) ## C1 C2 C3 C4 ## 15 18 21 24 colMeans(mat) ## C1 C2 C3 C4 ## 5 6 7 8 rowSums(mat) ## R1 R2 R3 ## 10 26 42 rowMeans(mat) ## R1 R2 R3 ## 2.5 6.5 10.5 Pour multiplier des matrices, il faut mettre l’opérateur de multiplication entre des signes de pourcentage : vecteur &lt;- c(2, 4, 5) vecteur%*%mat ## C1 C2 C3 C4 ## [1,] 67 78 89 100 Jeux de données Un des types d’objet les plus utilisés dans R est le jeu de données ou data.frame. L’objet data.frame a deux dimensions et peut être constitué de colonnes de différents types. On peut retrouver des colonnes de valeurs numériques, de valeurs logiques, d’entiers, de caractères ou de facteurs (factor). Créer un jeu de données La fonction data.frame() permet de créer un jeu de données. On peut combiner une série de vecteurs en un data.frame. Temps &lt;- c(1.2, 3.4, 2.1, 5.5) #vecteur de temps Masse &lt;- c(2.5, 4.2, 5.6, 3.4) #vecteur de masse Sexe &lt;- c(&quot;mâle&quot;, &quot;femelle&quot;, &quot;mâle&quot;, &quot;femelle&quot;) #vecteur de caractères jeu &lt;- data.frame(Temps, Masse, Sexe) #création du data.frame jeu ## Temps Masse Sexe ## 1 1.2 2.5 mâle ## 2 3.4 4.2 femelle ## 3 2.1 5.6 mâle ## 4 5.5 3.4 femelle On peut bien sûr créer un jeu de données en une seule étape : autos &lt;- data.frame(Vitesse = c(25, 40, 70, 100), Type = c(&quot;auto&quot;, &quot;camion&quot;, &quot;auto&quot;, &quot;camion&quot;)) autos ## Vitesse Type ## 1 25 auto ## 2 40 camion ## 3 70 auto ## 4 100 camion Connaître les caractéristiques de jeu de données Plusieurs des fonctions que nous avons déjà présentées permettent de décrire un jeu de données. Notons les fonctions class(), length(), dim(), nrow(), ncol(), names(), rownames(), et colnames(). Nous pouvons aussi utiliser la fonction str() pour connaître la structure du jeu de données, incluant l’identification de chacune des variables, son type, ainsi que les premières observations. str(jeu) ## &#39;data.frame&#39;: 4 obs. of 3 variables: ## $ Temps: num 1.2 3.4 2.1 5.5 ## $ Masse: num 2.5 4.2 5.6 3.4 ## $ Sexe : chr &quot;mâle&quot; &quot;femelle&quot; &quot;mâle&quot; &quot;femelle&quot; Remarquons que la variable Sexe a été créée comme vecteur de caractères et est maintenant reconnue comme facteur (factor) une fois ajoutée à l’objet de type data.frame. Plusieurs fonctions sont disponibles pour transformer un objet d’un type à un autre. Mentionnons ici as.factor(~) qui permet de convertir un vecteur en variable catégorique. La fonction summary() quant à elle, nous informe sur les statistiques descriptives des variables du jeu de données. summary(jeu) ## Temps Masse Sexe ## Min. :1.200 Min. :2.500 Length:4 ## 1st Qu.:1.875 1st Qu.:3.175 Class :character ## Median :2.750 Median :3.800 Mode :character ## Mean :3.050 Mean :3.925 ## 3rd Qu.:3.925 3rd Qu.:4.550 ## Max. :5.500 Max. :5.600 D’autres fonctions, comme head() ou tail(), permettent d’afficher les six premières ou six dernières lignes du jeu de données. Utilisons ces fonctions sur un jeu de données déjà dans R, lequel traite de données de diamètre, de hauteur et du volume de cerisiers. Afin de déterminer les jeux de données disponibles dans R, il suffit d’utiliser la commande data() sans valeurs entre les parenthèses. Afin de charger le jeu de données de notre choix, il suffit d’exécuter la commande data(), cette fois-ci en spécifiant le nom du jeu de données entre parenthèses. data(trees) #jeu de données déja dans R str(trees) ## &#39;data.frame&#39;: 31 obs. of 3 variables: ## $ Girth : num 8.3 8.6 8.8 10.5 10.7 10.8 11 11 11.1 11.2 ... ## $ Height: num 70 65 63 72 81 83 66 75 80 75 ... ## $ Volume: num 10.3 10.3 10.2 16.4 18.8 19.7 15.6 18.2 22.6 19.9 ... head(trees) #comparer à trees[1:6, ] ## Girth Height Volume ## 1 8.3 70 10.3 ## 2 8.6 65 10.3 ## 3 8.8 63 10.2 ## 4 10.5 72 16.4 ## 5 10.7 81 18.8 ## 6 10.8 83 19.7 tail(trees) #comparer à trees[(nrow(trees)-5):nrow(trees), ] ## Girth Height Volume ## 26 17.3 81 55.4 ## 27 17.5 82 55.7 ## 28 17.9 80 58.3 ## 29 18.0 80 51.5 ## 30 18.0 80 51.0 ## 31 20.6 87 77.0 Accéder aux variables d’un jeu de données On remarque rapidement qu’il n’est pas possible d’aller chercher une variable d’un jeu de données simplement en tapant son nom. Par exemple, la variable Vitesse se trouve dans le jeu de données autos créé plus haut. autos ## Vitesse Type ## 1 25 auto ## 2 40 camion ## 3 70 auto ## 4 100 camion names(autos) ## [1] &quot;Vitesse&quot; &quot;Type&quot; Pourtant, on obtient un message d’erreur si on spécifie Vitesse. L’objet autos est un jeu de données et les variables en sont des éléments. Il existe différentes stratégies pour extraire des éléments d’un jeu de données. La première, et celle qui est la plus polyvalente, consiste à utiliser l’opérateur $. Vitesse autos$Vitesse ## [1] 25 40 70 100 Nous spécifions le jeu de données suivi de l’opérateur $, lui-même suivi du nom de la variable d’intérêt : Vitesse est un élément de autos. Nous utiliserons la même stratégie pour extraire certains éléments d’objets plus complexes, tels que la sortie (output) d’une analyse statistique. Comme seconde stratégie, on peut utiliser [ , ] pour extraire les éléments du jeu de données, puisqu’il a deux dimensions comme la matrice. autos[, 1] ## [1] 25 40 70 100 Une variante de l’utilisation de [ , ] consiste à spécifier le nom de la variable. autos[, &quot;Vitesse&quot;] ## [1] 25 40 70 100 Importer des fichiers de données Au lieu d’entrer à la main les données dans R, nous allons généralement importer les données sauvegardées dans un fichier. Il est facile d’importer des fichiers au format texte brut en R en suivant quelques lignes directrices. Il existe une multitude de moyens pour importer des fichiers dans R, mais nous n’en présenterons que quelques-uns. Bien que le fichier doive être en format texte, l’extension peut être celle de votre choix. Pour faciliter l’analyse de jeux de données, nous recommandons de structurer vos fichiers en format « long ». Dans ce format, toutes les valeurs d’une même variable se trouvent sous une même colonne. Par exemple, dans une expérience agricole, on pourrait présenter les données dans un tableau en format « large » (Table 0.3). Table 0.3: Données récoltées lors d’une expérience agricole en format « large ». Témoin Engrais.A Engrais.B Engrais.C 6.1 6.3 7.1 8.1 5.9 6.2 8.2 8.5 5.8 5.8 7.3 7.6 5.4 6.3 6.9 7.8 Bien que le format « large » permette de visualiser rapidement les données, il n’est pas celui qui est généralement utilisable par plusieurs fonctions d’analyses statistiques. En effet, le format le plus souvent requis est le format « long ». Le tableau 0.4 présente le même jeu de données en ce format. Table 0.4: Données récoltées lors d’une expérience agricole en format “long”. Réponse Traitement 6.1 Témoin 5.9 Témoin 5.8 Témoin 5.4 Témoin 6.3 EngraisA 6.2 EngraisA 5.8 EngraisA 6.3 EngraisA Une fois le fichier en format « long », on peut sauvegarder en texte avec des séparateurs tabulations ou espace à partir du chiffrier de notre choix. Afin de rendre l’importation plus agréable et plus facile, il est important de respecter les points suivants : Réserver la première ligne de chaque colonne pour le nom de la colonne. Éviter les espaces dans le nom d’une colonne donnée (par exemple, ne pas écrire hauteur moyenne, mais plutôt hauteur_moyenne, hauteur.moyenne, hauteurmoyenne). Utiliser NA pour indiquer les valeurs manquantes à l’intérieur du jeu de données : aucune cellule ne peut être vide. Éviter les accents dans les noms des variables ou dans les données. La fonction principale d’importation de jeux de données est read.table(). Cette fonction comporte un argument file =, avec lequel on doit spécifier le chemin complet du fichier. L’argument logique header doit prendre la valeur TRUE. Pour réussir l’importation, il faut : Spécifier le bon chemin en se souciant des majuscules et des minuscules. Utiliser / ou \\\\ mais pas le \\ de MS-Windows. Terminer le chemin complet par le nom du fichier et mettre le tout entre \"\". Stocker le jeu de données dans un objet, sinon il ne sera imprimé qu’à l’écran. Remplacer les virgules par des points pour indiquer la décimale des valeurs numériques avant d’importer le fichier dans R ou spécifier l’argument dec = \",\" de la fonction read.table(). Cette démarche est nécessaire puisque R utilise le point pour indiquer les décimales. Bien que R reconnaisse généralement sans difficulté les caractères accentués (par exemple, é, à, ê), il peut y avoir un problème lorsque le fichier est créé dans un système d’exploitation et importé dans R dans un autre. Dans de tels cas, les caractères accentués ne seront pas reconnus par défaut. Par exemple, si vous travaillez sous Windows et que vous désirez qu’un collègue opérant sous Mac importe un de vos fichiers de données dans R, il peut survenir des problèmes d’importation en présence d’accents dans le fichier. Certains arguments de read.table() permettent de modifier ceci (encoding =), mais c’est plus simple d’éviter les accents. Illustrons avec le jeu de données vers qui traite de l’abondance de vers de terre dans différents types de parcs publics. Voici trois stratégies équivalentes pour importer un fichier en format texte et qui respecte les consignes mentionnées plus haut : Pour la première, on spécifie le chemin complet sur l’ordinateur où se trouve le fichier. Par exemple, pour importer un jeu de données stocké dans le fichier , on pourrait envoyer la commande vers &lt;- read.table(file = &quot;Guide_R/data/vers.txt&quot;, header = TRUE) La deuxième variation consiste à spécifier le répertoire où se trouve le fichier à l’aide de la fonction setwd( ). setwd(&quot;Guide_R/data/&quot;) vers &lt;- read.table(file = &quot;vers.txt&quot;, header = TRUE) Une troisième stratégie s’avère l’utilisation de la fonction file.choose( ). Cette dernière permet de sélectionner le fichier à l’aide d’une fenêtre graphique. Il faut toutefois savoir où se trouve le fichier … vers &lt;- read.table(file = file.choose( ), header = TRUE) Si vous ne respectez pas ces consignes, vous risquez de générer un message d’erreur. Vous devrez retracer l’erreur ou alors décortiquer plus en détail les pages d’aide des fonctions d’importation afin de modifier les arguments qui reflètent votre situation. Les fonctions read.delim(), read.delim2(), read.csv(), read.csv2() sont des versions de read.table() avec différentes valeurs par défaut. Il est aussi possible d’importer directement des fichiers en format MS-Excel ou MS-Access à l’aide de banques de fonctions téléchargeables. Il suffit d’utiliser RSiteSearch(\" \") avec le sujet de votre requête entre \"\" pour obtenir plus d’informations. Ajouter une variable Il est facile d’ajouter une variable à un jeu de données. Pour ce faire, on peut utiliser l’opérateur $ afin d’indiquer à R l’objet qui stocke le jeu de données et le nom de la nouvelle variable que l’on veut créer : head(vers) #affiche les 6 premières observations vers$log.Superficie &lt;- log(vers$Superficie) #créer une nouvelle variable head(vers) #inclut maintenant la nouvelle variable La nouvelle variable sera ajoutée dans une colonne à la suite des autres variables du jeu de données. Créer des sous-ensembles Nous pouvons utiliser différentes approches afin de sélectionner des sous-ensembles d’un jeu de données. Nous avons déjà vu l’extraction à l’aide de [ , ]. Par exemple, dans le jeu de données trees, on peut extraire les 15 premières observations des variables Girth et Height. trees[1:15, c(&quot;Girth&quot;, &quot;Height&quot;)] ## Girth Height ## 1 8.3 70 ## 2 8.6 65 ## 3 8.8 63 ## 4 10.5 72 ## 5 10.7 81 ## 6 10.8 83 ## 7 11.0 66 ## 8 11.0 75 ## 9 11.1 80 ## 10 11.2 75 ## 11 11.3 79 ## 12 11.4 76 ## 13 11.4 76 ## 14 11.7 69 ## 15 12.0 75 Les tests logiques s’avèrent aussi d’excellents outils pour créer des sous-ensembles de jeux de données. Par exemple, on peut créer un sous-ensemble de trees pour lequel Girth &gt; 12 ou de vers pour le type Vegetation correspondant à Prairie. trees22 &lt;- trees[trees$Girth &gt;= 12, ] vers.prairie &lt;- vers[vers$Vegetation == &quot;Prairie&quot;, ] Il est aussi possible de combiner des tests logiques. Créons, par exemple, un jeu de données à partir de vers pour lequel Vegetation correspond à Prairie et Pente &gt; 5 en utilisant l’opérateur &amp; (intersection). versb &lt;- vers[vers$Vegetation == &quot;Prairie&quot; &amp; vers$Pente &gt; 5, ] Comparons le résultat de l’union des deux conditions avec l’opérateur |. versc &lt;- vers[vers$Vegetation == &quot;Prairie&quot; | vers$Pente &gt; 5, ] La fonction subset( ) utilise les tests logiques pour créer un sous-ensemble d’un jeu de données. Cette fonction est une alternative à la sélection avec [, ] combinée à un test logique et donnera exactement le même résultat que dans l’exemple précédent. vers.les.prairie &lt;- subset(x = vers, subset = vers$Vegetation == &quot;Prairie&quot;) identical(vers.les.prairie, vers.prairie) ## [1] TRUE versbb &lt;- subset(x = vers, subset = vers$Vegetation == &quot;Prairie&quot; &amp; vers$Pente &gt; 5) identical(versb, versbb) ## [1] TRUE Effectuer un tri Les tris dans R comportent quelques subtilités : la fonction à utiliser dépend de ce que l’on veut trier. Pour faire le tri en ordre croissant d’une seule variable ou vecteur, on peut utiliser sort(). Cette fonction comporte un argument decreasing = FALSE qui peut prendre la valeur FALSE pour un tri en ordre décroissant. Comme alternative à l’argument decreasing = TRUE, la fonction rev() permet de faire un tri en ordre décroissant. sort(vers$Superficie) #en ordre croissant ## [1] 0.8 1.5 1.8 1.9 2.1 2.2 2.4 2.8 2.9 2.9 3.1 3.3 3.5 3.6 3.7 3.8 3.9 4.1 4.4 5.1 sort(vers$Superficie, decreasing = TRUE) #en ordre décroissant ## [1] 5.1 4.4 4.1 3.9 3.8 3.7 3.6 3.5 3.3 3.1 2.9 2.9 2.8 2.4 2.2 2.1 1.9 1.8 1.5 0.8 rev(sort(vers$Superficie)) #en ordre décroissant ## [1] 5.1 4.4 4.1 3.9 3.8 3.7 3.6 3.5 3.3 3.1 2.9 2.9 2.8 2.4 2.2 2.1 1.9 1.8 1.5 0.8 Comme toute autre manipulation dans R, si on veut que le tri prenne effet, il faut assigner ce tri à un objet. Autrement, le résultat s’affichera à l’écran et disparaîtra à tout jamais sans être disponible plus tard dans la session de travail. Afin de trier un jeu de données, on doit plutôt utiliser order(). Par exemple, pour trier le jeu de données vers en ordre croissant selon la variable Pente, on procède ainsi : ##tri en ordre croissant selon Pente vers.ord &lt;- vers[order(vers$Pente), ] ##tri en ordre décroissant selon Pente vers.ord.dec &lt;- vers[order(vers$Pente, decreasing = TRUE), ] ##autre moyen d&#39;obtenir un tri décroissant avec sort( ) vers.ord.dec2 &lt;- vers[rev(order(vers$Pente)), ] Il est facile d’effectuer un tri pour seulement une partie du jeu de données en spécifiant les colonnes désirées. Une autre option est de faire un tri selon plusieurs variables. ##tri des variables 1, 2 et 4 selon Pente vers.ordb &lt;- vers[order(vers$Pente), c(1,2,4)] ##tri du jeu de données selon Pente ET Superficie vers.ord2 &lt;- vers[order(vers$Pente, vers$Superficie) , ] Créer des tableaux récapitulatifs des fréquences Quelques fonctions sont très pratiques pour réaliser des résumés d’un jeu de données selon quelques variables. Notamment, pour obtenir l’équivalent du tableau croisé dynamique de MS EXCEL ou du pilote de données de Calc de OpenOffice, la fonction table() s’avère fort utile. Cette fonction résume la fréquence à laquelle chaque valeur d’une variable apparaît dans un jeu de données dans un tableau, lequel est un autre type d’objet en R. table(vers$Vegetation) #la fréquence de chaque type de végétation ## ## Arable Chaparral Prairie Pre Verger ## 3 4 9 3 1 table(vers$Superficie) #la fréquence de chaque observation numérique ## ## 0.8 1.5 1.8 1.9 2.1 2.2 2.4 2.8 2.9 3.1 3.3 3.5 3.6 3.7 3.8 3.9 4.1 4.4 5.1 ## 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 class(table(vers$Superficie)) ## [1] &quot;table&quot; On peut également construire un tableau à deux dimensions ou de dimensions supérieures selon des variables généralement catégoriques. L’argument deparse.level permet d’ajouter le nom des variables aux rangées ou aux colonnes. ##tableau à deux dimensions table(vers$Vegetation, vers$Pente) ## ## 0 1 2 3 4 5 6 8 10 11 ## Arable 1 0 2 0 0 0 0 0 0 0 ## Chaparral 1 0 0 0 0 0 0 1 2 0 ## Prairie 0 2 2 2 1 0 1 0 0 1 ## Pre 2 0 0 0 0 1 0 0 0 0 ## Verger 1 0 0 0 0 0 0 0 0 0 ##tableau à deux dimensions avec les noms table(vers$Vegetation, vers$Pente, deparse.level = 2) ## vers$Pente ## vers$Vegetation 0 1 2 3 4 5 6 8 10 11 ## Arable 1 0 2 0 0 0 0 0 0 0 ## Chaparral 1 0 0 0 0 0 0 1 2 0 ## Prairie 0 2 2 2 1 0 1 0 0 1 ## Pre 2 0 0 0 0 1 0 0 0 0 ## Verger 1 0 0 0 0 0 0 0 0 0 ##tableau à trois dimensions table(vers$Vegetation, vers$Pente, vers$Humide) ## , , = FALSE ## ## ## 0 1 2 3 4 5 6 8 10 11 ## Arable 1 0 2 0 0 0 0 0 0 0 ## Chaparral 1 0 0 0 0 0 0 0 1 0 ## Prairie 0 2 2 2 0 0 1 0 0 1 ## Pre 0 0 0 0 0 0 0 0 0 0 ## Verger 1 0 0 0 0 0 0 0 0 0 ## ## , , = TRUE ## ## ## 0 1 2 3 4 5 6 8 10 11 ## Arable 0 0 0 0 0 0 0 0 0 0 ## Chaparral 0 0 0 0 0 0 0 1 1 0 ## Prairie 0 0 0 0 1 0 0 0 0 0 ## Pre 2 0 0 0 0 1 0 0 0 0 ## Verger 0 0 0 0 0 0 0 0 0 0 La dernière commande présentée ci-dessus crée une série de tableaux à deux dimensions pour chaque valeur de la variable Humide (i.e., TRUE, FALSE). La fonction xtabs() offre la possibilité de résumer une variable de fréquence qui apparaît dans un jeu de données selon une série de variables en utilisant une formule du genre freq ~ var1 + var2 où la variable de fréquence apparaît à la gauche de l’équation et les variables de classification à la droite. ##on crée un jeu de données jeu.freq &lt;- data.frame(Freq = c(0, 1, 0, 10, 2, 3, 5, 0), Trait = rep(c(&quot;A&quot;, &quot;B&quot;), 4), Sexe = rep(c(&quot;F&quot;, &quot;M&quot;), 4)) jeu.freq ## Freq Trait Sexe ## 1 0 A F ## 2 1 B M ## 3 0 A F ## 4 10 B M ## 5 2 A F ## 6 3 B M ## 7 5 A F ## 8 0 B M xtabs(Freq ~ Trait + Sexe, data = jeu.freq) ## Sexe ## Trait F M ## A 7 0 ## B 0 14 Cette fonction possède un argument data qui permet de spécifier où se trouvent les variables pour lesquelles on désire un tableau. La plupart des fonctions de modélisation, comme lm() pour effectuer des modèles de régression, comportent ce même argument pour faciliter la syntaxe. Si aucune variable de fréquence n’apparaît dans le jeu de données, xtabs() aura le même effet que table(). table(jeu.freq$Trait) ## ## A B ## 4 4 xtabs(~ Trait, data = jeu.freq) ## Trait ## A B ## 4 4 table(jeu.freq$Trait, jeu.freq$Sexe) ## ## F M ## A 4 0 ## B 0 4 #comparer avec l&#39;exemple plus haut avec Freq ~ Trait + Sexe xtabs(~ Trait + Sexe, data = jeu.freq) ## Sexe ## Trait F M ## A 4 0 ## B 0 4 Modifier la structure d’un jeu de données On peut convertir un jeu de données du format “long” au format “large”. En présence de mesures répétées, la fonction reshape( ) permet de passer d’un format à l’autre. Considérons, par exemple, un jeu de données qui traite de la concentration en indométhacine (un anti-inflammatoire) dans le plasma sanguin de patients. Le jeu de données original est en format “long”. On peut le convertir en format “large” comme suit: head(Indometh, 15) #15 premières observations - format long ## Subject time conc ## 1 1 0.25 1.50 ## 2 1 0.50 0.94 ## 3 1 0.75 0.78 ## 4 1 1.00 0.48 ## 5 1 1.25 0.37 ## 6 1 2.00 0.19 ## 7 1 3.00 0.12 ## 8 1 4.00 0.11 ## 9 1 5.00 0.08 ## 10 1 6.00 0.07 ## 11 1 8.00 0.05 ## 12 2 0.25 2.03 ## 13 2 0.50 1.63 ## 14 2 0.75 0.71 ## 15 2 1.00 0.70 large &lt;- reshape(data = Indometh, idvar = &quot;Subject&quot;, timevar = &quot;time&quot;, direction = &quot;wide&quot;) #mettre en format large large ## Subject conc.0.25 conc.0.5 conc.0.75 conc.1 conc.1.25 conc.2 conc.3 conc.4 conc.5 conc.6 conc.8 ## 1 1 1.50 0.94 0.78 0.48 0.37 0.19 0.12 0.11 0.08 0.07 0.05 ## 12 2 2.03 1.63 0.71 0.70 0.64 0.36 0.32 0.20 0.25 0.12 0.08 ## 23 3 2.72 1.49 1.16 0.80 0.80 0.39 0.22 0.12 0.11 0.08 0.08 ## 34 4 1.85 1.39 1.02 0.89 0.59 0.40 0.16 0.11 0.10 0.07 0.07 ## 45 5 2.05 1.04 0.81 0.39 0.30 0.23 0.13 0.11 0.08 0.10 0.06 ## 56 6 2.31 1.44 1.03 0.84 0.64 0.42 0.24 0.17 0.13 0.10 0.09 Il est possible de passer du format “large” au format “long” avec la même fonction: long &lt;- reshape(data = large, idvar = &quot;Subject&quot;, timevar = &quot;Time&quot;, v.names = &quot;Concentration&quot;, varying = 2:12, direction = &quot;long&quot;) #mettre en format long long.ord &lt;- long[order(long$Subject), ] #tri selon Subject Dans certains cas, les valeurs d’une variable sont entrées dans différentes colonnes selon les traitements, comme dans le tableau 0.3. La plupart des analyses en R ne pouvant utiliser ce format, il faut convertir le jeu de données en format long afin d’insérer les valeurs de la variable réponse dans une seule colonne. Les fonctions stack() et unstack() permettent de telles manipulations. fert &lt;- data.frame(Temoin = c(6.1, 6.3, 7.1, 8.1), EngraisA = c(5.9, 6.2, 8.2, 8.5), EngraisB = c(5.8, 5.8, 7.3, 7.6), EngraisC = c(5.4, 6.3, 6.9, 7.8)) fert #format large ## Temoin EngraisA EngraisB EngraisC ## 1 6.1 5.9 5.8 5.4 ## 2 6.3 6.2 5.8 6.3 ## 3 7.1 8.2 7.3 6.9 ## 4 8.1 8.5 7.6 7.8 long &lt;- stack(fert) #format long - 1 colonne par variable long ## values ind ## 1 6.1 Temoin ## 2 6.3 Temoin ## 3 7.1 Temoin ## 4 8.1 Temoin ## 5 5.9 EngraisA ## 6 6.2 EngraisA ## 7 8.2 EngraisA ## 8 8.5 EngraisA ## 9 5.8 EngraisB ## 10 5.8 EngraisB ## 11 7.3 EngraisB ## 12 7.6 EngraisB ## 13 5.4 EngraisC ## 14 6.3 EngraisC ## 15 6.9 EngraisC ## 16 7.8 EngraisC unstack(long) #remettre en format large ## Temoin EngraisA EngraisB EngraisC ## 1 6.1 5.9 5.8 5.4 ## 2 6.3 6.2 5.8 6.3 ## 3 7.1 8.2 7.3 6.9 ## 4 8.1 8.5 7.6 7.8 Exporter un jeu de données Alors que read.table() permet d’importer un jeu de données à partir d’un fichier, write.table() permet de sauvegarder un jeu de données (un objet R) dans un fichier. Tout comme son homologue read.table(), elle comporte des arguments pour spécifier le chemin et le nom du fichier, le type de séparateur, l’option d’inclure ou non les en-têtes des colonnes ou les étiquettes des rangées. Illustrons avec le jeu de données fert créé plus haut. ##exporter à un fichier avec séparateur espace write.table(x = fert, file = &quot;/chemin_sur_ordi/fert.txt&quot;, row.names = FALSE, col.names = TRUE, sep = &quot; &quot;) L’argument sep spécifie le type de séparateur entre les valeurs du jeu de données du fichier. Les plus communes sont sep = \" \" pour un espace, sep = \",\" pour la virgule, sep = \"\\t\" pour la tabulation, et sep = \";\" pour le point-virgule. Lors de l’exportation avec write.table(), R met le nom des en-têtes de colonnes, les étiquettes des rangées ainsi que les valeurs de variables catégoriques ou de caractères entre \" \". L’argument quote prend la valeur TRUE par défaut (garde les \" \" dans le fichier). L’importation d’un fichier avec \" \" dans R se fait sans difficulté, mais peut causer des problèmes dans d’autres logiciels. On peut simplement spécifier quote = FALSE afin de remédier à la situation. Une autre option possible est de stocker un objet avec la fonction save(). Alors que write.table() permet d’exporter des fichiers de données (un objet à 2 dimensions), la fonction save() sauvegarde des objets beaucoup plus complexes tout en gardant leur structure intacte. ##on crée un tableau tableau &lt;- table(vers$Vegetation, vers$Pente, vers$Humide) tableau ## , , = FALSE ## ## ## 0 1 2 3 4 5 6 8 10 11 ## Arable 1 0 2 0 0 0 0 0 0 0 ## Chaparral 1 0 0 0 0 0 0 0 1 0 ## Prairie 0 2 2 2 0 0 1 0 0 1 ## Pre 0 0 0 0 0 0 0 0 0 0 ## Verger 1 0 0 0 0 0 0 0 0 0 ## ## , , = TRUE ## ## ## 0 1 2 3 4 5 6 8 10 11 ## Arable 0 0 0 0 0 0 0 0 0 0 ## Chaparral 0 0 0 0 0 0 0 1 1 0 ## Prairie 0 0 0 0 1 0 0 0 0 0 ## Pre 2 0 0 0 0 1 0 0 0 0 ## Verger 0 0 0 0 0 0 0 0 0 0 save(tableau, file = &quot;un.tableau.rdata&quot;) Par exemple, après avoir stocké le résultat d’une analyse dans un objet, il est possible de sauvegarder les résultats dans un fichier. Ce fichier peut-être importé dans R à un moment ultérieur en invoquant la fonction load( ). Chacune de ces deux fonctions nécessite le chemin menant au fichier. ##dans une session ultérieure, on importe l&#39;objet directement load(file = &quot;un.tableau.rdata&quot;) tableau ## , , = FALSE ## ## ## 0 1 2 3 4 5 6 8 10 11 ## Arable 1 0 2 0 0 0 0 0 0 0 ## Chaparral 1 0 0 0 0 0 0 0 1 0 ## Prairie 0 2 2 2 0 0 1 0 0 1 ## Pre 0 0 0 0 0 0 0 0 0 0 ## Verger 1 0 0 0 0 0 0 0 0 0 ## ## , , = TRUE ## ## ## 0 1 2 3 4 5 6 8 10 11 ## Arable 0 0 0 0 0 0 0 0 0 0 ## Chaparral 0 0 0 0 0 0 0 1 1 0 ## Prairie 0 0 0 0 1 0 0 0 0 0 ## Pre 2 0 0 0 0 1 0 0 0 0 ## Verger 0 0 0 0 0 0 0 0 0 0 Nous venons de faire un survol des fonctions de base de R. Nous continuerons à explorer d’autres fonctionnalités de R lors des leçons subséquentes. Un groupe folk alternatif apprécié par l’auteur. Pour plus d’informations, voir (http://fr.wikipedia.org/wiki/16_Horsepower).↩︎ Généralement, choisir un nom qui est déjà utilisé par une fonction de R ne causera pas de problèmes, mais c’est préférable d’éviter cette situation.↩︎ Pour les albums lancés en Amérique du Nord avant la désintégration du groupe en 2005.↩︎ "],["exercices.html", "Exercices", " Exercices Question 1 a. Créez une variable à partir des valeurs suivantes (voir Créer un vecteur) : 2.1 3.2 1.9 5.3 4.5 6.5 4.3 0.7 1.1 4.6 Réponse ##créer variable variable &lt;- c(2.1, 3.2, 1.9, 5.3, 4.5, 6.5, 4.3, 0.7, 1.1, 4.6) b. À partir de la variable que vous venez de créer, utilisez la fonction appropriée pour déterminer le nombre d’observations (voir Connaître les caractéristiques d’un vecteur). Réponse length(variable) ## [1] 10 c. Sélectionnez les observations 1, 5, 8 et stockez-les dans un nouvel objet (voir Sélectionner des valeurs). Réponse nouvel.objet &lt;- variable[c(1, 5, 8)] d. Créez une matrice de 2 rangées et de 5 colonnes. Insérez dans cette matrice les valeurs de la variable créée en a (voir Créer une matrice) en insérant les données de façon à remplir une colonne à la fois, de telle sorte que les deux premières observations se retrouvent dans la première colonne, les deux suivantes dans la deuxième colonne, et ainsi de suite ). Réponse la.matrice &lt;- matrix(data = variable, nrow = 2, ncol = 5) la.matrice ## [,1] [,2] [,3] [,4] [,5] ## [1,] 2.1 1.9 4.5 4.3 1.1 ## [2,] 3.2 5.3 6.5 0.7 4.6 e. Calculez la somme de chaque colonne de la matrice avec la fonction la plus appropriée (voir Opérations et manipulation de matrices). Réponse colSums(la.matrice) ## [1] 5.3 7.2 11.0 5.0 5.7 Question 2 a. Importez le jeu de données contenu dans le fichier richesse.txt (voir Importer des fichiers de données). Réponse ##importer en spécifiant le chemin complet ##si le fichier n&#39;est pas dans le répertoire ##de travail rich &lt;- read.table(&quot;Guide_R/data/richesse.txt&quot;, header = TRUE) b. Créez une nouvelle variable à partir du log naturel (à base \\(e\\)) de la variable Profondeur et ajoutez-la au jeu de données (voir Ajouter une variable). Réponse ##on crée une variable de log de la Profondeur rich$log.Profondeur &lt;- log(rich$Profondeur) c. Modifiez le nom de la variable Profondeur à la.profondeur directement dans le jeu de données (voir Accéder aux variables d’un jeu de données). Indice: vous aurez besoin de sélectionner le nom de la variable que vous voulez remplacer avec [ ]. Réponse names(rich) ## [1] &quot;Profondeur&quot; &quot;Nombre.espece&quot; &quot;Site&quot; &quot;log.Profondeur&quot; names(rich)[1] &lt;- &quot;la.profondeur&quot; names(rich) ## [1] &quot;la.profondeur&quot; &quot;Nombre.espece&quot; &quot;Site&quot; &quot;log.Profondeur&quot; d. Créez une variable binaire à partir de la variable Nombre.espece en assignant la valeur pauvre lorsque la variable a des valeurs \\(\\leq 3\\) et riche lorsque la variable prend des valeurs &gt; 3 (voir Tests logiques). Réponse rich$Nombre.espece.bin &lt;- ifelse(rich$Nombre.espece &lt;= 3, &quot;pauvre&quot;, &quot;riche&quot;) head(rich) ## la.profondeur Nombre.espece Site log.Profondeur Nombre.espece.bin ## 1 20.38067 4 foret 3.014587 riche ## 2 27.07474 0 foret 3.298601 pauvre ## 3 32.58788 4 foret 3.483941 riche ## 4 18.47868 1 foret 2.916618 pauvre ## 5 31.95783 3 foret 3.464417 pauvre ## 6 30.30124 1 foret 3.411189 pauvre e. Créez un sous-jeu de données pour la partie correspondant aux sites de type champ et stockez-le dans un objet (voir Créer des sous-ensembles). Réponse ##avec [ , ] champ &lt;- rich[rich$Site == &quot;champ&quot;, ] ##ou avec subset champ &lt;- subset(rich, subset = rich$Site == &quot;champ&quot;) head(champ) ## la.profondeur Nombre.espece Site log.Profondeur Nombre.espece.bin ## 16 26.92344 2 champ 3.292997 pauvre ## 17 20.46983 1 champ 3.018952 pauvre ## 18 23.51757 1 champ 3.157748 pauvre ## 19 42.24314 3 champ 3.743442 pauvre ## 20 31.99812 3 champ 3.465677 pauvre ## 21 24.21516 3 champ 3.186979 pauvre f. Exportez le sous-jeu de données créé en e dans un fichier (voir Exporter un jeu de données). Réponse ##séparateur tabulation write.table(champ, file = &quot;Guide_R/data/champ.txt&quot;, sep = &quot;\\t&quot;, row.names = FALSE, col.names = TRUE) ##séparateur espace write.table(champ, file = &quot;Guide_R/data/champ.txt&quot;, sep = &quot; &quot;, row.names = FALSE, col.names = TRUE) ##séparateur point-virgule write.table(champ, file = &quot;Guide_R/data/champ.txt&quot;, sep = &quot;;&quot;, row.names = FALSE, col.names = TRUE) "],["statistiques-descriptives.html", "Module 1 Statistiques descriptives", " Module 1 Statistiques descriptives Le contenu théorique de chaque leçon est présenté dans un document comme celui-ci et intègre souvent du code en langage de programmation R. Nous sommes conscients que la convention en français est d’utiliser la virgule pour indiquer la décimale. Toutefois, nous utilisons systématiquement le point pour désigner la décimale dans le texte. Ce choix vient du fait que la syntaxe de R utilise le point comme décimale - à la fois pour la saisie de valeurs numériques et pour l’affichage des sorties d’analyses. Ainsi, l’usage du point uniformisera le texte et nous sommes d’avis que cette décision facilitera sa compréhension. À quoi servent les statistiques Les statistiques constituent une partie importante de la démarche scientifque. Elles s’appliquent à des domaines aussi variés que l’écologie, le génie, la psychologie, la médecine, et les sciences sociales. Les objectifs de l’analyse statistique sont les suivants : Estimer des paramètres. Par exemple, la question “Quel est le taux d’obésité au Québec ?” est un problème d’estimation. C’est-à-dire que nous cherchons à trouver la vraie valeur de cette densité. Tester des hypothèses. Par exemple, en posant la question “Les ours des aires protégées du Québec ont-ils une plus grande masse que leurs homologues à l’extérieur des aires protégées ?” Ici, nous voulons déterminer si la différence observée dans les deux groupes est due au hasard ou à un réel effet. Inférer les résultats dans un contexte plus général (où inférer signife “tirer des conclusions”). C’est ce que font, par exemple, régulièrement les maisons de sondage d’opinions politiques auprès de la population. Faire des prédictions. La prédiction consiste à construire un modèle et l’utiliser pour prédire le comportement d’une variable d’intérêt. On peut utiliser la prédiction en médecine, météorologie ou en toxicologie, par exemple. Définissons maintenant certains termes et concepts en statistique. "],["théories.html", "1.1 Théories", " 1.1 Théories 1.1.1 Paramètre vs statistique Le paramètre est un concept important. Il désigne une valeur numérique inconnue qui caractérise une population d’intérêt. Par exemple, la taille moyenne en cm des résidents de l’île de Montréal est une valeur inconnue (mais qui existe). C’est-à-dire qu’il serait possible de calculer cette valeur si on mesurait chaque individu de cette région. Un paramètre est habituellement représenté par une lettre grecque (\\(\\mu\\), \\(\\sigma\\)). Si la taille moyenne des résidents était de 1.91 m, on écrirait \\(\\mu\\) = 1.91 m. À l’opposé, une statistique est une quantité qui peut être calculée à partir des données d’un échantillon. Par exemple, si nous désirons calculer la taille moyenne des résidents de l’île de Montréal, nous pourrions le faire en mesurant la taille de 100 résidents de l’île. Une statistique est normalement désignée par une lettre romaine (s, sd, \\(\\bar{x}\\)). La population statistique est l’ensemble des éléments sur lesquels on veut baser nos conclusions (taille des résidents de Montréal). On ne connaît pas la taille moyenne des gens de cette population. Il existe deux options afin d’obtenir de l’information sur cette moyenne: mesurer la taille de chaque résident de Montréal (peu pratique et logistiquement difficile); utiliser un échantillon construit à partir de tailles d’individus sélectionnés aléatoirement dans la population de Montréal. Pour faire une analogie, l’échantillon est à la population, ce que la statistique est au paramètre. Si nous poursuivons avec notre exemple d’échantillon de 100 résidents de Montréal (100 observations), la valeur numérique obtenue constituera une estimation de la taille moyenne (\\(\\mu\\)) des résidents de Montréal. L’estimation est une valeur possible que peut prendre un paramètre. Pour récapituler, on infère sur la population à partir d’un échantillon. Si la moyenne de l’échantillon est de 1.7 m (\\(\\bar{x}\\) = 1.7 m), on peut dire que 1.7 est une estimation de la moyenne de la population \\(\\mu\\). Bref, on peut tirer des conclusions sur la population à partir d’un échantillon qui provient de cette même population. Afin de faire une bonne estimation, l’échantillon doit être aléatoire et représentatif de la population. Dans un échantillon aléatoire, chaque élément de la population a une chance égale d’être inclus dans l’échantillon. Si on sélectionne aléatoirement 100 résidents de Montréal pour estimer la taille moyenne des individus dans la population et que, par malchance, tous les résidents sélectionnés proviennent du même quartier, l’échantillon ne sera pas représentatif de la population. 1.1.2 Mesures de la tendance centrale Certaines mesures décrivent la valeur autour de laquelle se concentrent la plupart des observations d’un échantillon ou d’une population. On parle alors de tendance centrale ou de paramètres de position. On peut estimer ces paramètres à partir d’un échantillon. La moyenne arithmétique est un exemple de ce genre de mesure: \\[ \\bar{x} = \\frac{\\sum\\limits_{i=1}^nx_i}{n} \\] où \\(x_i\\) correspond à la valeur \\(i\\) de la variable \\(x\\) et \\(n\\) correspond au nombre d’observations. À noter que \\(\\Sigma\\) (la lettre grecque sigma) indique la somme de toutes les observations de \\(i = 1\\) jusqu’à \\(n\\). De façon plus générale, on appelle estimateur une formule ou équation utilisée pour estimer une certaine valeur, alors que l’estimation est le résultat de l’estimateur. Lors d’une expérience sur la hauteur de semis sur un sol argileux après une saison de croissance, on obtient les valeurs suivantes en cm: 12.3, 4.2, 5.9, 9.1, 3.3, 5.1, 7.3, 3.8, 8.0, 6.1. Le calcul de la moyenne arithmétique se fait comme suit: \\[ \\bar{x} = \\frac{\\sum\\limits_{i=1}^nx_i}{n} = \\frac{12.3 + 4.2 + 5.9 + \\ldots + 8.0 + 6.1}{10} \\\\ \\bar{x} = 6.51 \\] Ainsi, la moyenne arithmétique de cet échantillon est de 6.51 cm. L’estimateur de \\(\\bar{x}\\) est un estimateur non-biaisé de \\(\\mu\\) si: les observations sont effectuées sur des individus sélectionnés aléatoirement; les observations sont indépendantes; les observations de la variable décrivant la population suivent une distribution normale. La moyenne géométrique est une autre mesure de tendance centrale, particulièrement appropriée pour décrire des processus multiplicatifs4. Un processus multiplicatif est un effet qui ou en présence de valeurs extrêmes: \\[ \\bar{x}_{g\\acute{e}om} = \\sqrt[n]{\\prod_{i=1}^n x_i} \\\\ \\bar{x}_{g\\acute{e}om} = e^\\frac{{\\sum\\limits_{i=1}^n \\log(x_i)}}{n} \\] Disons qu’après un décompte d’insectes sur 5 quadrats5 dans un champ agricole, on observe les abondances suivantes: 10, 1, 1000, 1, 10. \\[ \\bar{x}_{g\\acute{e}om} = \\sqrt[5]{10 \\cdot 1 \\cdot 1000 \\cdot 1 \\cdot 10} \\\\ \\bar{x}_{g\\acute{e}om} = 10 \\] La moyenne géométrique de ces valeurs nous donne 10, alors que la moyenne arithmétique nous donne 204.4. La valeur 1000 se démarque nettement des autres et exerce une influence démesurée sur la moyenne arithmétique, et dans ce cas, la moyenne géométrique est un meilleur estimateur de la tendance centrale. La moyenne harmonique peut s’appliquer à des taux (p. ex., vitesses): \\[ \\bar{x}_{harm} = \\frac{n}{\\sum\\limits_{i=1}^n \\frac{1}{x_i}} \\] Par exemple, disons que nous avons suivi un ours noir par télémétrie. L’ours a parcouru un segment de 2 km à une vitesse de 1 km/h, un deuxième segment de 2 km à une vitesse de 2 km/h, un troisième segment de 2 km à une vitesse de 4 km/h et un dernier segment de 2 km à une vitesse de 1 km/h. Quelle est la vitesse moyenne de l’ours? On pourrait utiliser la moyenne harmonique pour résoudre le problème. Sachant que \\(vitesse = distance/temps\\), nous pouvons déterminer la distance totale parcourue: \\(4 * 2 \\: \\mathrm{km} = 8 \\:\\mathrm{km}\\). On peut ensuite évaluer le temps mis à parcourir ces 8 km: 1\\(^{\\mathrm{er}}\\) segment: 2 km * 1h/km = 2 h 2\\(^{\\mathrm{e}}\\) segment: 2 km * 1h/2 km = 1 h 3\\(^{\\mathrm{e}}\\) segment: 2 km * 1h/4 km = 0.5 h 4\\(^{\\mathrm{e}}\\) segment: 2 km * 1h/1 km = 2 h Le temps total est 5.5 h. Nous pouvons calculer la vitesse moyenne: \\[ \\mathrm{vitesse \\:moyenne} = 8 \\:\\mathrm{km}/5.5 \\:\\mathrm{h} \\\\ \\mathrm{vitesse \\:moyenne} = 1.45 \\:\\mathrm{km/h} \\] C’est exactement ce que nous donne la moyenne harmonique: \\[ \\bar{x}_{harm} = \\frac{4}{1/1 + 1/2 + 1/4 + 1/1} \\\\ \\bar{x}_{harm} = 1.45 \\] À noter que la moyenne harmonique s’applique à des vitesses si elles sont mesurées sur une même distance. Si les distances diffèrent, nous devrons utiliser une version pondérée de la moyenne harmonique. Les trois types de moyennes sont reliées par la relation suivante: \\[ \\bar{x}_{harmonique} &lt; \\bar{x}_{g\\acute{e}om} &lt; \\bar{x} \\] Si les observations sont égales (\\(x_1 = x_2 = x_3 \\ldots = x_n\\)), nous obtenons: \\[ \\bar{x}_{harmonique} = \\bar{x}_{g\\acute{e}om} = \\bar{x} \\] Il existe d’autres mesures de tendance centrale, notamment la médiane qui se définit comme étant la valeur qui sépare les observations en deux groupes égaux (50 % des valeurs &lt; médiane, 50 % des valeurs &gt; médiane). En présence de données normales, la médiane et la moyenne sont proches. La médiane est peu influencée par la présence de valeurs extrêmes (valeurs très grandes ou très faibles), alors que la moyenne est très sensible à la présence de valeurs extrêmes. Dans une expérience sur le temps de survie d’insectes exposés à un insecticide, on obtient les valeurs (en secondes) 1.1, 1.2, 1.3, 1.6, 3.2, 2.4, 5.2. La moyenne arithmétique de cet échantillon est de 2.29 secondes et la médiane est de 1.6 secondes. Si l’on ajoute une dernière observation dont la valeur extrême est 40 secondes, la moyenne arithmétique sera alors de 7 secondes et la médiane de 2 secondes. On constate que la médiane est beaucoup moins sensible à l’ajout de la valeur extrême, ce qui n’est pas le cas de la moyenne arithmétique. Figure 1.1: Histogramme illustrant une distribution unimodale (a) et bimodale (b). Le mode permet aussi de caractériser la tendance centrale, car il donne la ou les valeurs qui reviennent le plus souvent dans l’échantillon (Figure 1.1). Par exemple, si, dans un échantillon, on obtient les valeurs 12, 12, 12, 12, 12, 3, 3, 3, 3, 3, 3, 1, 2, 14, 15, 16, 21, 32, on dira qu’il y a deux modes (12 et 3). 1.1.3 Mesures de dispersion Certaines mesures décrivent plutôt l’étendue de la variabilité des données. On parle alors de mesures de dispersion ou de paramètres de variabilité. Plus la variabilité augmente, plus l’incertitude quant à la valeur des paramètres estimés à partir de données d’un échantillon augmente. Un niveau d’incertitude plus élevé augmente la difficulté de trouver des différences et de tester des hypothèses. L’étendue (range) est la mesure de dispersion la plus simple. Il s’agit de la différence entre la valeur minimale et la valeur maximale des observations. 1.1.3.1 Somme des carrés des erreurs La somme des carrés des erreurs (sum of squared errors, SSE) donne le carré de la différence entre chaque observation et la moyenne de l’échantillon: \\[ SSE = \\sum_{i=1}^n (x_i - \\bar{x})^2 \\] Cette mesure de variabilité est l’une des plus communes, et peut prendre des valeurs ≥≥ 0 (le carré assure des valeurs positives). Plus cette valeur est grande, plus il y a de variabilité dans les données (i.e., les observations sont plus éloignées de la moyenne). Un échantillon de 6 longueurs de tige d’une plante ligneuse donne 1.3 m, 4.5 m, 4.1 m, 2.1 m, 5.0 m, et 1.9 m, il s’ensuivra que \\(\\bar{x}\\) = 3.15 m et que \\(SSE=(1.3−3.15)2+(4.5−3.15)2+…+(1.9−3.15)2=12.24m^2\\). Une propriété importante de la SSE est qu’à chaque nouvelle observation ajoutée, elle augmente (pourvu que \\(x_{nouvelle}\\neq \\bar{x}\\)). Si on ajoute une septième valeur de 2.6 à notre échantillon de longueurs de tige présenté ci-haut, la moyenne arithmétique devient 3.07 et la SSE s’élèvera à 12.49 \\(m^2\\). Une meilleure mesure de dispersion devrait tenir compte de la taille de l’échantillon. Mais avant d’aller plus loin, allons visiter le concept de degrés de liberté (degrees of freedom, df), un concept souvent nébuleux que nous tenterons d’éclaircir ici. On peut voir les degrés de liberté comme étant la taille de l’échantillon corrigée pour le nombre de paramètres estimés.On peut obtenir cette valeur en soustrayant le nombre de paramètres estimés p de la taille d’échantillon n (i.e., n − p). Clarifions avec un exemple. Imaginez qu’on ait un échantillon de 5 observations dont on ne connait rien. Ces 5 observations pourraient prendre n’importe quelle valeur. Le degré de liberté est donc 5 (df = 5). Imaginez maintenant qu’on connaisse un paramètre de cet échantillon (p. ex., \\(\\bar{x}\\) = 7). On réduit la liberté des valeurs que peuvent prendre ces 5 observations. En effet, disons que les valeurs aient été ndéterminées pour 4 des observations et que la moyenne est connue, la dernière observation est obligée de prendre une valeur en particulier. Avec un paramètre connu, le degré de liberté est donc 4 (df = 5 − 1 = 4). 1.1.3.2 Carré moyen (variance) Comme nous l’avons mentionné plus tôt, la somme des carrés des erreurs (\\(SSE\\)) augmente avec la taille de l’échantillon. Une meilleure mesure devrait tenir compte de la taille d’échantillon. Le carré moyen(mean square, mean squared error, MSE) est une telle mesure de dispersion : \\[ MSE = \\frac{SSE}{df} = \\frac{\\sum\\limits_{i=1}^n (x_i - \\bar{x})^2}{n - 1} \\] À noter que le dénominateur correspond aux degrés de liberté, ici \\(n - 1\\), puisque nous avons estimé la moyenne arithmétique \\(\\mu\\) à l’aide de \\(\\bar{x}\\) pour trouver la \\(SSE\\). Ce carré moyen est en fait la variance de l’échantillon, \\(s^2 = MSE\\). Cette relation est importante et nous reviendrons sur cette notion lors de la leçon sur l’analyse de variance. On peut donc estimer la variance de la population à partir d’un échantillon en utilisant l’équation : \\[ s^2 = MSE = \\frac{SSE}{df} \\\\ s^2 = \\frac{\\sum\\limits_{i=1}^n (x_i - \\bar{x})^2}{n - 1} \\] Parfois, on utilise aussi la formule alternative (mais totalement équivalente): \\[ s^2 = \\frac{\\sum\\limits_{i=1}^n x_i^2 - \\frac{\\left (\\sum\\limits_{i=1}^n x_i\\right )^2}{n}}{n - 1} \\] L’écart-type (\\(s\\)) est simplement la racine carrée de la variance, et il indique la variabilité dans les données. La variance dépend énormément de la taille de l’échantillon. L’estimation devient difficile lorsqu’on a peu d’observations dans l’échantillon. Illustrons avec un exemple. Utilisons une petite simulation à l’aide de R pour générer des données provenant d’une population avec des caractéristiques connues, soit une population normale avec une moyenne de 10.1 (\\(\\mu = 10.1\\)) et une variance de 4 (\\(\\sigma^2 = 4\\)). Nous allons sélectionner aléatoirement trois observations provenant de cette population afin de constituer un échantillon de \\(n = 3\\). À partir de cet échantillon, nous pouvons calculer une variance qui sera une estimation de la vraie valeur. Nous estimerons \\(\\sigma\\) à l’aide de l’estimateur de la variance (\\(s\\)) d’un échantillon. Afin d’obtenir une meilleure idée de la performance de l’estimation de la variance, nous allons ensuite répéter l’exercice pour 29 autres échantillons de \\(n = 3\\) tirés de la même population, et calculer la variance de chaque échantillon de taille 3. Par la suite, nous ferons de même pour 30 échantillons constitués de 4 observations, 30 échantillons de 5 observations, …, 30 échantillons de 49 observations et 30 échantillons de 50 observations (fig. \\(\\ref{Figure:variance}\\)). On remarque que l’estimation de la variance est parfois très loin de la vraie valeur de 4, particulièrement pour les très petits échantillons (\\(n \\leq 10\\)). On obtient de meilleures estimations pour de plus grands échantillons, particulièrement au-delà de 30. C’est une des raisons pour laquelle on considère un échantillon de 30 observations comme ayant une taille suffisante – il permet de bien estimer la variance. On comprend rapidement que l’utilisation d’un petit échantillon peut nous amener loin de la vraie valeur de la variance. Mais pourquoi s’intéresser autant à la variance? Figure 1.2: Effet du nombre d’observations sur l’estimation de la variance. À noter que la ligne pointillée représente la vraie valeur de la population (\\(\\sigma^2 = 4\\)) à partir de laquelle les observations ont été sélectionnées aléatoirement La variance est une quantité importante en statistiques, puisqu’elle est requise pour construire des mesures de précision (p. ex., intervalles de confiance) et pour tester des hypothèses (p. ex., test t). Un petit échantillon peut produire une estimation très loin de la vraie valeur de la variance et invalider les conclusions d’une analyse statistique. Tel qu’illustré dans l’exemple 1.7, l’estimation de la variance s’améliore avec la taille de l’échantillon. Ce qui nous mène à visiter les concepts de précision et d’exactitude. 1.1.3.3 Précision vs exactitude La réalisation d’une expérience, impliquant l’échantillonnage des observations et l’estimation des quantités, s’apparente à un archer qui lance une flèche sur une cible, où la flèche correspond à une expérience et le point sur la cible correspond à une estimation. On veut que la flèche se rende le plus près du centre de la cible (c.-à-d., une bonne estimation), mais on veut que les flèches ne soient pas trop éloignées les unes des autres (c.-à-d., une bonne précision). En d’autres termes, un archer est précis si toutes ses flèches tombent très près du même point sur la cible (fig. \\(\\ref{Figure:accuracy}\\)a, c), ou encore il peut manquer d’exactitude lorsque ses flèches sont loin du centre de la cible (fig. \\(\\ref{Figure:accuracy}\\)c, d). Le meilleur des scénarios est un tir précis et exact (fig. \\(\\ref{Figure:accuracy}\\)a), et le pire est un tir ni précis, ni exact (fig. \\(\\ref{Figure:accuracy}\\)d). Le tir exact mais peu précis implique que l’estimation varie beaucoup d’un échantillon à l’autre (fig. \\(\\ref{Figure:accuracy}\\)b), et cette variation n’est pas souhaitable. Figure 1.3: Utilisation de cibles pour expliquer le concept de précision et d’exactitude avec quatre archers dans une compétition. Si tous les points se trouvent au centre et très près les uns des autres, l’archer est précis et exact (a), alors que si les points sont dans la région centrale mais éloignés les uns des autres, l’archer est exact mais peu précis (b). À l’opposé, si les points sont très près les uns des autres et loin du centre, l’archer est précis mais manque d’exactitude (c), tandis que dans le dernier scénario l’archer n’est ni précis ni exact (d). L’exactitude peut être vue comme un terme qualitatif. La valeur qui quantifie la déviation entre les estimations et la valeur réelle du paramètre s’appelle biais. Plus formellement, on appelle biais la différence entre la *8{valeur attendue d’une estimation et la valeur réelle** qu’on désire estimer : \\[ biais = E(\\hat{\\theta}) - \\theta \\] où \\(\\theta\\) est la valeur réelle du paramètre de la population, \\(\\hat{\\theta}\\) correspond à l’estimation d’un paramètre obtenu avec un seul jeu de données, et \\(E(\\hat{\\theta})\\) est la valeur attendue des estimations du paramètre. La valeur attendue est en fait la moyenne d’une série d’estimations \\(\\hat{\\theta}\\) obtenues à partir de plusieurs échantillons de taille égale provenant de la même population. Pour poursuivre notre analogie des archers, la valeur attendue correspond à la moyenne des positions des flèches sur la cible. Le biais exprime la tendance des différences entre les valeurs estimées d’un paramètre et la vraie valeur de ce paramètre. Lorsque le biais est de 0, on dit que l’estimateur est non biaisé (p. ex., l’estimateur de la moyenne arithmétique, \\(\\bar{x}\\), sous certaines conditions). Si on développait une mesure d’imprécision (\\(1/pr\\acute{e}cision\\)), on s’attendrait à ce qu’elle augmente proportionellement (\\(\\propto\\)6) avec la variance : \\[ impr\\acute{e}cision \\propto s^2 \\] Par contre, on s’attendrait à ce que l’imprécision diminue avec la taille de l’échantillon : \\[ impr\\acute{e}cision \\propto \\frac{s^2}{n} \\] Une mesure idéale s’exprimerait dans les mêmes unités que les observations : \\[ impr\\acute{e}cision \\propto \\sqrt{\\frac{s^2}{n}} \\] Une telle mesure existe déjà, c’est l’erreur-type de la moyenne (\\(SE\\)) : \\[ SE = s_{\\bar{x}} = \\sqrt{\\frac{s^2}{n}} \\] L’erreur-type de la moyenne d’un échantillon (\\(s_{\\bar{x}}\\) ou \\(SE\\)) représente l’écart-type de la distribution des moyennes calculées à partir d’échantillons de taille identique à celle de notre échantillon. Ainsi, l’erreur-type de la moyenne nous donne une indication sur la variabilité de l’estimation de ce paramètre si on répétait l’échantillonnage. Pour clarifier, l’écart-type nous informe sur la variabilité d’un échantillon alors que l’erreur-type de la moyenne nous indique la précision avec laquelle nous avons estimé ce paramètre. Nous revisiterons ce concept lors des deux dernières leçons consacrées aux modèles de régression. L’erreur-type nous permet de calculer des intervalles de confiance autour de la moyenne ou d’autres paramètres. L’intervalle de confiance est justement une autre mesure de précision autour d’une estimation. L’intervalle de confiance (\\(IC\\)) se définit comme étant l’intervalle à l’intérieur duquel se trouvera la moyenne de la population \\(\\mu\\) si l’on répète l’expérience un grand nombre de fois. Pour un \\(IC\\) à 95 %, \\[ P(\\bar{x} - 1.96 \\cdot SE \\leq \\mu \\leq \\bar{x} + 1.96 \\cdot SE) = 0.95 \\] Un \\(IC\\) à 95 % indique que la moyenne de la population (\\(\\mu\\)) devrait se trouver 95 % du temps (c.-à-d., la probabilité est de 0.95) à l’intérieur de l’intervalle si on répète l’échantillonnage à plusieurs reprises avec le même nombre d’observations. Pour un \\(IC\\) donné, la moyenne de la population \\(\\mu\\) est incluse ou non 7. L’\\(IC\\) est construit à partir d’un échantillon, mais concerne la moyenne \\(\\mu\\) de la population. Nous expliquerons en détail la construction et l’interprétation d’intervalles de confiance à la prochaine leçon(voir aussi la section 7 de la présente leçon pour comprendre l’origine du facteur 1.96). Pour l’instant, il suffit de réaliser qu’on peut utiliser cette intervalle pour en indiquer la précision. 1.1.3.4 Autres mesures de dispersion Le **coefficient de variation*}** (coefficient of variation, CV) est parfois utilisé pour représenter la variabilité: \\[ CV = \\frac{s}{\\bar{x}} \\cdot 100 \\: \\% \\] où \\(s\\) représente l’écart-type de l’échantillon et \\(\\bar{x}\\) correspond à la moyenne arithmétique de l’échantillon. On remarque que le \\(CV\\) est le ratio entre l’écart-type et la moyenne arithmétique. Un échantillon avec un CV de 14 % varie moins qu’un autre avec un \\(CV\\) de {30~%}. Les quantiles peuvent également nous aider à représenter à quel point les données varient. Le mot “quantile” est un terme générique qui désigne une quantité qui divise les données en compartiments après qu’elles ont été mises en ordre croissant. Les quartiles divisent les données en quatre compartiments, les déciles en 10 compartiments, et les percentiles en 100 compartiments. Par exemple, un 90\\(^{\\mathrm{e}}\\) percentile de 120 g signifie que 90 % des valeurs sont inférieures à 120 g et que 10 % des valeurs sont supérieures à 120 g. 1.1.4 Variables aléatoires On désigne variable aléatoire une variable dont les valeurs observées sont considérées comme résultant d’un processus aléatoire (c.-à-d., expérience aléatoire). Autrement dit, les valeurs exactes d’une variable aléatoire dans un échantillon ne peuvent être anticipées avec certitude avant de recueillir l’échantillon que nous utiliserons pour tirer des conclusions à propos de la population (p. ex., estimer un paramètre). Le tout implique une composante aléatoire. Par exemple, si on mesure la pression artérielle, le niveau de cholestérol, et le niveau d’activité (trois variables aléatoires) chez un groupe de gens sélectionnés aléatoirement, on ne peut prédire la valeur de ces trois variables chez un individu avant de les avoir mesurées. Les variables aléatoires peuvent être discrètesou continues. Par discrètes, on entend des variables binaires (p. ex., présence-absence, mort-vivant), catégoriques ordonnées ou non (p. ex., petit, moyen, grand; poisson, invertébré, mammifère) ou encore des variables apparaissant sous forme d’entiers (le nombre d’interruption de courant dans 5 municipalités depuis le dernier mois: 0, 1, 12, 4). Le cas échéant, la valeur peut uniquement prendre des valeurs entières – on ne peut avoir dénombré 2.4 individus dans un quadrat ou avoir un individu au 3/4 mort. Les variables continues sont celles qui peuvent prendre une infinité de valeurs sur un intervalle donné. La distance, la masse, le temps, la température, la longueur sont des variables pouvant être mesurées avec différentes résolutions, selon l’instrument utilisé pour effectuer la mesure. Par exemple, mesurons un serpent avec trois différents instruments – un pied à coulisse au mm près, une règle graduée au cm près et un ruban à mesurer au dm près. Le serpent a une longueur finie, mais chacun de nos instruments donnera une mesure différente. Le serpent ne changera pas de longueur entre les trois mesures, mais chaque instrument exprimera un certain nombre de chiffres significatifs. La présentation des valeurs d’une variable peut aussi varier selon le type de variable. Nous utilisons habituellement un diagramme à bâtons pour une variable discrète, alors qu’un histogramme illustre mieux les données d’une variable continue (Figure 1.4). Figure 1.4: Présentation des longueurs de 100 serpents (variable continue, a) et du nombre de pucerons dans 100 sites (variable discrète, b) 1.1.5 Loi des grands nombres et théorème de la limite centrale Deux principes importants agissent sur l’échantillonnage (et les échantillons) et nous permettent d’analyser les données. La loi des grands nombres stipule que la moyenne de l’échantillon (\\(\\bar{x}\\)) tend vers la moyenne de la population (\\(\\mu\\)) au fur et à mesure que la taille de l’échantillon augmente. D’où l’importance d’une bonne taille d’échantillon. Le théorème de la limite centrale, quant à lui, indique que, si on prend plusieurs échantillons indépendants d’une même population, et que l’on calcule la moyenne (ou somme) de chacun, ces moyennes (ou sommes) auront une distribution normale (voir prochaine section). Grace à ce théorème, on peut effectuer des analyses à partir d’un échantillon, même si on ne connaît pas les propriétés de la population originale. On ne peut pas généralement déterminer la normalité d’une population sans l’avoir recensée au complet. Toutefois, on peut le faire pour un échantillon qui provient de cette même population. 1.1.6 Distribution normale La distribution normale (ou loi normale) est une distribution théorique centrale en statistique à la base de nombreux traitements statistiques. Découverte initialement par le mathématicien Abraham De Moivre au 17\\(^{\\mathrm{e}}\\) siècle et redécouverte par Karl Friedrich Gauss 100 ans plus tard, elle a été longtemps désignée sous le nom de distribution gaussienne. On connaît bien les propriétés de la distribution normale et plusieurs approches utilisent cette distribution : les tests d’hypothèses ; l’estimation de paramètres par maximum de vraisemblance ; la construction d’intervalles de confiance. La distribution normale se définit par la fonction de densité de probabilité (probability density function, pdf) suivante: $$ f(x , ) = e{-()2} = $$ où \\(x\\) correspond à la valeur numérique d’intérêt, \\(\\mu\\) représente la moyenne de la population, \\(\\sigma\\) est l’écart-type de la population, \\(\\pi\\) est la constante 3.14159… et \\(e\\) est la constante 2.71828…. Cette distribution comporte deux paramètres, \\(\\mu\\) et \\(\\sigma\\) et on représente parfois cette distribution avec la notation N(\\(\\mu\\), \\(\\sigma\\)). En mots, l’équation nous donne la densité de probabilité d’une variable qui prend la valeur \\(x\\) et qui provient d’une distribution normale avec une moyenne \\(\\mu\\) et un écart-type \\(\\sigma\\). On peut interpréter la densité de probabilité comme on le ferait pour une fréquence relative d’un histogramme. La densité de probabilité de la distribution normale ne correspond pas à la probabilité d’observer \\(X = x\\). La raison de cette interprétation plus complexe provient du fait que la probabilité d’observer une valeur spécifique (p. ex., comme la masse ou la longueur) dans une distribution continue est de 0. La valeur mesurée d’une variable continue est en réalité un intervalle qui dépend de la précision de l’instrument de mesure au mg près, au g près, ou au kg près. Pour un serpent dont la mesure obtenue est de 102.54 cm, on obtient : \\[ P(x = 102.54) = P(102.54 \\leq x \\leq 102.54) = \\int_{102.54}^{102.54} f(x) dx = 0 \\] Si la règle utilisée pour mesurer le serpent donne une précision de \\(\\pm\\) 0.01 cm, notre mesure est en fait un intervalle défini par les bornes suivantes : \\[ \\mathrm{borne \\: inf\\acute{e}rieure} = 102.54 \\: \\mathrm{cm} - 0.01 \\: \\mathrm{cm} = 102.53 \\: \\mathrm{cm}\\\\ \\mathrm{borne \\: sup\\acute{e}rieure} = 102.54 \\: \\mathrm{cm} + 0.01 \\: \\mathrm{cm} = 102.55 \\: \\mathrm{cm} \\] La fonction de densité nous donne la densité de la distribution correspondant à la valeur \\(x\\). On peut obtenir la courbe de distribution normale pour une moyenne et écart-type donnés en substituant une série de valeurs dans l’équation. On veut connaître la densité de probabilité associée à une masse de souris de 3.4 g dans une population de souris suivant une distribution normale ayant une moyenne de 4.1 g et un écart-type de 1.5 g (c.-à-d., N(4.1, 1.5)). Nous avons donc : \\[ \\begin{aligned} x &amp;= 3.4 \\: \\mathrm{g} &amp; f(x = 3.4 \\mid 4.1, 1.5) &amp;= \\frac{1}{1.5 \\cdot \\sqrt{2\\pi}}e^{-\\frac{1}{2}\\left(\\frac{3.4 - 4.1}{1.5}\\right)^2} = 0.2385 \\\\ \\mu &amp;= 4.1 \\: \\mathrm{g} &amp; &amp; \\\\ \\sigma &amp;= 1.5 \\: \\mathrm{g} &amp; \\: . \\end{aligned} \\] On pourrait aussi déterminer la densité de probabilité associée à des souris de 8.9~g et de 10.2 g dans la même population : Souris de 8.9 g: \\[ f(x = 8.9 \\vert 4.1, 1.5) = \\frac{1}{1.5 \\cdot \\sqrt{2\\pi}}e^{-\\frac{1}{2}(\\frac{8.9 - 4.1}{1.5})^2} = 0.0016 \\] Souris de 10.2 g: \\[ f(x = 10.2 \\vert 4.1, 1.5) = \\frac{1}{1.5 \\cdot \\sqrt{2\\pi}}e^{-\\frac{1}{2}(\\frac{10.2 - 4.1}{1.5})^2} = 0.00007 \\] On peut ensuite représenter ces valeurs sur une distribution normale avec \\(\\mu = 4.1\\) et \\(\\sigma = 1.5\\). La figure 1.5 illustre la densité de probabilité pour les trois souris. La courbe a été obtenue en utilisant l’équation de la densité pour une moyenne de 4.1 et un écart-type de 1.5 et en faisant varier \\(x\\) dans l’intervalle de 0 à 12. On remarque que la distribution est plus “dense” dans la région de 2 à 6 g. Par conséquent, on constate que les valeurs dans cet intervalle sont plus probables que des valeurs à l’extérieur de cet intervalle. Nous avons donc plus de chance d’observer une valeur de 3.4 g dans cette population que des valeurs de 8.9 ou 10 g. Figure 1.5: Distribution normale d’une population de masses de souris où la moyenne est de 4.1 g et l’écart-type est de 1.5 g. À noter qu’on peut tracer la courbe de la distribution en substituant une série de valeurs de \\(x\\) dans la fonction de densité de probabilité pour un \\(\\mu = 4.1\\) et \\(\\sigma = 1.5\\), c.-à-d., N(4.1, 1.5). La distribution normale comprend deux paramètres, \\(\\mu\\) et \\(\\sigma\\), ce qui signifie que l’on peut tracer une courbe normale dès que nous connaissons ces deux valeurs. La moyenne (\\(\\mu\\)) détermine la position (fig. 1.6a) et la variance détermine la forme de la courbe (Figure 1.6b). Figure 1.6: Position de distributions normales avec même variance, mais différentes moyennes (a) et forme de distributions normales pour des variances différentes, mais une même moyenne (b). 1.1.6.1 Caractéristiques de la distribution normale La distribution normale est une distribution continue dans l’intervalle \\([-\\infty, +\\infty]\\). La somme de l’aire sous la courbe est 1. La distribution est symétrique autour de la moyenne \\(\\mu\\). On sait que: 90 % des observations se trouvent à 1.64\\(\\sigma\\) de \\(\\mu\\); 95 % des observations se trouvent à 1.96\\(\\sigma\\) de \\(\\mu\\); 99 % des observations se trouvent à 2.58\\(\\sigma\\) de \\(\\mu\\). 1.1.6.2 Distribution normale centrée réduite La distribution normale centrée réduite (standard normal distribution) est un cas particulier de la distribution normale où \\(\\mu = 0\\) et \\(\\sigma = 1\\) (Figure 1.7). Centrer consiste à soustraire la moyenne de chaque observation, \\(x_{i \\: centr\\acute{e}e} = x_i - \\mu\\). L’opération n’influence pas la variance, mais les observations centrées ont une moyenne de 0. Centrer et réduire, parfois aussi connu sous le terme standardiser, consiste à diviser chaque observation centrée par l’écart-type de l’échantillon, \\(x_{i \\: centr\\acute{e}e \\: r\\acute{e}duite} = \\frac{x_i - \\mu}{\\sigma}\\). Les observations centrées réduites ont une moyenne de 0 et une écart-type de 1. L’opération de centrer réduire est aussi appelée la transformation \\(z\\) ou l’écart normal, \\(z = \\frac{x_i - \\mu}{\\sigma}\\). Cette opération modifie l’échelle de la variable. La variable centrée réduite est exprimée en terme du nombre d’écart-types séparant chaque valeur de la moyenne. Figure 1.7: Distribution normale centrée réduite, c.-à-d., N(0, 1). On s’intéresse à la longueur d’ailes de pucerons dans une population. Après un recensement exhaustif, on détermine que la moyenne (\\(\\mu\\)) des longueurs d’aile dans une population est de 14.2 mm et que l’écart-type (\\(\\sigma\\)) est de 5.05 mm. On veut ensuite déterminer à combien d’écart-types de la moyenne se trouve une longueur d’aile de 22.6 mm chez un puceron de cette population. Nous avons donc, \\(z_i = \\frac{x_i - \\mu}{\\sigma} = \\frac{22.6 - 14.2}{5.05} = 1.66\\). On conclut que \\(x_i = 22\\) mm se trouve à \\(1.66 \\: \\sigma\\) de \\(\\mu\\) (Figure 1.8). Figure 1.8: Écart normal associé à une longueur d’ailes de puceron de 22.6 mm. 1.1.6.3 Probabilités cumulatives Les probabilités cumulatives sont beaucoup utilisées en statistique. Par exemple, on peut vouloir déterminer la probabilité d’observer un diamètre &gt; 2.3 cm pour un arbre mesuré à une hauteur 1 m du sol dans une forêt avec N(4, 12), c’est-à-dire, \\(P(\\mathrm{diam\\grave{e}tre} &gt; 2.3 \\:\\mathrm{cm})\\), ou encore déterminer la probabilité d’observer une profondeur de litière forestière entre 3 et 10 cm dans une population de sites avec N(12.5, 3.01), \\(P(\\mathrm{profondeur} &gt; 2.3 \\:\\mathrm{cm})\\). On peut résoudre ce genre de problème à l’aide de la distribution normale ou de la distribution normale centrée réduite. La probabilité cumulative correspond à l’aire sous la courbe dans un intervalle défini par une intégrale. Par exemple, on sait que l’aire sous la courbe d’une distribution normale centrée réduite entre -1.96 et 1.96 est de 0.95 : \\[ \\int\\limits_{-1.96}^{1.96} f(x \\: | \\: \\mu = 0, \\sigma = 1) dx = 0.95 \\] À noter que les probabilités cumulatives étaient autrefois obtenues à partir de de tables situées en annexe de livres de statistiques. De nos jours, nous utiliserons typiquement un logiciel comme R pour obtenir la probabilité cumulative. Dans R, la fonction pnorm( ) nous donne cette valeur et nous discuterons plus en détails de cette option dans les prochaines leçons. On a recensé tous les individus d’un édifice à bureaux d’une ville d’Amérique du Nord. La moyenne (\\(\\mu\\)) de la taille des individus dans cette population est de 170 cm avec un écart-type (\\(\\sigma\\)) de 8 cm. Quelle est la probabilité qu’un individu soit plus petit ou égal à 160 cm dans cette population? Pour résoudre le problème, on peut utiliser l’écart normal : \\[ z_i = \\frac{x_i - \\mu}{\\sigma} = \\frac{160 - 170}{8} \\\\ z_i = -1.25 \\] On peut écrire : \\[ P(x_i \\leq 160 \\: \\mathrm{cm}) = P(z \\leq -1.25) = 0.1056 \\] Figure 1.9: Probabilité cumulative associée à une valeur \\(\\leq\\) 160 cm dans une population avec N(170, 8). Ici, \\(P\\) est une probabilité cumulative que l’on peut obtenir en calculant l’aire sous la courbe pour la portion de la courbe à gauche du point -1.25 (Figure \\(\\ref{fig:z.ex2}\\)). Si on veut connaître la probabilité qu’un individu ait une taille supérieure à 185 cm dans la même population que celle de notre exemple précédent (édifice à bureaux), on pourrait encore une fois utiliser l’écart normal. Ainsi, on peut écrire : \\[ z_i = \\frac{x_i - \\mu}{\\sigma} = \\frac{185 - 170}{8} \\\\ z_i = 1.875 \\] La probabilité cumulative ici peut s’obtenir à l’aide de : \\[ P(x_i \\leq 185 \\: \\mathrm{cm}) = P(z \\leq 1.875) = 0.9696 \\] Toutefois, nous désirons \\(P(x_i &gt; 185 \\: \\mathrm{cm})\\), ce qui diffère des exemples précédents avec \\(P(x_i \\leq X)\\) (Figure \\(\\ref{fig:z.ex3}\\)a). L’astuce ici consiste à calculer le complément de \\(P(x_i \\leq 185 \\: \\mathrm{cm})\\) (Figure 1.10b). Puisque l’aire sous la courbe est de 1, on peut obtenir \\(P(x_i &gt; 185 \\: \\mathrm{cm})\\) simplement avec: Figure 1.10: Probabilité cumulative associée à une valeur &gt; 185 cm dans une population avec N(170, 8). Il est possible de déterminer la probabilité d’observer une valeur entre 165 cm et 180 cm dans la même population. Pour ce faire, il faut calculer la probabilité cumulative associée à chacune des bornes, comme suit : \\[ z_1 = \\frac{180 - 170}{8} = 1.25\\\\ z_2 = \\frac{165 - 170}{8} = -0.625 \\] On obtient les probabilités cumulatives de chaque \\(z_i\\) comme d’habitude: \\[ P(x_1 \\leq 180 \\: \\mathrm{cm}) = P(z_1 \\leq 1.25) = 0.8944\\\\ P(x_2 \\leq 165 \\: \\mathrm{cm}) = P(z_2 \\leq -0.625) = 0.2660 \\] La différence entre les deux probabilités cumulatives nous donnera la probabilité cumulative pour l’intervalle désiré (Figure 1.11): \\[ P(165 \\: \\mathrm{cm} \\leq x_i \\leq 180 \\: \\mathrm{cm}) = 0.8944 - 0.2660 = 0.6284 \\] Figure 1.11: Probabilité cumulative associée à 165 cm \\(\\leq\\) \\(\\ x_i\\) \\(\\leq\\) 180 cm dans une population avec N(170, 8). 1.1.6.4 Applications de la statistique z à un échantillon Comme nous venons de le voir, la statistique \\(z\\) peut s’appliquer aux observations d’une population dont on connaît les réelles valeurs de la moyenne et de l’écart-type. On utilise la distribution normale pour trouver la probabilité d’observer \\(x_i\\) (une valeur d’une observation) dans un intervalle donné d’une population avec une moyenne \\(\\mu\\) et un écart-type \\(\\sigma\\) connus. Toutefois, on peut aussi appliquer la même approche au niveau d’un paramètre d’une population. En d’autres mots, on peut déterminer l’écart normal (\\(z\\)) associé à la valeur de l’estimation d’un paramètre à partir d’un échantillon (p. ex., une moyenne, une médiane) d’une population avec \\(\\mu\\) et \\(\\sigma\\) connus. Ainsi, l’équation originale \\(z = \\frac{x_i - \\mu}{\\sigma}\\) qui dépendait de la normalité des observations devient \\(z = \\frac{\\bar{x} - \\mu}{\\sigma_{\\bar{x}}}\\) et s’intéresse à un paramètre d’une population, où \\(\\bar{x}\\) correspond à la moyenne arithmétique de l’échantillon, et \\(\\sigma_{\\bar{x}}\\) est l’erreur-type de la population. Autrement dit, au lieu de s’intéresser à des valeurs individuelles dans la population, nous ciblons plutôt la moyenne d’un échantillon tiré d’une population avec \\(\\mu\\) et \\(\\sigma_{\\bar{x}}\\) connus. Cette transition est possible en supposant que la moyenne provient d’une distribution normale des moyennes (grâce au théorème de la limite centrale). C’est le genre de traitement typique que nous faisons la plupart du temps avec les données d’un échantillon que nous récoltons à partir d’une population statistique. Exemple 1.13 Nous voulons déterminer la probabilité d’obtenir un échantillon aléatoire de 9 longueurs de becs d’oiseaux, lequel a une moyenne &gt; 50.0 mm dans une population avec \\(\\mu = 47.5 \\: \\mathrm{mm}\\) et \\(\\sigma = 12.89 \\: \\mathrm{mm}\\). On obtient: \\[ \\sigma_{\\bar{x}} = \\frac{\\sigma}{\\sqrt{n}} = \\frac{12.89}{\\sqrt{9}} = 4.30 \\\\ z = \\frac{\\bar{x} - \\mu}{\\sigma_{\\bar{x}}} = \\frac{50 - 47.5}{4.30} = 0.58 \\\\ P(\\bar{x} &gt; 50 \\: \\mathrm{mm}) = P(z &gt; 0.58) = 0.2803 \\] On constate qu’il est assez probable (\\(P(z &gt; 0.58) = 0.2803\\)) de tirer un échantillon de 9 longueurs de becs d’oiseaux avec des propriétés similaires à celles de l’échantillon original de la population d’intérêt. Par convention, on considère qu’une probabilité \\(P \\leq 0.05\\) est faible, bien que ce seuil soit arbitraire et qu’il existe de nouvelles approches qui mettent de côté la subjectivité du choix d’un tel seuil. Nous discuterons plus en détail des implications du choix de tels seuils ainsi que de méthodes alternatives dans des prochaines leçons. Malheureusement, on connaît rarement \\(\\sigma\\) dans la vraie vie et on doit utiliser une estimation obtenue à partir de l’échantillon. Comme nous l’avons souligné plus tôt, l’estimation de la variance est difficile dans les échantillons de petite taille. Comme le test \\(z\\) nécessite une estimation de la variance, ce dernier est peu utilisé pour tester des valeurs d’un échantillon de petite taille. Lorsque \\(\\sigma\\) est inconnu et doit être estimé à partir d’un échantillon, nous utiliserons plutôt la distribution du \\(t\\) de Student. C’est ce que nous verrons dans les prochaines leçons. 1.1.7 Conclusion Dans cette leçon, nous avons brièvement présenté les concepts de base importants en statistique, notamment les caractéristiques d’une population et d’un échantillon, les variables aléatoires, les mesures de tendance centrale et de dispersion. La distribution normale a été présentée, ainsi que le théorème de la limite centrale et la loi des grands nombres. Dans un processus multiplicatif, une variable a un effet multiplicatif sur une variable réponse. Parexemple, si on remarque que la croissance de semis à concentration modérée d’engrais est 2.5 fois plus élevée qu’à concentration faible, la concentration a un effet multiplicatif sur la croissance.↩︎ Un quadrat est une unité spatiale de dimension donnée (1 m × 1 m, 10 m × 10 m), disposée dans un site d’étude sur laquelle on fait des mesures en écologie, ici un décompte d’insectes.↩︎ Le symbole \\(\\propto\\) indique la proportionnalité entre deux variables. En d’autres mots, on peut passer des valeurs d’une variable en multipliant ou divisant par une constante non nulle pour obtenir les valeurs de l’autre.↩︎ Les gens ont souvent une interprétation bayésienne de l’intervalle de confiance en affirmant que c’est la probabilité que la moyenne soit comprise dans l’intervalle de confiance construit à partir d’un seul échantillon. Dans le cours, nous ferons uniquement appel aux statistiques classiques aussi appelées fréquentistes et nous utiliserons la définition classique de l’intervalle de confiance.↩︎ "],["statistiques-de-base-avec-r.html", "1.2 Statistiques de base avec R", " 1.2 Statistiques de base avec R 1.2.1 Fonctions statistiques de base Plusieurs fonctions de base sont disponibles pour réaliser les calculs présentés à la leçon Statistiques descriptives. Par exemple, on peut obtenir la moyenne arithmétique à l’aide de la fonction mean( ): ##On crée un vecteur variable &lt;- c(1.2, 1.5, 2.3, 2.1, 0.1, 5.4, 6.1, 3.2) ##Moyenne arithmétique mean(variable) ## [1] 2.7375 On peut calculer l’écart-type d’un échantillon à l’aide de la fonction sd( ), et la variance à l’aide de var( ): ##Écart-type sd(variable) ## [1] 2.072223 ##Écart-type sqrt(var(variable)) ## [1] 2.072223 ##Carré moyen des erreurs MSE &lt;- sum((variable - mean(variable))^2)/(length(variable) - 1) ##Racine-carrée du MSE = écart-type sqrt(MSE) ## [1] 2.072223 La médiane s’obtient facilement à l’aide de la fonction median( ): ##on calcule la médiane median(variable) ## [1] 2.2 De façon plus générale, on peut obtenir les quantiles d’un échantillon (quartiles, déciles, percentiles) à l’aide de quantile( ): ##Quartiles quantile(variable) ## 0% 25% 50% 75% 100% ## 0.100 1.425 2.200 3.750 6.100 ##Déciles quantile(variable, prob = c(0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1)) ## 10% 20% 30% 40% 50% 60% 70% 80% 90% 100% ## 0.87 1.32 1.56 1.98 2.20 2.48 3.11 4.52 5.61 6.10 ##2.5ième et 97.5ième percentiles quantile(variable, prob = c(0.025, 0.975)) ## 2.5% 97.5% ## 0.2925 5.9775 Bien qu’il n’existe pas de fonctions toute prête pour calculer l’erreur-type, on peut l’obtenir facilement: sd(variable)/sqrt(length(variable)) ## [1] 0.7326414 1.2.2 Distribution statistiques R comprend plusieurs distributions statistiques, ce qui rend obsolètes les tables de distributions statistiques que l’on utilisait autrefois. Les noms des fonctions de distributions statistiques commencent typiquement par la lettre “d”, telles que dnorm( ), dchisq( ), dpois( ), pour les fonctions de densité de la distribution normale, du khi-carré et de Poisson, respectivement. Ainsi, l’énoncé \\(f(x \\: \\vert \\: \\mu, \\sigma) = \\frac{1}{\\sigma \\sqrt{2\\pi}}e^{-\\frac{1}{2}(\\frac{x - \\mu}{\\sigma})^2}\\) se traduit dans le langage R par dnorm(x, mean, sd). On peut donc solutionner \\(f(x = 3.4 \\: \\vert \\: 4.1, 1.5) = \\frac{1}{1.5 \\cdot \\sqrt{2\\pi}}e^{-\\frac{1}{2}(\\frac{3.4 - 4.1}{1.5})^2}\\) avec : ##Calculer densité de probabilité dnorm(x = 3.4, mean = 4.1, sd = 1.5) ## [1] 0.2385223 1.2.2.1 Probabilité cumulative On peut obtenir la probabilité cumulative associée à \\(P(x \\leq 2.3 \\:\\mathrm{cm})\\) dans une population avec N(\\(\\mu = 4\\), \\(\\sigma = 12\\)) directement à l’aide de pnorm(q, mean, sd): ##Probabilité cumulative P(x &lt; 2.3) pnorm(q = 2.3, mean = 4, sd = 12) ## [1] 0.4436717 Il est aussi possible de calculer l’écart normal \\(z\\) pour obtenir la probabilité cumulative : ##Probabilité cumulative à partir du z z &lt;- (2.3 - 4)/12 pnorm(q = z, mean = 0, sd = 1) ## [1] 0.4436717 On peut calculer le complément, \\(P(x &gt; 2.3 \\:\\mathrm{cm})\\): ##P(x &gt; 2.3) 1 - pnorm(q = 2.3, mean = 4, sd = 12) ## [1] 0.5563283 ##P(x &gt; 2.3) à l&#39;aide de l&#39;argument lower.tail pnorm(q = 2.3, mean = 4, sd = 12, lower.tail = FALSE) ## [1] 0.5563283 À noter que l’argument lower.tail permet de spécifier la partie gauche de la distribution. Par défaut, cet argument prend la valeur TRUE, ce qui veut dire que pnorm() donne la probabilité cumulative associée à \\(P(x \\leq 2.3 \\: \\mathrm{cm})\\). Si lower.tail = FALSE, c’est la probabilité cumulative pour la portion droite de la distribution qui est calculée, \\(P(x &gt; 2.3 \\: \\mathrm{cm})\\). 1.2.2.2 Fonction de quantile Alors que pnorm() nous donne la probabilité cumulative pour un quantile et N(\\(\\mu, \\sigma)\\) donnés, qnorm() permet d’obtenir le quantile associé à une probabilité cumulative et N(\\(\\mu, \\sigma)\\) donnés. ##On connaît q, on veut trouver p pnorm(q = 1.96, mean = 0, sd = 1) ## [1] 0.9750021 ##On connaît p, on veut trouver q qnorm(p = 0.975, mean = 0, sd = 1) ## [1] 1.959964 1.2.2.3 Nombres aléatoires On peut générer des nombres aléatoires provenant d’une distribution normale avec une moyenne et un écart-type donnés à l’aide de rnorm(). Par exemple, pour générer un échantillon de 30 observations provenant d’une distribution normale avec une moyenne de 5.2 (\\(\\mu = 5.2\\)) et un écart-type de 9.3 (\\(\\sigma = 9.3)\\), on procède ainsi: rnorm(n = 30, mean = 5.2, sd = 9.3) ## [1] -5.2363812 2.1361361 0.3011311 14.2673387 -17.7739916 -9.3026133 -11.3246507 6.8461550 ## [9] 3.0471042 13.6141250 2.4075361 17.5214902 10.1071673 14.4077108 25.9460656 18.9730143 ## [17] 3.3120000 -11.0342477 -1.2842154 15.6671290 11.0324419 7.1440206 -11.0511271 18.7920010 ## [25] 11.7094413 8.1519404 -14.1162357 2.9833200 8.4015529 17.3012837 ##Un autre échantillon rnorm(n = 30, mean = 5.2, sd = 9.3) ## [1] -1.6124270 11.4817626 8.4525334 20.8241061 6.0612455 6.3076262 5.5756312 7.0758825 ## [9] 0.2320317 17.5802586 -11.3374816 13.0301788 1.6610982 12.3144666 9.8947239 1.9675440 ## [17] 1.5977410 16.0794961 9.2248124 6.3589486 3.8910765 6.4376539 13.1389341 27.6690893 ## [25] 17.3897674 13.2060804 0.1129850 9.1773988 6.0171239 -6.5901390 ##Un autre échantillon rnorm(n = 30, mean = 5.2, sd = 9.3) ## [1] 3.6785709 6.8416892 3.7258729 2.2478310 11.1032947 14.6380916 21.5935505 9.8814141 ## [9] 5.9585137 12.3102942 19.1595321 27.2887367 -4.5542297 15.5133371 -1.6836157 -6.2030749 ## [17] 6.0733576 -18.6266720 17.3499100 5.4086155 19.0235248 19.2177859 -5.0853282 -0.1873660 ## [25] -1.7421399 11.3806105 0.9224508 6.8150620 23.1308349 0.3419082 ##Un autre échantillon rnorm(n = 30, mean = 5.2, sd = 9.3) ## [1] 7.2135617 15.9216558 6.0659350 -1.1036486 -5.2447363 26.4559092 3.1149812 5.3037353 -0.5228139 ## [10] -0.9207635 12.1369322 9.7821762 12.9507889 9.5936729 -2.8227169 -8.0050764 -7.6184817 6.4950019 ## [19] 11.1006956 3.7684386 -3.1289699 7.9393173 0.3432332 -2.9806896 -8.4659100 12.7658153 17.3596326 ## [28] 7.7102953 1.3335973 9.7158424 On remarque qu’à chaque fois que l’on utilise rnorm(), on obtient un résultat des observations (aléatoires) différentes. "],["exercices-1.html", "1.3 Exercices", " 1.3 Exercices Veuillez consulter les sections Théories et Introduction à R - Statistiques de base de la section Statistiques descriptives pour réaliser les exercices suivants dans R. 1.3.1 Question 1 a. La somme des carrés des erreurs est une mesure de tendance centrale: Vrai ou Faux? Réponse Faux – C’est une mesure de dispersion. b. Parmi les mesures suivantes, déterminez laquelle des valeurs n’est pas un paramètre: (a) \\(\\mu\\) (b) \\(\\sigma\\) (c) \\(\\bar{x}\\) (d) \\(\\sigma^2\\) Réponse (c) \\(\\bar{x}\\) – C’est une statistique. c. L’écart-type est une mesure de précision d’un paramètre: Vrai ou Faux? Réponse Faux – C’est une mesure de variabilité des données. 1.3.2 Question 2 a. On mesure la profondeur de matière organique dans le sol de 100 parcelles en milieu agricole. À partir de 100 valeurs de profondeurs de sols, on obtient une moyenne de 12.3 cm et un écart-type de 4.5 cm. Calculez l’erreur-type de la moyenne. N.B. Cette question s’applique uniquement à l’approche par programmation. Réponse ##Calcul de l&#39;erreur-type de la moyenne SE &lt;- 4.5/sqrt(100) SE ## [1] 0.45 b. Dans une étude d’observation sur le temps de démarrage de 20 ordinateurs, on note les valeurs suivantes en secondes : 50, 45, 90, 61, 80, 120, 30, 58, 95, 40, 48, 31, 29, 51, 70, 66, 110, 97, 49, 99. Calculez la moyenne arithmétique et la variance de cet échantillon. Réponse ##On assemble les valeurs dans un vecteur temps &lt;- c(50, 45, 90, 61, 80, 120, 30, 58, 95, 40, 48, 31, 29, 51, 70, 66, 110, 97, 49, 99) ##On calcule la moyenne arithmétique mean(temps) ## [1] 65.95 ##La variance var(temps) ## [1] 774.7868 sd(temps)^2 ## [1] 774.7868 "],["intervalles-de-confiance-et-stratégies-déchantillonnage.html", "Module 2 Intervalles de confiance et stratégies d’échantillonnage", " Module 2 Intervalles de confiance et stratégies d’échantillonnage Dans la leçon précédente, nous avons vu des concepts de base importants en statistique, notamment les caractéristiques d’une population et d’un échantillon, les variables aléatoires, les mesures de tendance centrale et de dispersion. Nous avons également présenté le théorème de la limite centrale et la loi des grands nombres. Dans la présente leçon, nous poursuivons avec le concept de l’échantillonnage ainsi que différentes stratégies afin de récolter un échantillon. "],["théories-1.html", "2.1 Théories", " 2.1 Théories Échantillonnage Il est peu pratique voire souvent impossible d’étudier la population au complet. On utilise plutôt un échantillon pour faire des inférences sur la population. L’échantillon est constitué d’unités d’échantillonnage. Par unité d’échantillonnage, on entend la plus petite unité indépendante d’une expérience ou d’une étude d’observation sur laquelle on prend une mesure. L’échantillonnage permet de récolter de l’information afin de répondre à des questions du genre : Quel est le nombre d’arbres par hectare affligés par l’agrile du frêne à Gatineau? Combien y a-t-il de gisements d’or en Abitibi? Combien d’heures un portable d’une certaine compagnie dure-t-il avant de tomber en panne? Quelle partie de la population canadienne appuie la plus récente décision du gouvernement? On veut faire un sondage téléphonique pour déterminer le nombre d’heures passées par les téléspectateurs devant leur téléviseur par jour. Un bon nombre de décisions peuvent influencer le résultat — la région, le choix des répondants, la classe d’âge, la classe socio-économique, l’heure de l’appel. Que faire des biais potentiels associés à l’échantillonnage, comme les personnes sans téléphone, les personnes avec un numéro non affiché, les personnes ne voulant pas participer et les personnes possédant un afficheur? Plusieurs stratégies d’échantillonnage sont disponibles afin d’assembler un échantillon. Le choix de la stratégie d’échantillonnage dépend des caractéristiques de la population et des problèmes potentiels associés aux unités d’échantillonnage. Nous verrons plusieurs stratégies d’échantillonnage classiques dans les prochaines sections. Peu importe la stratégie d’échantillonnage utilisée, l’échantillon doit être représentatif de la population afin de permettre une bonne estimation des paramètres qui nous intéressent. On fait face à un problème de représentativité si un groupe particulier d’unités d’échantillonnage parmi la population d’intérêt n’apparaît pas dans l’échantillon. Le paramètre estimé à partir d’un échantillon qui n’est pas représentatif de la population risque d’être biaisé. 2.1.1 Problèmes d’échantillonnage Lors de l’échantillonnage, on présume souvent que: - la variable d’intérêt est mesurée sans erreur sur chaque observation, individu ou unité d’échantillonnage; - les erreurs dans les estimations proviennent uniquement de la nature de l’échantillon (erreur d’échantillonnage). L’erreur d’échantillonnage se définit comme étant la variabilité aléatoire d’un échantillon qui est tiré d’une population donnée. Par exemple, disons que l’on inscrive l’âge de chaque élève d’une école secondaire sur des bouts de papier à raison d’une valeur par bout de papier. S’il y a 1000 élèves, nous aurons une valeur d’âge pour chaque bout de papier. Si on sélectionne aléatoirement 50 observations à partir des élèves présents à la cafétéria pendant l’heure du dîner, on obtiendra 50 bouts de papier sur lequel est inscrit l’âge d’un élève. On peut demander à un collègue d’échantillonner 50 observations à partir de la même population lors de la même période de dîner. Ce dernier obtiendra fort probablement un échantillon différent du premier. La variation qui existe entre ces deux échantillons de la même population est communément appelée l’erreur d’échantillonnage – elle reflète uniquement le fait que le processus d’échantillonnage amène une variabilité entre les échantillons. Toutefois, on peut avoir d’autres types d’erreurs plus sournoises qui peuvent biaiser nos estimations. 2.1.1.1 Portion de la population non représentée L’impossibilité de sélectionner certains individus parmi la population amène des problèmes par rapport aux interprétations et aux conclusions que l’on peut tirer à propos de la population. Voici quelques exemples de ce problème: - Certains sites sélectionnés en haute altitude ne peuvent être échantillonnés à cause de la température inclémente. - Dans une étude sur la croissance des arbres, on ne peut pas mesurer la taille des grands arbres avec une échelle trop courte. - Lors d’un sondage téléphonique, il est impossible de rejoindre les individus qui n’ont pas de téléphone et ceux qui ne veulent pas répondre au sondage. Pour résoudre ce problème, on peut: - utiliser de l’échantillonnage additionnel afin d’obtenir des observations provenant de la portion manquée de la population; - choisir une nouvelle variable qui permet de mesurer toute la population et qui est fortement corrélée à la variable d’intérêt (p. ex., le diamètre du tronc plutôt que la hauteur pour les arbres); - redéfinir la population afin de mieux refléter ce qui a été mesuré au lieu de s’intéresser à toute la population, on cible une partie particulière de cette population. 2.1.1.2 Erreurs de mesure ou de saisie des données Parfois, des problèmes peuvent se glisser dans la mesure ou dans la saisie des données. Pour éviter ce type de problème, le contrôle de la qualité du travail, tant en ce qui concerne la mesure des données que la saisie et la gestion de celles-ci, s’avère la meilleure solution. Le meilleur moyen pour détecter les erreurs de mesure ou de saisie des données est d’utiliser des méthodes graphiques, lesquelles permettent d’identifier rapidement des valeurs extrêmes, aberrantes ou erronées, c’est-à-dire, des observations qui divergent nettement des autres. 2.1.1.3 Erreurs liées à la probabilité de détection Certains organismes ou éléments d’intérêt sont difficiles à détecter lors d’un inventaire, malgré les efforts déployés pour standardiser le protocole. Par exemple, il est souvent impossible de détecter tous les éléments liés à un phénomène d’intérêt en raison de sa visibilité (p. ex., symptômes peu apparents pour une maladie), des conditions au moment de l’observation (p. ex., beaucoup de vent lors de l’écoute de chants d’oiseaux), de l’expérience de l’observateur (p. ex., jeune médecin analysant une radiographie), ou des conditions à l’endroit de l’observation (p. ex., couches géologiques empêchant la détection de gisements). Le même problème se manifeste lorsqu’on étudie des populations humaines, par exemple lorsqu’on désire estimer la population de sans-abris au Canada. Le problème est similaire lorsqu’on s’intéresse à la présence d’un élément d’intérêt dans une série de sites. Ainsi, même si on ne détecte pas l’élément dans un site, cela ne signifie pas pour autant qu’il y est absente. L’erreur associée à la détection imparfaite ne peut pas être contrôlée en standardisant le protocole. La meilleure méthode pour pallier le problème consiste à estimer explicitement la probabilité de détection avec des méthodes appropriées telles que les modèles de capture-marquage-recapture (CMR), les modèles d’échantillonnage de la distance (distance sampling) et les modèles d’occupation de site (site occupancy analyses). 2.1.2 Échantillonnage complètement aléatoire L’échantillonnage complètement aléatoire est la stratégie d’échantillonnage la plus simple et celle qui offre le plus de flexibilité à l’étape des analyses. Avec cette approche, chaque unité d’échantillonnage a une chance égale d’être incluse dans l’échantillon et la sélection de chaque unité est le résultat d’un processus aléatoire. La sélection aléatoire peut être réalisée: en écrivant les numéros de chaque unité d’échantillonnage sur des bouts de papier, de les mettre dans une urne, de bien mélanger, et de tirer un nombre donné de bouts papier au hasard; en utilisant une table de nombres aléatoires dans un livre de statistiques; en utilisant un générateur de nombres aléatoires dans R. Pour générer des nombres aléatoires avec R, on peut utiliser une distribution uniforme (continue). Cette distribution se définit par : \\[ f(x \\vert \\text{min}, \\text{max}) = \\begin{cases} \\frac{1}{(\\text{max} - \\text{min})}&amp; \\text{si} \\: \\text{min} \\leq x \\leq \\text{max}, \\\\ 0&amp; \\text{autrement} \\:. \\end{cases} \\] On remarque deux paramètres, soit le minimum de l’intervalle (min) et le maximum de l’intervalle (max). Cette distribution décrit des événements qui ont exactement la même probabilité de se produire pour un intervalle donnée (max \\(-\\) min, Figure 2.1). Figure 2.1: Distribution uniforme pour l’intervalle 0 et 10. On veut déterminer la densité de probabilité associée à la valeur 3.46 dans un intervalle de 0 à 10 d’une distribution uniforme. \\[ f(3.46 \\vert 0, 10) = \\frac{1}{(\\text{max} - \\text{min})} = \\frac{1}{(10 - 0)} = 0.1 \\] On peut aussi déterminer la densité associée aux valeurs 8, 6.15, ou 10 dans la même distribution: \\[ f(8 \\vert 0, 10) = \\frac{1}{(\\text{max} - \\text{min})} = \\frac{1}{(10 - 0)} = 0.1 \\\\ f(6.15 \\vert 0, 10) = \\frac{1}{(\\text{max} - \\text{min})} = \\frac{1}{(10 - 0)} = 0.1 \\\\ f(10 \\vert 0, 10) = \\frac{1}{(\\text{max} - \\text{min})} = \\frac{1}{(10 - 0)} = 0.1 \\] On remarque que, peu importe la valeur, la densité de probabilité est de 0.1 tant et aussi longtemps que la valeur est dans l’intervalle 0 et 10. Ainsi, dès que les valeurs sont à l’extérieur de l’intervalle, la densité est de 0: \\[ f(100.2 \\vert 0, 10) = 0 \\\\ f(-1.4 \\vert 0, 10) = 0 \\\\ f(10.1 \\vert 0, 10) = 0 \\] Puisqu’elle assigne à tous les éléments une probabilité égale d’être sélectionnés, la distribution uniforme est souvent utilisée pour exécuter des sélections aléatoires. Si nous avons une population de 400 éléments (par exemple des arbres), et que nous voulons choisir un échantillon de 40 unités, nous pourrions procéder en assignant un numéro à chaque élément et en utilisant la distribution uniforme pour sélectionner aléatoirement les observations: ##on &quot;numérote&quot; 400 éléments population &lt;- 1:400 ##on utilise la distribution uniforme round(runif(n = 60, min = 0, max = 400), digits = 0) ## [1] 229 187 384 388 37 188 268 49 91 290 134 67 190 12 208 178 385 3 270 34 124 378 307 177 59 330 ## [27] 24 344 0 112 361 190 198 93 220 321 348 246 73 46 374 241 242 285 23 298 389 16 200 283 0 186 ## [53] 166 286 364 204 158 110 269 130 On note ici que nous avons demandé 60 valeurs aléatoires d’une distribution uniforme continue alors que nous en voulons 40, au cas où il y aurait des doublons dans les 40 premières valeurs. De plus, l’utilisation de la fonction round( ) permet d’arrondir les valeurs à l’entier le plus près. Encore plus simple, nous pourrions utiliser la fonction sample( ) qui permet de sélectionner aléatoirement avec ou sans remise. La sélection sans remise consiste à retirer les observations qui ont déja été tirées afin de ne pouvoir les sélectionner à nouveau. La sélection avec remise implique de pouvoir sélectionner la même observation à plusieurs reprises pour constituer l’échantillon. Autrement dit, la même observation peut apparaître plusieurs fois dans l’échantillon. ##on crée une série de 10 observations serie10 &lt;- 1:10 serie10 ## [1] 1 2 3 4 5 6 7 8 9 10 ##on sélectionne aléatoirement sans remise sample(x = serie10, size = 10, replace = FALSE) ## [1] 4 9 10 1 8 2 6 7 3 5 set.seed(seed = 3) ##on sélectionne aléatoirement avec remise sample(x = serie10, size = 10, replace = TRUE) ## [1] 5 10 7 4 10 8 8 4 10 7 En observant le code attentivement, on remarque que la sélection avec remise (replace = TRUE) peut donner plusieurs fois la même valeur. Typiquement, pour sélectionner des observations et construire notre échantillon, nous utiliserons la sélection sans remise. À noter qu’on peut également utiliser la randomisation afin d’attribuer des traitements particuliers à certaines unités ou encore randomiser la séquence des observations à effectuer 8. Par randomisation, on entend l’application d’un processus aléatoire sur l’attribution d’une condition aux unités d’échantillonnage. Par exemple, dans une expérience étudiant l’effet d’un nouveau médicament sur des patients, on pourrait sélectionner aléatoirement 50 patients afin de constituer deux groupes: un groupe de patients qui prendront le médicament et un autre groupe de patients qui ne le prendront pas. Ici, on pourrait randomiser le traitement, c’est-à-dire, déterminer aléatoirement le traitement que recevra chaque patient. En contrepartie, la sélection avec remise est plutôt utilisée dans le contexte du , qui est une approche permettant d’obtenir des erreurs-types d’une statistique d’intérêt ou de construire des intervalles de confiance. 2.1.3 Intervalle de confiance Le concept d’intervalle de confiance est une des notions les moins bien comprises dans les cours de statistiques. Proprement dit, c’est l’intervalle à l’intérieur duquel se trouvera le paramètre de la population (p. ex., \\(\\mu\\)) si l’on répète l’échantillonnage avec la même taille d’échantillon un grand nombre de fois. Avant de construire un intervalle de confiance, il faut choisir une probabilité d’erreur ou seuil (\\(\\alpha\\)). Par convention, \\(\\alpha\\) prend la valeur 0.01, 0.05 ou 0.1. Le niveau de confiance (coefficient de confiance) est \\(1 - \\alpha\\). Par conséquent, si on utilise un \\(\\alpha = 0.05\\), le niveau de confiance est 1 - 0.05 = 0.95 = 95 %. L’intervalle est un intervalle de confiance à 100(1 - \\(\\alpha\\))% = 95 %. À partir du théorème de la limite centrale vu à la leçon précédente Statistiques descriptives, on sait que la distribution des moyennes des échantillons approxime une distribution normale. Nous connaissons aussi certaines propriétées de la distribution normale centrée réduite, notamment, que 95 % des observations se trouvent à 1.96 \\(\\sigma\\) de la moyenne. Ainsi, on peut construire un intervalle de confiance autour de \\(\\mu\\): \\[ \\text{Si} \\: n \\ge 50, \\\\ \\bar{x} \\pm z_{\\alpha/2} \\cdot SE \\] où la première ligne signifie que cet intervalle est seulement valide pour des échantillons d’au moins 50 observations (voir l’explication plus bas). Dans la deuxième ligne, \\(SE\\) correspond à l’erreur-type de la moyenne et \\(z_{\\alpha/2}\\) correspond à l’écart normal qui définit les deux bornes (négative et positive) d’un intervalle qui exclue une proportion \\({\\alpha/2}\\) des valeurs à gauche et une proportion \\({\\alpha/2}\\) des valeurs à droite de la distribution normale centrée réduite, c’est-à-dire qu’inversement, cet intervalle inclue \\({100(1 - \\alpha) \\%}\\) des valeurs de cette distribution. Nous divisons donc par 2 le seuil d’erreur \\({\\alpha}\\) puisque l’intervalle de confiance a une borne à gauche et une autre à droite. Donc, si \\(\\alpha = 0.05\\), on a \\(z_{\\alpha/2} = z_{0.025} = 1.96\\), et il y a 2.5 % de chance d’avoir une valeur à gauche de la borne \\(1.96 \\cdot SE\\) (aire sous la courbe de 0.025) et 2.5 % de chance d’avoir une valeur à droite de la borne \\(1.96 \\cdot SE\\) (aire sous la courbe de 0.025). Nous pouvons écrire: \\[ IC \\: \\text{à} \\: 95\\:\\%: \\\\ P(\\bar{x} - 1.96 \\cdot SE \\leq \\mu \\leq \\bar{x} + 1.96 \\cdot SE) = 0.95 \\\\ IC \\: \\text{à} \\: 90\\:\\%: \\\\ P(\\bar{x} - 1.64 \\cdot SE \\leq \\mu \\leq \\bar{x} + 1.64 \\cdot SE) = 0.90 \\] On peut transposer les notions de la distribution normale dans R. La fonction dnorm( ) correspond à la fonction de densité de probabilité de la distribution normale pour une moyenne et un écart-type donnés. Ainsi, pour obtenir la densité de probabilité associée à \\(X = 3.5\\) dans une distribution normale avec une moyenne (\\(\\mu\\)) de 4.2 et un écart-type (\\(\\sigma\\)) de 10, nous procédons ainsi: dnorm(x = 3.5, mean = 4.2, sd = 10) ## [1] 0.03979661 La fonction qnorm( ) permet de trouver le quantile associé à une probabilité cumulative d’une distribution normale donnée pour un \\(\\mu\\) et \\(\\sigma\\) donnés. Pour déterminer l’écart-normal \\(z_{0.05}\\), on peut faire: ##z pour 0.05 qnorm(p = 0.05, mean = 0, sd = 1) ## [1] -1.644854 ##la distribution est symétrique et on ##peut obtenir le quantile du côté droit qnorm(p = 0.95, mean = 0, sd = 1) ## [1] 1.644854 ##z pour 0.025 qnorm(p = 0.025, mean = 0, sd = 1) ## [1] -1.959964 La probabilité cumulative \\(P(X &lt; x)\\) associée à un quantile, \\(\\mu\\) et \\(\\sigma\\) s’obtient avec pnorm( ): ##probabilité cumulative pour z = 0.025 pnorm(q = -1.96, mean = 0, sd = 1) ## [1] 0.0249979 ##probabilité cumulative pour z = 0.05 pnorm(q = -1.64, mean = 0, sd = 1) ## [1] 0.05050258 On connaît rarement la \\(SE\\) réelle de la population (\\(\\sigma_{\\bar{x}}\\)) et on doit l’estimer à partir de l’échantillon. L’estimation de ce paramètre est bonne lorsque \\(n &gt; 50\\). Toutefois, l’estimation de la \\(SE\\) est moins bonne pour des échantillons avec moins de 50 observations. En réalité, avec de petits échantillons, l’intervalle de confiance basé sur le \\(z\\) est trop étroit – il indique une précision plus grande qu’elle ne l’est en réalité. On doit donc corriger notre intervalle de confiance en utilisant la distribution du \\(t\\) de Student. 2.1.3.1 Distribution du t de Student La distribution du \\(t\\) de Student vient du statisticien et maître brasseur de Guinness, William Sealy Gosset. Au début du siècle dernier, la brasserie Guiness a investi beaucoup de ressources afin de sélectionner les variétés de houblon et d’orge qui possédaient les meilleures caractéristiques. Pour ce faire, ils embauchèrent des chimistes et des statisticiens pour arriver à leurs fins en leur donnant le statut de brasseurs et en mettant à leur disposition des laboratoires et des champs pour effectuer des expériences. Étant lié à Guinness et pour ne pas divulguer des secrets de production, Gosset publia sa découverte de la distribution sous le nom de Student. Cette distribution ne possède qu’un paramètre, soit les degrés de liberté (\\(n - 1\\)). Avec R, nous pouvons déterminer la densité de probabilité, le quantile et la probabilité cumulative pour un degré de liberté donné avec les fonctions dt( ), qt( ) et pt( ), respectivement. Bien que de forme très semblable à la distribution normale, la distribution du \\(t\\) de Student a un plus grand nombre d’observations dans les extrémités que la distribution normale (Figure 2.2. Pour un \\(n\\) et une probabilité cumulative donnés, les quantiles de la distribution du \\(t\\) de Student sont plus grands que celui de la distribution normale centrée-réduite (Figure @ref{fig:qt}). Par conséquent, l’intervalle de confiance obtenu avec le \\(t\\) est plus large qu’avec \\(z\\) lorsque \\(n &lt; 50\\). Figure 2.2: Comparaison de la distribution normale centrée réduite et de la distribution du \\(t\\) de Student. On note qu’il y a un plus grand nombre de valeurs dans les extrémités de la distribution du \\(t\\) que dans la distribution normale centrée réduite dès que \\(n &lt; 50\\). Figure 2.3: Valeurs des quantiles de la distribution du \\(t\\) de Student en fonction de la taille d’échantillon (\\(df = n - 1\\)) pour une probabilité cumulative de 0.95. La ligne horizontale pointillée correspond à l’écart-normal de la distribution normale centrée réduite pour la même probabilité cumulative. De façon plus générale, on peut construire un intervalle à (\\(1 - \\alpha\\)): \\[ P(\\bar{x} - t_{\\alpha/2, (n - 1)} \\cdot SE \\leq \\mu \\leq \\bar{x} + t_{\\alpha/2, (n - 1)} \\cdot SE) = (1 - \\alpha) \\] Pour un intervalle de confiance à 95 \\(\\%\\), \\((1 - \\alpha) = 0.95\\), \\[ P(\\bar{x} - t_{0.05/2, (n - 1)} \\cdot SE \\leq \\mu \\leq \\bar{x} + t_{0.05/2, (n - 1)} \\cdot SE) = (1 - \\alpha) \\] Une inspectrice municipale mesure l’épaisseur de l’asphalte à 32 endroits où une compagnie privée a fait la chaussée dans Montréal. L’épaisseur moyenne de l’asphalte est de 20.1 cm et l’écart-type estimé à partir de l’échantillon est de 3.39 cm. Quel est l’intervalle de confiance à 95 % autour de la moyenne? On peut obtenir les quantiles nécessaires à la solution du problème comme suit: ##détermination du quantile du t à alpha/2 t.stat &lt;- qt(p = 0.025, df = 31) t.stat ## [1] -2.039513 ##convertir le quantile en valeur positive ##pour procéder au calcul des bornes t.stat &lt;- t.stat *(-1) t.stat ## [1] 2.039513 ##calcul de la SE SE &lt;- 3.39/sqrt(32) SE ## [1] 0.599273 ##calcul de la borne inférieure 20.1 - t.stat * SE ## [1] 18.87777 ##calcul de la borne supérieure 20.1 + t.stat * SE ## [1] 21.32223 Nous pouvons donc écrire: \\[ \\bar{x} = 20.1 \\: \\text{cm} \\\\ s = 3.39 \\: \\text{cm} \\\\ n = 32 \\\\ SE = s_{\\bar{x}} = \\frac{3.39}{\\sqrt{32}} = 0.60 \\: \\text{cm} \\\\ \\\\ IC \\: \\text{à} \\: 95\\:\\%: \\\\ P(\\bar{x} - t_{0.05/2, (n - 1)} \\cdot SE \\leq \\mu \\leq \\bar{x} + t_{0.05/2, (n - 1)} \\cdot SE) = 0.95\\\\ P(\\bar{x} - t_{0.025, (32 - 1)} \\cdot 0.60 \\leq \\mu \\leq \\bar{x} + t_{0.025, (32 - 1)} \\cdot 0.60) = 0.95\\\\ P(18.88 \\leq \\mu \\leq 21.32) = 0.95\\\\ IC \\: \\text{à} \\: 95\\%: (18.88, 21.32) \\: . \\] On peut comparer les intervalles de confiance à 90 % et 99 % à partir du même échantillon: \\[ IC \\: \\text{à} \\: 90\\%: \\\\ P(\\bar{x} - t_{0.10/2, (32 - 1)} \\cdot SE \\leq \\mu \\leq \\bar{x} + t_{0.10/2, (32 - 1)} \\cdot SE) = 0.90\\\\ IC \\: \\text{à} \\: 99\\%: \\\\ P(\\bar{x} - t_{0.01/2, (32 - 1)} \\cdot SE \\leq \\mu \\leq \\bar{x} + t_{0.01/2, (32 - 1)} \\cdot SE) = 0.99\\\\ IC \\: \\text{à} \\: 90\\%: (19.08, 21.12) \\\\ IC \\: \\text{à} \\: 95\\%: (18.88, 21.32) \\\\ IC \\: \\text{à} \\: 99\\%: (18.45, 21.75) \\: . \\] On remarque rapidement que la largeur de l’intervalle de confiance augmente avec le seuil de confiance. Dans notre exemple, il est le plus étroit à 90 % et le plus large à 99 %. Comme nous l’avons mentionné plus haut, la moyenne de population (\\(\\mu\\)) sera incluse dans l’intervalle de confiance (\\(1 - \\alpha\\))% du temps lorsqu’on répète l’expérience un grand nombre de fois. L’intervalle de confiance implique le concept de rééchantillonnage (resampling), c’est-à-dire, la construction de plusieurs échantillons à partir de la même population, et c’est ce que nous verrons dans la prochaine section. 2.1.3.2 Rééchantillonnage Les statistiques classiques impliquent souvent un échantillonnage répété dans leur interprétation. Tout comme l’intervalle de confiance, la probabilité d’un test statistique représente ce que l’on devrait observer en moyenne lorsqu’il n’y a pas d’effet dans la population. Ici, “en moyenne” indique ce qu’on obtiendrait si on répétait l’expérience un grand nombre de fois avec une série d’échantillons de taille égale à celle de notre échantillon original, tous provenant de la même population. L’exemple suivant illustre ce principe à l’aide d’une population de 200 000 semis dans une plantation. Imaginons qu’une compagnie d’exploitation forestière coupe la forêt sur un site de 20 hectares (ha). La compagnie a replanté le site à raison d’un semis d’épinette noire par m\\(^2\\), ce qui équivaut à un total de 200 000 semis. Deux ans après avoir planté les semis, on veut connaître la croissance moyenne des semis sur le site. Le total de 200 000 semis du site de 20 ha constitue la population d’intérêt. Chacun des semis de la population croît à sa propre vitesse (Figure 2.4a), mais on ne peut connaître sa croissance sans la mesurer. On embauche un technicien pour sélectionner aléatoirement 25 semis à mesurer afin d’estimer la hauteur moyenne des semis dans cette même population (Figure 2.4b). Figure 2.4: Distribution de la population complète de semis dans une plantation où \\(\\mu = 74.99\\) et \\(\\sigma = 14.99\\) (a) et d’un échantillon aléatoire de 25 observations tiré de cette population (b). On peut calculer l’intervalle de confiance à 95 % à partir de notre échantillon, ce qui donne: \\[ \\bar{x} = 78.45 \\: \\text{cm} \\\\ s = 14.4 \\: \\text{cm} \\\\ n = 25 \\\\ SE = \\frac{14.44}{\\sqrt{25}} = 2.88 \\: \\text{cm} \\\\ \\\\ IC \\: \\text{à} \\: 95\\:\\%: \\\\ P(\\bar{x} - t_{0.05/2, (25 - 1)} \\cdot SE \\leq \\mu \\leq \\bar{x} + t_{0.05/2, (25 - 1)} \\cdot SE) = 0.95\\\\ P(78.45 - 2.06 \\cdot 2.88 \\leq \\mu \\leq 78.45 + 2.06 \\cdot 2.88 = 0.95\\\\ IC \\: \\text{à} \\: 95\\%: (72.5, 84.39) \\] L’intervalle de confiance obtenu à partir de l’échantillon de 25 observations est \\((72.5, 84.39)\\). À noter que qt( ) permet d’obtenir le quantile du \\(t\\) associé à la probabilité cumulative correspondant à \\(\\alpha/2\\). On constate que la vraie moyenne de la population (\\(\\mu = 74.99\\)) est contenue dans l’intervalle que l’on a construit. En d’autres mots, l’intervalle de confiance inclut ou n’inclut pas la vraie moyenne. On pourrait demander à 19 autres techniciens de sélectionner aléatoirement 25 semis, de les mesurer, puis de construire un intervalle de confiance à 95 % à partir de leur échantillon. On devrait s’attendre à ce que 19 des 20 intervalles de confiance (\\(19/20 = 0.95\\)) ainsi construits incluent réellement la moyenne de la population (\\(\\mu\\)). On peut aussi utiliser le concept de rééchantillonnage pour illustrer l’erreur-type de la moyenne. Dans la leçon 1, nous avons indiqué que la moyenne d’un échantillon provenait d’une distribution de moyennes d’échantillons. On pourrait demander à 1000 techniciens d’obtenir un échantillon aléatoire de 25 semis et de mesurer la hauteur de chaque arbre. Si chaque technicien calcule la moyenne de son échantillon, nous obtiendrons une distribution de moyennes d’échantillons (Figure 2.5. Rappelons-nous que l’erreur-type de la moyenne est une estimation de la variation entre les moyennes d’échantillons de même taille provenant de la même population. L’écart-type calculé à partir des valeurs des moyennes nous donnera l’erreur-type de la moyenne. Figure 2.5: Distribution des 1000 moyennes d’échantillons de 25 semis tirés de la population de 200 000 semis. 2.1.4 Autres stratégies d’échantillonnage 2.1.4.1 Échantillonnage stratifié En plus de l’échantillonnage complètement aléatoire, d’autres approches permettent également de construire un échantillon à partir d’une population. L’échantillonnage stratifié s’applique lorsque la population d’intérêt est divisée en régions ou strates. On entend par “strate” une tranche ou une partie de la population. Par exemple, si on s’intéresse à la masse des ours noirs dans l’est du Québec, on pourrait échantillonner des ours dans différents types d’habitats de cette région: urbain, aire de conservation, aire de coupe forestière. Si la masse des ours est sujette à différer d’un type d’habitat à l’autre, il serait inapproprié de procéder à un échantillonnage complètement aléatoire. Effectivement, certains types d’habitats risqueraient d’être sous-représentés et on supposerait erronément que le type d’habitat n’est pas une source de variation de la masse d’ours noir. L’échantillonnage stratifié consiste à partitionner la population afin que les unités à l’intérieur d’une même strate soient le plus similaires possible. Même si les strates diffèrent les unes des autres, l’échantillon stratifié sera représentatif de la population. On peut stratifier une région par une variable connue (habitat, élévation, type de sol) ou par sous-unité géographique. On peut également stratifier l’échantillonnage par espèce, sexe ou classe d’âge. La sélection des unités d’échantillonnage dans chacune des strates doit être indépendante de la sélection dans les autres strates. L’échantillonnage aléatoire est encore important ici. Lorsque chaque unité est sélectionnée aléatoirement à l’intérieur de chaque strate, on parle d’échantillonnage stratifié aléatoire. L’estimation de la moyenne sera la plus précise si les unités sont le plus semblables possible à l’intérieur de chaque strate. L’échantillonnage coûte cher en ressources (temps, argent, personnel). C’est pourquoi il est préférable de penser à la stratégie d’échantillonnage avant de débuter la collecte des données. Considérez la notation suivante: \\(L\\): nombre de strates; \\(n\\): taille de l’échantillon; \\(n_{h}\\): nombre d’échantillons dans la strate \\(h\\); \\(N_{h}\\): taille de la strate \\(h\\); \\(N\\): taille de population. Afin d’optimiser l’effort et les coûts, on peut répartir l’effort d’échantillonnage en fonction de la taille des strates. Si les \\(L\\) strates sont de taille égale, on peut attribuer \\(n_{h}\\) égal partout (\\(n_{h} = n/L\\)). Par exemple, si \\(n = 30 \\: \\text{et} \\: L = 3\\), nous aurons donc \\(n_{h} = 30/3 = 10\\). Si les strates sont de taille inégale, on peut attribuer un effort proportionnel à la taille de chaque strate (\\(N_h\\)) par rapport à la somme de l’ensemble des strates (\\(N\\)) afin d’obtenir une fraction constante partout qui équivaut à (\\(n/N\\)). Si la strate \\(h\\) contient \\(N_h\\) unités, l’effort sera \\(n_h = \\frac{n \\cdot N_h}{N}\\). Par exemple, si \\(n = 30, N_h = 200 \\: \\text{et} \\: N = 1000\\), nous obtiendrons \\(n_{h} = \\frac{n \\cdot N_h}{N} = \\frac{30 \\cdot 200}{1000} = 6\\). On peut aussi opter de répartir l’effort d’échantillonnage pour obtenir la moyenne avec la plus faible variance pour une taille d’échantillon \\(n\\) donnée – c’est ce qu’on appelle l’allocation optimale. Cette dernière option requiert une estimation de la variance à partir de données préléminaires ou d’anciens échantillons. Elle permet également d’y incorporer le coût en ressources (e.g., argent, temps). On attribue de plus gros échantillons aux plus grandes strates ou aux strates les plus variables et des plus petits échantillons aux strates plus petites ou qui sont plus coûteuses à échantillonner. Ceci permet d’optimiser l’allocation des ressources dévouées à l’échantillonnage. 2.1.4.2 Échantillonnage par grappes et échantillonnage systématique Dans l’échantillonnage par grappes, la population est divisée en groupes (grappes) d’unités secondaires près les unes des autres et les grappes sont sélectionnées aléatoirement. Par grappes, on entend un groupe composés d’unités spatiales adjacentes, comme des quadrats ou des transects contigus. La figure 2.6 montre cinq grappes de quatre quadrat. Ces grappes sont sélectionnées aléatoirement dans l’aire d’étude, mais les unités qui composent la grappe ne le sont pas. Ce genre de dispositif ne peut pas être analysé comme si les données étaient complètement indépendantes les unes des autres. Par conséquent, des analyses moins conventionnelles sont nécessaires pour estimer les paramètres d’intérêt. Figure 2.6: Exemple de grappes de quadrats pour l’échantillonnage par grappes. L’échantillonnage systématique est une variation de l’échantillonnage par grappe où on dispose systématiquement les unités d’échantillonnage parmi la population. Le point de départ de la grille est sélectionné aléatoirement, mais les unités d’échantillonnage ne le sont pas (Figure 2.7). Figure 2.7: Exemple d’échantillonnage systématique avec deux grappes (grilles) de quadrats. Avec l’échantillonnage par grappes ou l’échantillonnage systématique, on peut obtenir des estimés avec une faible variance si les grappes sont semblables entre elles. Idéalement, toute la variabilité de la population devrait être observée dans chaque grappe afin d’obtenir un échantillon représentatif de la population. Ce dernier point contraste avec l’échantillonnage stratifié qui minimisait la variance à l’intérieur des strates. Il y a toutefois quelques mises en garde: l’échantillonnage systématique avec une seule grappe permet d’estimer une moyenne, mais la variance ne se calcule pas comme pour un échantillon aléatoire; la forme et la taille des grappes influencent l’efficacité; en présence de forte hétérogénéité spatiale caractérisée par des patrons périodiques, l’échantillonnage systématique est déconseillé. Par exemple, échantillonner dans un milieu vallonné, où les unités d’échantillonnage tombent toujours dans une vallée sur toute l’aire d’étude ne capture que les caractéristiques des observations sur l’ensemble du milieu vallonné. 2.1.4.3 Échantillonnage multistade Il existe d’autres stratégies d’échantillonnage, telles que l’échantillonnage multistade qui consiste à sélectionner aléatoirement des unités secondaires à l’intérieur d’unités primaires. Pour illustrer cette stratégie, considérons une étude qui vise à caractériser des planches de bois destinées à la construction. Pour ce faire, on pourrait sélectionner 10 camions parmi ceux qui arrivent à l’entrepôt d’un centre de rénovation et choisir aléatoirement 15 planches dans chaque camion. Cette stratégie se distingue de l’échantillonnage par grappes par une sélection aléatoire des unités secondaires dans les grappes. Tout comme l’échantillonnage par grappes, l’échantillonnage multistade est logistiquement plus facile à réaliser que l’échantillonnage complètement aléatoire. Toutefois, les calculs des moyennes et des variances sont plus complexes que pour l’échantillonnage complètement aléatoire. Il est possible d’ajouter d’autres niveaux d’échantillonnage. Par exemple, dans une étude sur les insectes en forêt tropicale, on pourrait sélectionner aléatoirement des parcelles de 20 x 20 m dans l’aire d’étude dans lesquelles on sélectionnerait 5 quadrats de 1 x 1 m, pour ensuite établir 1 microquadrat de 0.25 x 0.25 m dans chaque quadrat afin de compter le nombre d’espèces de mousses. 2.1.4.4 Échantillonnage adaptatif L’échantillonnage adaptatif est une classe de stratégies d’échantillonnage qui regroupe plusieurs méthodes où la sélection des unités est modifée au cours de l’échantillonnage en fonction des observations qui s’accumulent. On peut l’utiliser pour des espèces rares ou difficiles à observer où les individus sont regroupés spatialement (bancs de poissons, colonie d’insectes), ou encore pour échantillonner des dépôts géologiques (gisements d’or dont on ne connait pas initialement la localisation exacte). Toutefois, les méthodes de calcul des moyennes et de variance sont particulièrement complexes avec cette méthode d’échantillonnage, ce qui fait qu’elle est encore peu utilisée. En effet, peu de techniques d’analyses dans le moment permettent de traiter des données provenant de cette stratégie d’échantillonnage. 2.1.4.5 Estimation de la probabilité de détection Certaines stratégies d’échantillonnage permettent d’estimer la probabilité de détection d’organismes ou d’éléments étudiés qui sont difficiles à détecter lorsque présents sur un site lors d’un inventaire. Les facteurs tels que la coloration, le comportement, le type d’habitat, la méthode d’échantillonnage, les conditions météorologiques, le temps de la journée, l’abondance et la période de l’année peuvent tous influencer la détection. Dans le meilleur des cas, ce genre de problème amène des sous-estimation de l’abondance ou des patrons d’occurrence. Dans le pire des cas, les problèmes de détection entraînent des conclusions erronées lorsqu’on compare différents traitements entre eux. Ces approches sont particulièrement appliquées en écologie animale et constituent une branche importante des statistiques. La littérature à ce sujet croît à une vitesse fulgurante. Parmi les stratégies qui estiment la probabilité de détection, on en compte trois principales, notamment l’échantillonnage de la distance, les méthodes de capture-marquage-recapture et les analyses d’occupation de sites. 2.1.4.5.1 Échantillonnage de la distance Pour estimer l’abondance, on peut utiliser l’échantillonnage de la distance (distance sampling), lequel consiste à établir des parcelles d’échantillonnage circulaires 9 ou des transects 10 disposés aléatoirement sur l’aire d’étude et dans lesquels on mesure la distance entre l’observateur et les individus détectés. Ceci permet de construire une fonction de détection et d’estimer l’abondance. 2.1.4.5.2 Méthode de capture-marquage-recapture Les méthodes de capture-marquage-recapture (CMR) consistent à capturer, marquer et recapturer des individus sur un ou plusieurs sites pendant plusieurs visites successives. On peut ainsi estimer la probabilité de capture et l’abondance des individus sur le site, laquelle est corrigée pour la détection imparfaite. Les méthodes de CMR sont également utilisées pour estimer des paramètres vitaux (vital rates), tels que la probabilité de survie, d’immigration et de transition d’un état à un autre, par exemple d’un état non reproducteur à reproducteur. 2.1.4.5.3 Analyses d’orccupation de sites Les analyses d’occupation de sites (site occupancy analyses) constituent une famille de méthodes généralement utilisée dans le domaine de la conservation de la biodiversité et permettant d’estimer la détection d’une espèce animale ou végétale sur une série de sites au cours de plusieurs visites successives, et ainsi d’obtenir une meilleure estimation de la probabilité de présence de l’espèce d’intérêt. Des modifications du modèle de base permettent d’estimer les probabilités d’extinction et de colonisation ou encore de co-occurrence entre deux espèces. Parmi toutes les stratégies d’échantillonnage présentées, nous allons utiliser l’échantillonnage complètement aléatoire et l’échantillonnage stratifié aléatoire. À noter que les exemples vus dans le cours supposent que la détection des individus est parfaite. 2.1.5 Conclusion Dans ce texte, nous avons introduit le concept d’intervalle de confiance en nous appuyant sur le rééchantillonnage. Selon la taille d’échantillon, nous avons utilisé la distribution normale ou la distribution du t de Student pour construire les intervalles de confiance. Parmi les différentes stratégies d’échantillonnage présentées, nous verrons le plus souvent l’échantillonnage complètement aléatoire et l’échantillonnage stratifié aléatoire. Plusieurs fonctions de R utilisent des générateurs de nombres aléatoires dans R, notamment rnorm() et sample(). Les algorithmes aléatoires nécessitent une valeur initiale appelée seed afin de démarrer le processus aléatoire. Par défaut, ces valeurs proviennent de l’horloge interne de l’ordinateur (année, mois, jour, heure, minute, seconde, microseconde). Ainsi, si on effectue la fonction rnorm( ) ousample(,~replace = FALSE) plusieurs fois, on obtiendra un résultat différent à chaque fois. On peut aussi procurer une valeur initiale seed afin d’obtenir le même résultat à chaque fois à l’aide de la fonction . L’argument seed de cette fonction prend un entier comme valeur.↩︎ Une parcelle d’échantillonnage est tout simplement un cercle dans lequel on échantillonne les éléments d’intérêt sur le terrain comme des oiseaux ou des grenouilles.↩︎ Un transect est un corridor de largeur et longueur définie qui est disposé dans un site afin d’échantillonner les éléments d’intérêt comme des herbivores.↩︎ "],["les-graphiques-avec-r.html", "2.2 Les graphiques avec R", " 2.2 Les graphiques avec R 2.2.1 Graphiques de base R a été conçu pour faire des analyses statistiques et des graphiques. Ce n’est donc pas surprenant d’y trouver plusieurs fonctions graphiques, des plus simples aux plus élaborées. Pour l’instant, passons en revue les fonctions graphiques de base. 2.2.2 Histogramme La fonction hist() permet de construire un histogramme de fréquence à partir d’une variable numérique (Figure 2.8). On remarque que les étiquettes de l’axe des x’s (abscisses) et des y’s (ordonnées) ainsi que le titre principal laissent un peu à désirer: par défaut, hist() affiche le nom de la variable tel qu’il a été spécifié. On peut changer les étiquettes à l’aide des arguments xlab, ylab et main (Figure 2.9). vers &lt;- read.table(file = &quot;Module_2/data/vers.txt&quot;, header = TRUE) hist(vers$Superficie) Figure 2.8: Histogramme des valeurs de superficie dans le jeu de données de vers de terre. hist(vers$Superficie, ylab = &quot;Fréquences&quot;, xlab = &quot;Superficie&quot;, main = &quot;Histogramme de fréquences pour les valeurs de superficie&quot;) Figure 2.9: Histogramme plus élaboré à partir des valeurs de superficie dans le jeu de données de vers de terre. 2.2.3 Diagramme à bâtons Pour tracer un diagramme à bâtons, typiquement nécessaire pour illustrer des valeurs discrètes, on utilise barplot() (Figure 2.10). Cette fonction nécessite un vecteur ou une matrice comme argument principal. Quant aux matrices, on peut soit illustrer les résultats sous forme de bâtons superposés ou adjacents (Figure 2.11). On remarque qu’une légende a été ajoutée à l’aide de l’argument legend.text = TRUE (Figure 2.11). Nous verrons plus tard que la fonction legend() est beaucoup plus flexible. Il est à noter également que l’ajout de couleur peut se faire directement avec l’argument col. table(vers$Vegetation) #fréquences de chaque type de végétation ## ## Arable Chaparral Prairie Pre Verger ## 3 4 9 3 1 barplot(height = table(vers$Vegetation), ylab = &quot;Fréquences&quot;, xlab = &quot;Type de végétation&quot;, col = &quot;bisque&quot;) #diagramme en bâton Figure 2.10: Diagramme à bâtons des fréquences de différentes classes de végétation dans le jeu de données de vers de terre. tab.freq &lt;- table(vers$Humide, vers$Vegetation) tab.freq ## ## Arable Chaparral Prairie Pre Verger ## FALSE 3 2 8 0 1 ## TRUE 0 2 1 3 0 par(mfrow = c(2, 2)) #organiser 4 graphiques dans une matrice de 2 x 2 barplot(tab.freq, main = &quot;Bâtons superposés&quot;) #bâtons superposés barplot(tab.freq, beside = TRUE, main = &quot;Bâtons adjacents&quot;) #bâtons adjacents barplot(tab.freq, beside = TRUE, legend.text = TRUE, main = &quot;Ajouter une légende&quot;) #ajouter une légende barplot(tab.freq, beside = TRUE, legend.text = TRUE, col = c(&quot;green&quot;, &quot;turquoise&quot;), main = &quot;Légende et couleurs&quot;) #ajouter légende et couleurs Figure 2.11: Diagramme à bâtons avec différentes structures et couleurs à partir des fréquences de chaque classe de végétation. 2.2.4 Diagramme de boîtes et moustaches On peut obtenir un diagramme de boîtes et moustaches (boxplot ou box-and-whisker plot) à l’aide de la fonction boxplot() (Figure 2.12). Cette fonction permet d’illustrer une seule variable ou de visualiser plusieurs groupes simultanément. Ce graphique illustre la médiane (et non la moyenne arithmétique) par un trait gras à l’intérieur de la boîte. La limite inférieure de la boîte correspond au \\(25^{\\mathrm{e}}\\) percentile tandis que la limite supérieure indique le \\(75^{\\mathrm{e}}\\) percentile. La différence entre ces deux limites se nomme l’écart interquartile. Le graphique de boîtes et moustaches montre où se situent 50% des observations et s’avère un bon moyen de visualiser rapidement les données. Par défaut, les moustaches représentent 1.5 fois l’écart interquartile ou la valeur la plus extrême: la moustache présente la plus petite de ces deux valeurs. Ceci peut-être modifié à l’aide de l’argument range. Par exemple, si range = 0, les moustaches s’étendront jusqu’aux valeurs extrêmes. boxplot(Densite.vers ~ Vegetation, data = vers) Figure 2.12: boxplot 2.2.5 Graphiques génériques La fonction plot() permet de créer des nuages de points (type = \"p\"), des lignes (type = \"l\"), une combinaison de lignes et de points (type = \"b\"), et quelques autres graphiques inusités (Figure 2.13). Cette fonction comporte un grand nombre d’arguments. On peut modifier plusieurs paramètres graphiques généraux de R à l’aide de par(). Par exemple, on peut ajouter plusieurs graphiques dans une même fenêtre, à l’aide des arguments mfrow ou mfcol. La première remplit la fenêtre une rangée à la fois, alors que la deuxième remplit la fenêtre une colonne à la fois. Un point important pour créer des lignes: il faut ordonner les données selon la variable indépendante (i.e., l’axe des x’s), sinon on obtient des zigzags qui n’ont aucun sens (Figure 2.13). vers.ord &lt;- vers[order(vers$Superficie), ] #ordonner selon Superficie par(mfrow = c(2, 2)) #combiner graphiques sur c(2 rangées, 2 colonnes) plot(Densite.vers ~ Superficie, type = &quot;p&quot;, data = vers) #points plot(Densite.vers ~ Superficie, type = &quot;l&quot;, data = vers) #lignes plot(Densite.vers ~ Superficie, type = &quot;l&quot;, data = vers.ord) #lignes plot(Densite.vers ~ Superficie, type = &quot;b&quot;, data = vers.ord) #points et lignes Figure 2.13: Différents types de graphiques obtenus avec la fonction exttt{plot( )}. 2.2.6 Sauvegarder des graphiques Lorsqu’on a obtenu le graphique convoité, on peut le sauvegarder facilement. Plusieurs options se présentent. Par exemple, sous MS-Windows, on peut copier le graphique à partir de la fenêtre graphique et le coller directement dans un document MS-Word ou Writer de OpenOffice. Le format vectoriel (metafile) est un format particulièrement facile à coller directement dans les documents. Autrement, on peut le sauvegarder sous différents formats à partir du menu dans la fenêtre du graphique nouvellement créé. Une autre option encore plus flexible consiste à utiliser les appareils graphiques (graphical devices) afin de spécifier le fichier dans lequel le graphique sera sauvegardé, et de spécifier plusieurs caractéristiques, comme la résolution désirée ou la taille en pixels, en centimètres ou en pouces. Une vérification rapide avec ?Devices nous indique qu’on peut sauvegarder entre autres, en format pdf, bitmap, postscript, jpeg, png, et tiff. Pour sauvegarder un diagramme de boîtes et moustaches en pdf, on pourrait procéder ainsi: ##on indique à R qu&#39;on veut sauvegarder en pdf pdf(file = &quot;Module_2/images/boxplot.pdf&quot;, width = 6, height = 6) ##on crée le graphique - n&#39;apparaîtra pas à l&#39;écran boxplot(Densite.vers ~ Vegetation, data = vers) ##on indique à R que le graphique est terminé dev.off( ) # ## png ## 2 On remarque que la commande pdf() indique que le graphique sera sauvegardé sous le format pdf dans le fichier boxplot.pdf à l’endroit spécifié. Si aucun chemin n’est spécifié, le fichier sera sauvegardé dans le répertoire de travail. On peut facilement déterminer le répertoire travail à l’aide de la commande getwd(). Les arguments width et height de pdf( ) spécifient la largeur et la hauteur de la figure en pouces, et ces valeurs prennent 7 par défaut. La commande dev.off( ) émise après le graphique informe R que le graphique est terminé. Le graphique à réaliser doit se trouver entre ces deux commandes et n’apparaîtra pas à l’écran puisqu’il sera sauvegardé dans le fichier directement. ##on indique à R qu&#39;on veut sauvegarder en jpeg jpeg(file = &quot;Module_2/images/boxplot.jpeg&quot;, width = 1200, height = 1200, quality = 100) ##on crée le graphique - n&#39;apparaîtra pas à l&#39;écran boxplot(Densite.vers ~ Vegetation, data = vers) ##on indique à R que le graphique est terminé dev.off( ) ## png ## 2 Le graphique sera sauvegardé en format jpeg à l’endroit spécifié avec une taille de 1200 x 1200 pixels et une qualité de 100 % (avec peu de compression). Nous continuerons à explorer d’autres fonctionnalités graphiques lors des leçons subséquentes. "],["exercices-2.html", "2.3 Exercices", " 2.3 Exercices 2.3.1 Question 1 a. Parmi les stratégies d’échantillonnage suivantes, laquelle est la plus appropriée pour comparer la densité de pins gris dans cinq différents types d’habitats? échantillonnage complètement aléatoire échantillonnage systématique échantillonnage systématique échantillonnage stratifié aléatoire Réponse (d) échantillonnage stratifié aléatoire b Dès qu’un échantillon comporte moins de 50 observations, il faut utiliser la distribution normale. Vrai ou faux? Réponse Faux – Il faut plutôt utiliser la distribution du \\(t\\) de Student. c Nommez un problème qui peut survenir dans un échantillon. Réponse La non-représentativité de l’échantillon, les erreurs de saisies des données ou de mesure, et les erreurs liées à la probabilité de détection sont tous des problèmes potentiels dans un échantillon. d La distribution cunéiforme permet de sélectionner aléatoirement des unités dans une population. Vrai ou faux? Réponse Faux – C’est plutôt la distribution uniforme qu’on utilise. e Donnez un désavantage d’utiliser une stratégie d’échantillonnage systématique. Réponse L’échantillonnage systématique ne donne pas un échantillon représentatif en présence de patrons périodiques et les inférences qu’on peut en tirer sont très limitées en présence d’une seule grille. f Quelle stratégie d’échantillonnage serait la plus appropriée pour étudier l’occurrence d’un petit champignon très rare et difficile à voir sur la litière forestière? Réponse La méthode d’analyse d’occupation de sites serait la plus appropriée, mais, à la limite, l’échantillonnage adaptatif pourrait aussi être approprié. 2.3.2 Question 2 a Importez le jeu de données croissance.csv, qui présente la croissance en kg de bovins auxquels on a donné l’une des trois types de moulées. Réponse ##importer jeu de données croissance &lt;- read.table(&quot;Module_2/data/croissance.csv&quot;, header = TRUE) ##on regarde une partie du jeu de données et sa structure head(croissance) ## diete gain ## 1 ble 17.37125 ## 2 ble 16.81489 ## 3 ble 18.08184 ## 4 ble 15.78175 ## 5 ble 17.70656 ## 6 ble 18.22717 str(croissance) ## &#39;data.frame&#39;: 48 obs. of 2 variables: ## $ diete: chr &quot;ble&quot; &quot;ble&quot; &quot;ble&quot; &quot;ble&quot; ... ## $ gain : num 17.4 16.8 18.1 15.8 17.7 ... b Calculez l’intervalle de confiance à 95 % autour de la moyenne de chacun des trois groupes définis par le type de moulée. Vous trouverez de plus amples détails sur le script suivant dans le forum du cours à l’entrée “Explications sur le script du calcul de l’intervalle de confiance”. Réponse ##on visualise les données boxplot(croissance$gain ~ croissance$diete) ##on crée un sous-jeu de données pour chaque diète avoine &lt;- croissance[croissance$diete == &quot;avoine&quot;, ] ble &lt;- croissance[croissance$diete == &quot;ble&quot;, ] orge &lt;- croissance[croissance$diete == &quot;orge&quot;, ] ##avoine - IC à 95% n.avoine &lt;- nrow(avoine) moy.avoine &lt;- mean(avoine$gain) sd.avoine &lt;- sd(avoine$gain) SE.avoine &lt;- sd.avoine/sqrt(n.avoine) IC.inf.avoine &lt;- moy.avoine + qt(p = 0.025, df = n.avoine - 1) * SE.avoine IC.sup.avoine &lt;- moy.avoine - qt(p = 0.025, df = n.avoine - 1) * SE.avoine ##ble - IC à 95% n.ble &lt;- nrow(ble) moy.ble &lt;- mean(ble$gain) sd.ble &lt;- sd(ble$gain) SE.ble &lt;- sd.ble/sqrt(n.ble) IC.inf.ble &lt;- moy.ble + qt(p = 0.025, df = n.ble - 1) * SE.ble IC.sup.ble &lt;- moy.ble - qt(p = 0.025, df = n.ble - 1) * SE.ble ##orge - IC à 95% n.orge &lt;- nrow(orge) moy.orge &lt;- mean(orge$gain) sd.orge &lt;- sd(orge$gain) SE.orge &lt;- sd.orge/sqrt(n.orge) IC.inf.orge &lt;- moy.orge + qt(p = 0.025, df = n.orge - 1) * SE.orge IC.sup.orge &lt;- moy.orge - qt(p = 0.025, df = n.orge - 1) * SE.orge ##assembler le tout dans un petit tableau out &lt;- data.frame(Groupe = c(&quot;avoine&quot;, &quot;ble&quot;, &quot;orge&quot;), Moyenne = c(moy.avoine, moy.ble, moy.orge), IC.inf = c(IC.inf.avoine, IC.inf.ble, IC.inf.orge), IC.sup = c(IC.sup.avoine, IC.sup.ble, IC.sup.orge)) out ## Groupe Moyenne IC.inf IC.sup ## 1 avoine 21.32882 20.44115 22.21649 ## 2 ble 18.43134 17.57944 19.28324 ## 3 orge 24.42164 23.21268 25.63060 c Présentez graphiquement les moyennes avec des barres d’erreur à l’aide des fonctions plot( ), axis( ), points( ) et arrows. À noter que le plus simple est de créer le graphique par étapes: Utilisez plot( ) pour créer un graphique vide avec les étiquettes des axes et les bonnes limites, mais en supprimant l’axe des x avec l’argument axt = \"n\". La raison en est simple: à chaque fois que l’on spécifie un graphique d’une variable numérique en fonction d’une variable catégorique, comme diete, R donne un diagramme de boîtes et moustaches – ce n’est pas ce qu’on veut ici. Ajoutez l’axe des x’s à l’aide de axis( ). Ajoutez les points avec points( ). Ajoutez les barres d’erreur avec arrows( ). Cette dernière fonction nécessite un point de départ (x0, y0) et un point d’arrivée (x1, y1). Bien qu’elle puisse créer des flèches, on peut obtenir des barres d’erreur en utilisant angle = 90. L’argument code = 3 spécifie que l’on veut une flèche au point d’arrivée et au point de départ. Vous trouverez de plus amples détails sur le script suivant dans le forum du cours à l’entrée “Explications sur le script du graphique de la moyenne et des barres d’erreur”. Réponse ##présenter les résultats dans un graphique ##créer un graphique vide ##voir ?par pour les paramètres graphiques plot(y = 0, x = 0, xlab = &quot;Type de moulée&quot;, ylab = &quot;Moyenne (kg)&quot;, main = &quot;Moyennes ± IC à 95 %&quot;, ylim = c(min(out$IC.inf), max(out$IC.sup)), ##détermine les limites sur le graphique xlim = c(0, 4), xaxt = &quot;n&quot;) #on n&#39;affiche pas l&#39;axe des x&#39;s tout de suite ##on ajoute l&#39;axe des x&#39;s axis(side = 1, at = c(1, 2, 3), labels = c(&quot;avoine&quot;, &quot;ble&quot;, &quot;orge&quot;)) ##ajouter les points points(y = out$Moyenne, x = c(1, 2, 3)) ##ajouter les barres d&#39;erreurs arrows(x0 = c(1, 2, 3), x1 = c(1, 2, 3), y0 = out$IC.inf, y1 = out$IC.sup, length = 0.05, code = 3, angle = 90) ##les barres d&#39;erreurs ont un point de départ x0, y0 et ##d&#39;arrivée x1, y1 ##length donne la longueur des flèches ##angle = 90 demande des flèches à 90o ##code = 3 demande une flèche au point de départ et d&#39;arrivée "],["tests-dhypothèse-sur-un-seul-groupe.html", "Module 3 Tests d’hypothèse sur un seul groupe", " Module 3 Tests d’hypothèse sur un seul groupe Lors de la leçon précédente, nous avons illustré le concept d’intervalle de confiance en nous appuyant sur le rééchantillonnage. Afin de construire des intervalles de confiance, nous avons utilisé la distribution normale ou la distribution du t de Student selon la taille de l’échantillon. Différentes stratégies d’échantillonnage ont été présentées, bien que l’échantillonnage complètement aléatoire et l’échantillonnage stratifié aléatoire soient les approches les plus fréquemment utilisées dans le cours. "],["théories-2.html", "3.1 Théories", " 3.1 Théories 3.1.1 Hypothèse Une hypothèse est une explication potentielle que l’on formule pour décrire nos observations du monde extérieur. On entend par “observations”“, les”données” qui sont recueillies. Les hypothèses impliquent souvent une relation de cause à effet. Les études scientifiques permettent de comprendre la cause des phénomènes observés. La formulation d’hypothèses peut provenir d’observations personnelles préliminaires, de prédictions à partir de modèles théoriques, de la littérature scientifique ou du raisonnement. Les données utilisées pour tester les hypothèses peuvent provenir d’expériences contrôlées (controlled experiments) ou d’études d’observation (observational studies). Lors d’une expérience contrôlée (exemple 3.1), les traitements sont assignés aléatoirement aux unités expérimentales, et on contrôle toutes les variables sauf celles pour lesquelles on veut mesurer l’effet. Le résultat d’une expérience est très robuste et permet de tirer des conclusions solides quant aux traitements étudiés. L’étude d’observation (exemple 3.2), quant à elle, implique de sélectionner aléatoirement les unités expérimentales, mais l’assignation des traitements n’est pas déterminée aléatoirement par l’expérimentateur. Puisqu’on contrôle très peu de variables, il est plus difficile de conclure en une relation de cause à effet d’une variable sur une autre. On peut faire davantage confiance à des conclusions qui reposent sur des manipulations plutôt que sur des observations où on ne contrôle peu ou pas de variables. Les deux types d’approches expérimentales sont importantes et complémentaires. L’expérience contrôlée peut parfois manquer de réalisme en simplifiant trop le système à l’étude, alors que l’étude d’observation mesure la variation en conditions plus naturelles, mais non contrôlées. Une étude d’observation peut révéler des patrons bruts et suggérer une expérience contrôlée afin de raffiner notre enquête. On réalise une expérience visant l’effet de différents types de fourrage sur le goût du lait de vache. Pour ce faire, on sélectionne 30 vaches de la même race et on leur attribue aléatoirement l’un de trois types de fourrage (millet, trèfle, et luzerne), afin d’avoir 10 vaches pour chaque traitement. On soumet les vaches aux mêmes conditions de séjour (photopériode, température ambiante, disponibilité d’eau, “bonne” musique). À la fin d’une période donnée pendant laquelle les vaches ont été nourries avec le type de fourrage prescrit (p. ex., une semaine, un mois), nous pourrons ensuite échantillonner le lait de chaque vache. Après avoir soumis les échantillons de lait à notre goûteur, le résultat de l’expérience permettra de déterminer la différence de goût du lait selon le type de fourrage perceptible par un goûteur. On réalise une étude d’observation afin d’étudier le nombre de grenouilles présentant des malformations dans des étangs en milieux agricoles. On sélectionne aléatoirement 20 étangs dans des milieux à faible intensité agricole et 20 étangs additionnels dans des milieux à forte intensité agricole. Dans chaque étang, on effectue des inventaires afin d’estimer le nombre de grenouilles qui ont des malformations à l’aide de méthodes qui tiennent compte de la probabilité de détection. On mesure également d’autres variables, telles que la taille de l’étang, la distance de chaque étang par rapport au champ agricole le plus près, et la concentration d’une substance toxique dans l’eau de chaque étang. Une analyse statistique permet de déterminer comment le nombre de malformations varie selon les autres variables mesurées. Si on observe que le nombre de malformations est plus élevé dans les étangs près des champs agricoles, on ne peut pas conclure avec certitude que cette augmentation est due à la proximité des champs puisque nous n’avons pas contrôlé toutes les variables. En fait, il se peut que certaines substances toxiques différentes de celles qu’on a mesurées, provenant des champs agricoles situés à proximité, soient amenées par ruissellement dans les étangs. Ainsi, la cause des malformations n’est pas nécessairement la proximité des champs agricoles, mais plutôt les produits toxiques qui y sont appliqués. Après avoir détecté un effet de la proximité, il serait approprié d’effectuer une expérience contrôlée en faisant varier certaines variables pour quantifier directement leurs effets. 3.1.1.1 Hypothèse scientifique Une hypothèse scientifique est une hypothèse qui peut être testée avec des observations ou des résultats expérimentaux et qui permettront de la modifier ou de la rejeter. Toute bonne hypothèse scientifique devrait générer de nouvelles prédictions11. À noter aussi qu’un modèle peut appuyer une hypothèse particulière et que ce modèle mène à des prédictions. Une bonne hypothèse scientifique ne devrait fournir qu’une seule prédiction qui ne provient pas d’explications alternatives. Finalement, une bonne hypothèse doit avoir la possibilité d’être rejetée ou réfutée (ou falsifiée) en présence de données qui la contredisent. Une hypothèse qui n’a aucune chance d’être rejetée, pour laquelle il est impossible de récolter des données, n’est pas une hypothèse scientifique – on doit distinguer croyance et hypothèse. Par exemple, on pourrait considérer les énoncés suivants qui ne peuvent pas être réfutés et qui ne sont donc pas de vraies hypothèses (pour le moment, en tout cas …) : Il y a de la vie sur Pluton. Les populations de yétis et de sasquatchs sont repoussées dans des milieux encore plus reculés à la suite du développment urbain. Les chiens rêvent à leurs meilleurs moments de la journée pendant la nuit. En contrepartie, les énoncés suivants sont des hypothèses qui peuvent être potentiellement réfutées avec des données: La lumière abondante dans les coupes forestières favorise les espèces végétales intolérantes à l’ombre. Un groupe de grande taille confère à des poissons une meilleure chance de survie face à un prédateur. Les ordinateurs de la compagnie XX ont une durée de vie plus grande que les ordinateurs manufacturés par la compagnie YY. Pour chacune des hypothèses ci-dessus, on pourrait développer un protocole afin de tester l’hypothèse à partir d’une expérience contrôlée ou d’une étude d’observation. Les données récoltées permettraient de rejeter ou non l’hypothèse. Il faut garder à l’esprit que la formulation d’une bonne hypothèse est le point de départ d’un bon projet scientifique. 3.1.1.2 Méthode scientifique La méthode scientifique est une approche utilisée pour élaborer des hypothèses selon des observations et des prédictions. Différents types de raisonnements peuvent être utilisés selon la méthode scientifique, notamment la déduction (exemple 3.3) et l’induction (exemple 3.4). Le raisonnement déductif consiste à tirer des conclusions (inférer) à partir de données ou modèles partant du général pour arriver au spécifique. Par opposition, le raisonnement inductif consiste à tirer des conclusions à partir de données ou modèles partant du spécifique au général. Voici un exemple de déduction : 1. Tous les mélèzes en Abitibi sont de l’espèce Larix laricina. 2. J’ai échantillonné ce mélèze en Abitibi. 3. Ce mélèze est un Larix laricina. Les éléments 1 et 2 sont des prémisses et le troisième est la conclusion. Ensemble, ces trois éléments forment un syllogisme. Voici un exemple d’induction : 1. Ces 25 mélèzes sont des Larix laricina. 2. Ces 25 mélèzes ont été échantillonnés en Abitibi. 3. Tous les mélèzes d’Abitibi sont des Larix laricina. Avec la déduction, si les prémisses sont vraies, la conclusion a de très bonnes chances d’être correcte, alors qu’avec l’induction, si les prémisses sont vraies, la conclusion est probablement vraie, mais elle peut aussi être fausse. On utilise l’un ou l’autre de ces raisonnements pour construire des hypothèses, mais on peut aussi les utiliser de façon concomitante. La statistique est une approche inductive, puisqu’on tire des conclusions en partant du spécifique (l’échantillon) pour l’appliquer au plus général (la population). La figure 3.1 montre l’utilisation de l’induction dans la démarche scientifique. Figure 3.1: Schéma de l’approche d’induction en sciences. 3.1.1.3 Inférence bayésienne L’inférence bayésienne est une version moderne de l’approche inductive qui peut utiliser toute l’information disponible (résultats publiés, estimations des paramètres, croyances ou connaissances) afin de construire une hypothèse. Cette méthode a été développée au 18\\(^{\\mathrm{e}}\\) siècle. Cependant, comme les calculs sont très complexes, elle a été peu utilisée jusqu’au début des années 1990. Avec la venue d’ordinateurs puissants et d’algorithmes efficaces disponibles dans des logiciels libres (BUGS, WinBUGS, OpenBUGS, JAGS), cette branche des statistiques est en plein essor. Puisque le cours couvre les statistiques fréquentistes ou fishériennes, développées par Sir Ronald A. Fisher, nous ne discuterons pas plus des approches bayésiennes qui nécessitent une connaissance approfondie des statistiques. 3.1.1.4 Méthode hypothético-déductive Avec la méthode hypothético-déductive, au lieu de commencer avec une seule hypothèse, on considère plusieurs hypothèses de travail pouvant expliquer le phénomène d’intérêt (Figure 3.2). Chaque hypothèse est testée et peut être réfutée avec de nouvelles données. On élimine des hypothèses au fur et à mesure selon les données qui sont récoltées dans des études ou des expériences successives. L’explication la plus plausible est l’hypothèse qui a résisté à la falsification à plusieurs reprises. Figure 3.2: Schéma de l’approche hypothético-déductive en sciences. L’hypothèse qui est conforme aux prédictions est celle qui a résisté à la falsification après plusieurs expériences successives. La méthode hypothético-déductive comporte plusieurs avantages : Elle force à considérer plusieurs hypothèses dès le début. Elle illustre les différences de prédiction entre chaque hypothèse. Les explications simples sont les premières considérées, les plus complexes ensuite. Plusieurs hypothèses sont testées en même temps (l’induction teste une hypothèse à la fois). Toutefois, la méthode hypothético-déductive comporte un désavantage important : La méthode ne fonctionne pas si l’hypothèse correcte ne figure pas parmi les hypothèses énoncées avant l’expérience. En contrepartie, la méthode inductive peut commencer par une hypothèse incorrecte mais peut arriver à celle correcte après modification répétée de l’hypothèse suite à de nouvelles observations. 3.1.2 Hypothèse statistiques Alors que l’hypothèse scientifique est l’élément qui devrait avoir motivé l’expérience, l’hypothèse statistique est une formulation “mathématique” de l’hypothèse scientifique. En effet, dans les approches classiques de tests d’hypothèse statistique, on compte deux hypothèses, l’hypothèse nulle (\\(H_0\\)) et l’hypothèse alternative (\\(H_a\\)). L’hypothèse nulle (\\(H_0\\)) représente l’absence de différence ou l’absence d’un effet. Elle se base sur un test statistique et une distribution indiquant que la variabilité observée dans les données est due au hasard ou à l’erreur d’échantillonnage. En d’autres mots, l’hypothèse nulle suppose qu’il ne se passe rien. L’hypothèse alternative (\\(H_a\\)), quant à elle, est l’hypothèse statistique qui correspond à un effet ou une différence. C’est cette hypothèse qui indique qu’il se passe quelque chose. Le test d’hypothèse statistique consiste à essayer de rejeter \\(H_0\\). Si l’hypothèse nulle (\\(H_0\\)) est fausse, on la rejette et on se tourne vers l’hypothèse alternative (\\(H_a\\)). On parle toujours en termes de \\(H_0\\): on rejette \\(H_0\\) ou non. On ne peut pas accepter \\(H_0\\), seulement échouer de la rejeter faute de preuves. C’est ce système binaire d’hypothèses que les chercheurs utilisent depuis près d’un siècle. Toutefois, des méthodes alternatives ont été développées récemment qui permettent de comparer plusieurs hypothèses (&gt; 2) simultanément, à l’aide de la sélection de modèles. On réalise une expérience pour tester l’hypothèse scientifique suivante: la lumière abondante dans les coupes forestières favorise les espèces végétales intolérantes à l’ombre. Pour ce faire, on pourrait comparer la croissance moyenne de certaines espèces intolérantes à l’ombre dans 10 parcelles en coupes forestières et 10 parcelles en forêt sans coupe (témoins). Après avoir récolté les données, on pourrait procéder à une analyse statistique pour tester les hypothèses statistiques suivantes à propos des moyennes des deux groupes: \\(H_0\\) (hypothèse nulle): \\(\\mu_{coupes} = \\mu_{t\\acute{e}moins}\\) \\ \\(H_a\\) (hypothèse alternative): \\(\\mu_{coupes} \\neq \\mu_{t\\acute{e}moins}\\) Si le test statistique indique que la différence observée entre les parcelle coupées et témoins est peu probable pour un échantillon de même taille tiré à partir de la même population lorsque \\(H_0\\) est vraie, nous allons rejeter \\(H_0\\). En d’autres mots, on se base sur une distribution statistique qui reflète \\(H_0\\) pour situer la valeur observée de la statistique du test obtenue à partir de l’échantillon. Si cette valeur se trouve dans les extrémités de la distribution (c.-à-d., dans les queues), on va considérer cette valeur comme peu plausible lorsque \\(H_0\\) est vraie. On conclura alors qu’il y a une différence de croissance des espèces intolérantes à l’ombre dans les coupes et dans les témoins, ce qui confirmera notre hypothèse scientifique de départ. Pour déterminer si une différence est statistiquement significative (statistically significant) entre deux groupes ou deux échantillons, on doit procéder à un test statistique. Le test statistique est le processus par lequel on évalue la probabilité d’observer la valeur d’une statistique lorsque \\(H_0\\) est vraie pour une taille d’échantillon donnée dans une population. Par le fait même, nous effectuons un test d’hypothèse. Lorsqu’on compare les moyennes de deux groupes, il est peu probable d’observer exactement les mêmes valeurs dans les deux groupes. Il faut alors déterminer si la différence que nous avons observée est due à la variabilité naturelle dans la population ou à un effet réel (une vraie différence). Pour ce faire, il faut connaître la variabilité (la variance, \\(s^2\\)) dans chaque groupe. Il est normal que deux échantillons tirés de la même population soient différents en raison de la variabilité naturelle de la population et de l’erreur d’échantillonnage. Il faut un moyen de décider si la différence observée est due au hasard ou à un effet réel. Le seuil de signification (\\(\\alpha\\), significance level) est justement un critère qui est utilisé depuis longtemps pour distinguer entre le résultat du hasard et d’un effet réel. Par convention, on utilise \\(\\alpha = 0.01, 0.05\\) ou \\(0.10\\). Si la probabilité cumulative de la statistique du test utilisé est inférieure ou égale au seuil de signification (\\(P \\leq \\alpha\\)), on rejette \\(H_0\\). Ainsi, avec \\(P \\leq 0.05\\), on devrait observer une différence comme celle de l’échantillon lorsque \\(H_0\\) est vraie dans moins de 5 % des cas, si on répétait l’expérience. Autrement dit, si pour les caractéristiques des données le phénomène (\\(H_0\\)) est rare (c.-à-d., \\(P \\leq \\alpha\\)), on rejette \\(H_0\\). Par conséquent, on ne rejette pas \\(H_0\\) si le phénomène (\\(H_0\\)) est fréquent pour les caractéristiques des données de notre échantillon. On poursuit l’exemple précédent sur la croissance d’espèces végétales intolérantes à l’ombre dans des coupes et des parcelles témoins. Comme mentionné plus tôt, nous considérons les deux hypothèses suivantes et un seul de signification de \\(0.05\\): \\(H_0\\): \\(\\mu_{coupes} = \\mu_{t\\acute{e}moins}\\) \\(H_a\\): \\(\\mu_{coupes} \\neq \\mu_{t\\acute{e}moins}\\) \\(\\alpha = 0.05\\) On effectue un test statistique qui détermine la probabilité que l’échantillon provienne d’une population où \\(H_0\\) est vraie. Si d’après le test, on a une probabilité cumulative inférieure ou égale à 5 % (\\(P \\leq 0.05\\)) d’observer une différence comme celle dans notre échantillon lorsque \\(H_0\\) est vraie, on rejette \\(H_0\\). Autrement dit, si \\(P \\leq 0.05\\), il est peu probable d’observer une différence comme celle de notre échantillon lorsque \\(H_0\\) est vraie si on répète l’étude un grand nombre de fois. La déclaration que le résultat du test est statistiquement significatif apporte peu d’information. Il est préférable d’ajouter l’estimation du paramètre pour chaque groupe, comme la la moyenne \\(\\bar{x}\\) ainsi qu’une mesure de précision. Toutefois, le choix d’un seuil de signification (0.05, 0.01, 0.10) n’est pas sans conséquence – il influence notre probabilité de commettre des erreurs dans nos conclusions. 3.1.3 Erreurs de type I et II Le choix du seuil de signification influence la probabilité d’arriver à la bonne conclusion. Si on fixe \\(\\alpha\\) à 0.05, on va rejeter \\(H_0\\) si la probabilité d’observer une différence comme celle de l’échantillon (lorsque \\(H_0\\) est vraie) est inférieure à 0.05. Même avec la meilleure des intentions, on va donc rejeter \\(H_0\\) dans des cas où \\(H_0\\) est vraie avec une probabilité de \\(\\alpha\\). Bien qu’il existe des valeurs dans les extrémités d’une distribution lorsque \\(H_0\\) est vraie, notre choix de traiter ces valeurs comme des cas peu probables fait en sorte qu’on rejette parfois \\(H_0\\) alors que nous n’aurions pas dû la rejeter. Avec \\(\\alpha = 0.05\\), on va rejeter \\(H_0\\) à tort 5 % du temps (1 fois sur 20). On appelle ce type d’erreur, l’erreur de type I (erreur \\(\\alpha\\), type I error). C’est équivalent de déclarer un accusé “coupable” alors qu’il est “non coupable” (Table 3.1). De la même façon, à d’autres occasions, nous ne rejetterons pas \\(H_0\\) lorsqu’elle est fausse et qu’elle aurait dû être rejetée. Il s’agit de l’erreur de type II (erreur \\(\\beta\\), type II error) – on déclare l’accusé “non coupable” alors qu’il est “coupable”. La probabilité de commettre une erreur de type II peut se calculer à partir de formules ou de simulations, mais elle dépend de la taille de l’effet que l’on veut observer, de la taille de l’échantillon, de la variance des données, de la formulation de \\(H_0\\), du design expérimental et du type d’échantillonnage. Les calculs de la puissance ne seront pas présentés dans le cours. CONCLUSION À PARTIR DU TEST Table 3.1: Représentation des différents scénarios lors d’un test d’hypothèse nulle (\\(H_0\\)). On ne rejette pas \\(H_0\\) On rejette \\(H_0\\) RÉALITÉ \\(H_0\\) vraie décision correcte erreur de type I (\\(\\alpha\\)) (faux positif) \\(H_0\\) fausse erreur de type II (\\(\\beta\\)) (faux négatif) décision correcte (puissance = 1 - \\(\\beta\\)) 3.1.3.1 Puissance La puissance, définie comme étant \\(1 - \\beta\\), est la probabilité de rejeter correctement \\(H_0\\) lorsqu’elle est fausse. La puissance dépend de la taille de l’effet, de la taille de l’échantillon, de la variabilité des données, de l’hypothèse testée et du dispositif expérimental. Cette quantité peut être calculée à l’aide de données préliminaires avant d’effectuer l’échantillonnage afin de nous guider quant à la taille d’échantillon nécessaire étant donné l’effet que l’on veut mesurer (p. ex., les différences entre moyennes de groupes). Le calcul de la puissance doit s’effectuer a priori avec des données préliminaires. Le calcul a posteriori de la puissance, à partir des données de l’expérience ou de l’étude d’observation, est à proscrire. %D’ailleurs, l’usage des analyses de puissance a posteriori a fait l’objet de vives critiques dans les années 1990’s dans le monde statistique. Dans toute analyse, nous voulons que la puissance soit élevée (le plus près possible de 1). Les mesures suivantes permettent d’augmenter la puissance d’une analyse : augmenter l’erreur de type I – si \\(\\alpha\\) augmente, \\(\\beta\\) diminue, et par conséquent, \\(1 - \\beta\\) augmente; augmenter la taille d’échantillon; minimiser la variance des données. 3.1.3.2 Relation entre les erreurs de type I et II On doit viser de faibles erreurs de type I et II. Toutefois, les deux types d’erreur sont liées entre elles: lorsqu’on réduit un type d’erreur, l’autre augmente automatiquement. Le choix de réduire un type d’erreur plutôt qu’un autre dépend du type d’étude réalisée et de la nature de sa discipline. Par exemple, en pharmacologie, il est à l’avantage d’une compagnie pharmaceutique de fixer le seuil \\(\\alpha\\) à une valeur plus faible afin de réduire la probabilité de déclarer un médicament nocif, ce qui augmente l’erreur de type II. En biologie de la conservation, on veut réduire les chances de ne pas déclarer un effet sur une espèce en voie d’extinction lorsqu’il y a réellement un effet – pour ce faire, on augmentera l’erreur de type I en augmentant le seuil \\(\\alpha\\). Certains auteurs argumentent que l’erreur de type I est la plus sérieuse. L’erreur de type I est liée à une déclaration (basée sur le rejet de \\(H_0\\)) qu’un mécanisme complexe se produit dans le système étudié alors que ce n’est pas le cas, puisqu’on a incorrectement rejeté \\(H_0\\). D’autres chercheurs construiront leur expérience en se basant sur les résultats erronés. L’erreur de type II, quant à elle, représente notre incapacité à rejeter \\(H_0\\) alors que nous aurions dû la rejeter. Cependant, quelqu’un avec un meilleur dispositif expérimental ou avec plus de données pourra la rejeter plus tard. Dans le domaine médical ou environnemental, l’erreur de type II peut avoir de graves répercussions. Le meilleur moyen de réduire simultanément les deux types d’erreur est d’augmenter la taille de l’échantillon. 3.1.4 Utilisation de tests d’hypothèse Pour utiliser correctement les tests d’hypothèse, nous avons besoin de données qui proviennent d’un échantillon aléatoire. Toutefois, il faut être conscient que les tests \\(H_0\\) ne font pas l’unanimité puisque des problèmes potentiels sont associés à leur utilisation. Le choix du seuil \\(\\alpha\\), c’est-à-dire la probabilité d’erreur de type I, est arbitraire. Il a des conséquences sur les erreurs de type I et II, sur la puissance, et donc sur nos conclusions. Le choix du seuil \\(\\alpha\\) devrait se baser sur les conséquences associés à commettre une erreur de type I ou II. Par exemple, le coût d’une erreur de type I, comme pour le verdict de coupable pour un innocent, a de sérieuses conséquences pour un accusé de meurtre dans un pays avec peine de mort. Dans ce cas, il est souhaitable de réduire l’erreur de type I afin de diminuer le risque de rejeter \\(H_0\\) alors qu’elle n’aurait pas dû être rejetée. Par contre, pour un accusé d’excès de vitesse, les conséquences d’une erreur de type I sont beaucoup moindres. En effet, le verdict de culpabilité requiert peu de preuves et entraîne une amende. Dans ce cas, on peut se permettre d’augmenter l’erreur de type I. L’utilisation d’un seuil \\(\\alpha\\) rigide est déconseillée. Certains déclarent un effet à \\(P = 0.05\\), mais pas à \\(P = 0.051\\), même si le phénomène est toujours présent. L’utilisation de tests \\(H_0\\) pour des résultats évidents n’amène pas beaucoup d’information (test pour déterminer si un échantillon avec \\(\\bar{x} = 20\\) et \\(SE = 1.2\\) provient d’une population avec \\(\\mu = 2000\\)). Il faut faire attention à l’interprétation incorrecte de \\(P\\). \\(P\\) n’indique pas la probabilité que \\(H_0\\) soit vraie. En fait, lors d’une expérience, \\(H_0\\) est vraie ou fausse. La probabilité \\(P\\) indique la plausibilité des données de l’échantillon quand \\(H_0\\) est vraie. Plus cette valeur est faible (c.-à-d., \\(P &lt; \\alpha\\)), moins il y a de preuve que notre échantillon provienne d’une population où l’hypothèse nulle est vraie. Il est important de faire la distinction entre un effet statistiquement significatif et un effet biologiquement significatif. Avec un échantillon suffisamment grand, on peut rejeter n’importe quelle \\(H_0\\). Il est donc important de distinguer lorsque l’effet, bien que statistiquement significatif, n’a pas d’importance pratique ou scientifique. C’est à l’expérimentateur de décider si une différence statistiquement significative a une importance scientifique. Par exemple, nous effectuons une expérience de trois semaines sur la croissance de semis avec deux types d’engrais. À la fin de l’expérience, nous comparons la hauteur moyenne des semis des deux groupes. Si notre effet est de 0.01 mm (\\(\\bar{x}_{\\mathrm{engrais \\: A}} - \\bar{x}_{\\mathrm{engrais \\: B}} = 0.01 \\: \\mathrm{mm}\\)), est-ce suffisant pour déclarer l’engrais A meilleur que l’engrais B? Est-ce que le résultat justifiera d’acheter l’engrais A même s’il coûte trois fois plus cher que l’engrais B? Le non-rejet de \\(H_0\\) n’équivaut pas à l’absence d’un effet. Si la puissance est trop faible, il sera impossible de rejeter \\(H_0\\) lorsqu’elle est fausse. De plus, après un grand nombre de tests \\(H_0\\), il est normal de rejeter \\(H_0\\) même lorsqu’elle est vraie. Avec \\(\\alpha = 0.05\\), le rejet d’une seule \\(H_0\\) sur 20 tests effectués pourrait être le fruit du pur hasard et une conséquence directe du choix de \\(\\alpha\\). 3.1.4.1 Conseils sur la présentation des résultats de tests d’hypothèse La simple déclaration que l’hypothèse nulle a été rejetée amène peu d’information. Il est préférable d’au moins présenter la valeur du test statistique, les degrés de liberté qui y sont associés ainsi que la valeur exacte du \\(P\\). Ainsi, les lecteurs pourront juger par eux-mêmes de la pertinence de vos conclusions, qu’ils utilisent ou non le même seuil de signification que vous. Afin de bonifier l’utilisation des tests \\(H_0\\), il est fortement conseillé de joindre à tout test d’hypothèse les estimations des paramètres (p. ex., moyennes de groupes) ainsi que des mesures de précision de ces estimations, telles que des erreurs-types ou des intervalles de confiance. Ces valeurs sont nécessaires dans les méta-analyses, lesquelles sont des analyses effectuées lors d’une revue de littérature sur un thème particulier à partir des résultats d’articles publiés. 3.1.5 Estimation de paramètres Lorsque nous effectuons une analyse statistique, nous désirons parfois obtenir plus d’information que de simplement savoir s’il y a un effet ou non. Par exemple, quel est le taux de croissance de larves de grenouilles si nous augmentons la quantité de nourriture de 1 g par jour? Quel est le succès de germination lorsque nous utilisons une certaine variété de graines? Dans de tels cas, le problème de test d’hypothèse statistique devient un problème d’estimation de paramètre. La régression linéaire, où on estime une ordonnée à l’origine (axe des \\(y\\)) et une pente est un exemple de ce problème. On peut utiliser les intervalles de confiance pour mesurer l’incertitude de l’estimation. On réalise rapidement que les tests d’hypothèse et les intervalles de confiance sont liés: les deux utilisent un seuil \\(\\alpha\\). Ainsi, un intervalle de confiance qui inclut 0 correspond à \\(H_0: \\beta = 0\\). Nous reviendrons sur l’estimation de paramètres au cours des prochaines leçons. 3.1.6 Tests d’hypothèse sur la moyenne d’un seul groupe Les tests d’hypothèse effectués sur la moyenne d’un groupe permettent de déterminer si un échantillon appartient à une population donnée. Typiquement, nous utilisons le test \\(t\\) pour réaliser cette analyse. En d’autres mots, on teste la probabilité d’observer une valeur de \\(t\\) qui a la même valeur ou qui est supérieure à celle calculée à partir de notre échantillon si on refaisait l’expérience avec un grand nombre d’échantillons tirés d’une population où l’hypothèse nulle est vraie. Ce concept de “rééchantillonnage” explique d’où vient le nom de statistiques “fréquentistes”. Autrement dit, on base nos conclusions sur la fréquence du phénomène d’intérêt que l’on aurait observée si on avait récolté un grand nombre d’échantillons de la même taille que notre échantillon original à partir de la population où l’hypothèse nulle est vraie. Bien sûr, on travaille avec un seul échantillon (les données que l’on a récoltées), mais on détermine la fréquence du phénomène qui nous intéresse si on avait eu plusieurs échantillons tirés d’une population en accord avec l’hypothèse nulle. 3.1.6.1 Test bilatéral et test unilatéral Le test d’hypothèse peut être bilatéral (two-sided test) ou unilatéral (one-sided test). Dans le test bilatéral, on considère l’information contenue dans les deux queues de la distribution théorique (Figure 3.3a). À l’opposé, un test unilatéral s’intéresse spécifiquement à l’information contenue dans l’une des deux queues de la distribution (Figure 3.3b, c). Le choix d’exécuter un test unilatéral plutôt qu’un test bilatéral dépend des objectifs de l’analyse. Les deux prochains exemples illustrent le test unilatéral et le test bilatéral, respectivement. Figure 3.3: Tests d’hypothèse bilatéral (a) et unilatéral (b, c) associés à la distribution du \\(t\\) de Student avec 19 degrés de liberté (\\(df = 19\\)). On note qu’avec un seuil \\(lpha = 0.05\\), on détermine si la valeur observée est dans le 2.5 \\(%\\) des queues de la distribution (0.025 à gauche + 0.025 à droite \\(= 0.05\\)). Le test unilatéral, quant à lui, détermine si la valeur observée se trouve dans le 5 \\(%\\) d’une des deux queues. On effectue une étude en Montérégie afin de déterminer l’âge d’ours noirs que l’on veut comparer à la moyenne d’âge d’une autre population. On connaît l’âge moyen d’une population d’ours noirs en Abitibi, qui est de 7 ans. On récolte un échantillon de \\(n = 20\\) ours noirs en Montérégie et on détermine l’âge de chaque individu capturé. On peut ensuite effectuer un test d’hypothèse afin de savoir si l’âge moyen des ours qui ont été capturés en Montérégie diffère de l’âge moyen des ours noirs d’Abitibi. Ce test est bilatéral, puisqu’on s’intéresse à la fois à une différence positive ou négative entre les deux moyennes. \\(H_0: \\: \\mu = 7\\) (hypothèse de nulle ou de non-différence) \\(H_a: \\: \\mu \\neq 7\\) \\(\\alpha = 0.05\\) Données récoltées (âges d’ours): 18, 13, 17, 2, 8, 14, 8, 5, 9, 2, 8, 7, 7, 3, 19, 21, 20, 11, 7, 16 Si on teste sans spécifier si la moyenne des âges d’ours en Montérégie est supérieure ou inférieure à celle d’ours abitibiens, le test sera bilatéral. Nous pouvons utiliser le test \\(t\\) de Student pour nous assister dans notre décision de rejeter ou non \\(H_0\\) : \\[ t = \\frac{\\bar{x} - \\mu}{SE} = \\frac{10.8 - 7}{6.1/\\sqrt{20}} = 2.748 \\] On détermine ensuite la probabilité d’observer une valeur de \\(t\\) supérieure ou égale à celle qu’on a obtenue à partir de l’échantillon. Dans notre cas, on obtient \\(P(\\lvert t \\rvert \\geq 2.748) = 0.0128\\) – on a une probabilité de \\(\\sim 0.01\\) d’observer une valeur absolue de \\(t\\) supérieure ou égale à 2.748 dans une population où \\(H_0\\) est vraie. En d’autres mots, il est peu probable d’observer cette valeur dans une population où \\(H_0\\) est vraie. Puisque nous avons fixé le seuil \\(\\alpha\\) à 0.05, et que la probabilité est inférieure à ce seuil, nous rejetons donc \\(H_0\\). Étant donné le faible appui en faveur de cette hypothèse, nous concluons qu’il est peu probable que \\(H_0\\) soit vraie. Nous pourrons donc dire que l’âge moyen des ours en Montérégie n’est pas de 7 ans. Dans R, on peut trouver la solution en calculant le \\(t\\) à la main et en utilisant les fonctions telles que qt( ) et pt( ). Ainsi, la solution dans R est : ##on crée un vecteur avec les observations age.ours &lt;- c(18, 13, 17, 2, 8, 14, 8, 5, 9, 2, 8, 7, 7, 3, 19, 21, 20, 11, 7, 16) ##on calcule la moyenne moy.age.ours &lt;- mean(age.ours) ##on calcule l&#39;erreur-type SE &lt;- sd(age.ours)/sqrt(length(age.ours)) ##on calcule le t t.val &lt;- (moy.age.ours - 7)/SE t.val ## [1] 2.747787 ##on calcule les degrés de liberté df.ours &lt;- length(age.ours) - 1 ##on détermine P(t.val &gt;= 2.748) - queue à droite p.droite &lt;- 1 - pt(q = t.val, df = df.ours) ##on détermine P(t.val &lt;= -2.748) - queue à gauche p.gauche &lt;- pt(q = -t.val, df = df.ours) ##valeur du P dans les deux queues p.droite + p.gauche ## [1] 0.0127962 ##autre façon d&#39;obtenir le P bilatéral ##puisque t est symétrique 2 * p.droite ## [1] 0.0127962 Toutefois, on peut exécuter le test-\\(t\\) à l’aide de la fonction t.test( ) et obtenir exactement le même résultat. Cette fonction comporte un argument x pour spécifier les valeurs de l’échantillon, un argument mu pour indiquer la valeur de la moyenne de la population à laquelle nous comparons la moyenne de l’échantillon (valeur de \\(H_0\\)), ainsi qu’un argument alternative qui donne l’hypothèse alternative testée, ici pouvant prendre les valeurs \"two.sided\" (bilatérale), \"less\" (inférieure à \\(\\mu\\)), \"greater\" (supérieure à \\(\\mu\\)). t.test(x = age.ours, mu = 7, alternative = &quot;two.sided&quot;) ## ## One Sample t-test ## ## data: age.ours ## t = 2.7478, df = 19, p-value = 0.0128 ## alternative hypothesis: true mean is not equal to 7 ## 95 percent confidence interval: ## 7.893578 13.606422 ## sample estimates: ## mean of x ## 10.75 D’autres situations nous obligent à tester une hypothèse en dirigeant notre attention vers l’une des deux queues de la distribution, nous donnant ainsi un test unilatéral. C’est ce que nous montrons dans l’exemple suivant. On veut étudier l’efficacité d’un médicament sur la perte de masse. Pour ce faire, on sélectionne 12 patients chez qui on mesure la masse corporelle en kg. On administre un médicament qui stimule la perte de masse à chacun des 12 patients, à raison d’un comprimé par jour. Après un mois de la prise du médicament, on mesure la masse des mêmes individus. Les valeurs de l’échantillon montrent la différence entre la masse finale et initiale (kg). Une valeur positive indique un gain alors qu’une valeur négative est associée à une perte de masse :0.2, -0.5, -1.3, -1.6, -0.7, 0.4, -0.1, 0.0, -0.6, -1.1, -1.2, -0.8 \\(H_0: \\: \\mu \\geq 0\\) (il n’y a pas de perte de masse)\\ \\(H_a: \\: \\mu &lt; 0\\) (il y a perte de masse - à noter qu’un gain de masse indique que le médicament ne fonctionne pas)\\ \\(\\alpha = 0.05\\) On rejettera \\(H_0\\) uniquement s’il est rare de rencontrer des valeurs aussi faibles ou plus faibles que celle de la statistique observée pour notre échantillon (dans la queue de gauche). \\[ t = \\frac{\\bar{x} - \\mu}{SE} = \\frac{-0.608 - 0}{0.633/\\sqrt{12}} = -3.329 \\] ##on crée un vecteur avec les observations poids &lt;- c(0.2, -0.5, -1.3, -1.6, -0.7, 0.4, -0.1, 0.0, -0.6, -1.1, -1.2, -0.8) ##on calcule la moyenne moy.poids &lt;- mean(poids) ##on calcule l&#39;erreur-type SE &lt;- sd(poids)/sqrt(length(poids)) ##on calcule le t t.val &lt;- (moy.poids - 0)/SE t.val ## [1] -3.328513 ##on calcule les degrés de liberté df.poids &lt;- length(poids) - 1 ##on détermine P(t.val &lt;= -3.329) - queue à gauche p.gauche &lt;- pt(q = t.val, df = df.poids) p.gauche ## [1] 0.00336448 On peut également utiliser t.test( ) pour exécuter l’analyse de façon plus succincte: t.test(x = poids, mu = 0, alternative = &quot;less&quot;) ## ## One Sample t-test ## ## data: poids ## t = -3.3285, df = 11, p-value = 0.003364 ## alternative hypothesis: true mean is less than 0 ## 95 percent confidence interval: ## -Inf -0.2801098 ## sample estimates: ## mean of x ## -0.6083333 On conclut que la probabilité d’observer une valeur de \\(t\\) de -3.329 dans un échantillon de 12 observations tiré d’une population où la moyenne est de 0 est très faible (c.-à-d., \\(P(t \\leq -3.329) = 0.0034\\)). En d’autres mots, il est peu probable d’observer une valeur de \\(t\\) aussi faible que celle de l’échantillon lorsque la moyenne de la population est de 0. Ainsi, nous rejetons \\(H_0\\) et concluons que le médicament a stimulé la perte de masse. 3.1.6.2 Suppositions Chaque analyse statistique comporte des suppositions (assumption) ou conditions qu’il faut respecter afin que les résultats du test soient valides. La vérification des suppositions fait partie de toute bonne analyse statistique. Si l’expérimentateur néglige cette étape importante, il sera impossible de faire la distinction entre des résultats pertinents et ceux qui devraient être mis à la poubelle. Le test-\\(t\\) sur un groupe suppose : que les données sont indépendantes, c’est-à-dire, qu’elles sont le résultat d’un échantillonnage aléatoire; que les erreurs (résidus) suivent une distribution normale; que la moyenne provient d’une distribution normale de moyennes. Ces suppositions doivent être vérifiées. 3.1.6.2.1 Indépendance des observations L’indépendance des observations est une condition sine qua non à la réalisation des analyses classiques en statistiques. Si cette condition n’est pas respectée, la variabilité des données sera sous-estimée. De ce fait, on risque de rejeter plus facilement \\(H_0\\) et d’arriver à des conclusions erronées. Toutes les analyses que nous voyons dans le cours, incluant celles des prochaines leçons, requièrent que l’indépendance des observations soit respectée. À noter que des observations prises sur les mêmes unités, mais à des périodes différentes, ne sont pas indépendantes. Par exemple, mesurer la hauteur des mêmes arbres sur 5 années consécutives ou calculer le pourcentage de couvert d’herbacées dans des quadrats le long du même transect ne sont pas des stratégies d’échantillonnage qui mènent à des observations indépendantes. Le meilleur moyen de s’assurer de l’indépendance des observations est d’utiliser une bonne stratégie d’échantillonnage. Certaines stratégies d’échantillonnage qui permettent d’obtenir un échantillon constitué d’observations indépendantes ont fait l’objet de la leçon précédente. 3.1.6.2.2 La normalité des résidus et des moyennes On peut utiliser les résidus de l’analyse pour tester la normalité. On entend par résidus les erreurs ou la déviation des observations par rapport aux valeurs prédites par le modèle. Dans le cas du test-\\(t\\), le modèle estime la moyenne de l’échantillon. Les résidus provenant du test-\\(t\\) sont donc la différence entre les valeurs observées (\\(x_i\\)) et les valeurs prédites (\\(\\bar{x}\\)) : \\[ \\epsilon_i = x_i - \\bar{x} \\] Dans l’exemple sur la perte de masse suite à la prise d’un médicament, on caculerait ainsi les résidus : ##on crée un vecteur avec les observations residus &lt;- poids - mean(poids) residus ## [1] 0.808333333 0.108333333 -0.691666667 -0.991666667 -0.091666667 1.008333333 0.508333333 0.608333333 ## [9] 0.008333333 -0.491666667 -0.591666667 -0.191666667 La vérification de la normalité peut se faire à l’aide de méthodes formelles telles que les tests d’Anderson-Darling, de Cramér-von Mises, ou de Shapiro-Wilk. L’hypothèse nulle de ces tests correspond au respect de la supposition de normalité (\\(H_0\\): la normalité est respectée). Le rejet de cette hypothèse indique que les résidus ne suivent pas la distribution normale. Plusieurs tests de normalité sont disponibles dans dans le package (banque de fonctions) . Étant donné que ce package n’est pas distribué avec l’installation de base de , nous devons l’installer avant de pouvoir l’utiliser. Le document Introduction à R - les packages illustre toutes les étapes pour y arriver. ##on charge le package library(nortest) ##on fait le test d&#39;Anderson-Darling ad.test(residus) ## ## Anderson-Darling normality test ## ## data: residus ## A = 0.18668, p-value = 0.8815 ##on fait le test de Cramer - von Mises cvm.test(residus) ## ## Cramer-von Mises normality test ## ## data: residus ## W = 0.027575, p-value = 0.8641 ##on fait le test de Shapiro-Wilk shapiro.test(residus) ## ## Shapiro-Wilk normality test ## ## data: residus ## W = 0.96694, p-value = 0.8763 Dans ce cas, les trois tests suggèrent que les résidus suivent une distribution normale. Toutefois, chaque test a ses particularités. Certains sont sensibles à la trop faible ou à la trop grande taille de l’échantillon ou à la présence de valeurs extrêmes. Des méthodes graphiques informelles deviennent alors utiles, particulièrement pour des analyses statistiques plus complexes comme nous verrons plus tard dans le cours. Par exemple, le graphique quantile-quantile est un outil efficace pour évaluer la normalité (Figure 3.4). On peut l’obtenir facilement dans R : ##un graphique quantile-quantile qqnorm(residus, main = &quot;Graphique quantile-quantile&quot;, ylab = &quot;Quantiles de l&#39;échantillon&quot;, xlab = &quot;Quantiles théoriques&quot;) ##on ajoute la droite théorique qqline(residus) Figure 3.4: Graphique quantile-quantile pour évaluer la normalité des résidus du test-\\(t\\) sur les données de perte de masse (a) et un échantillon qui ne suit pas la normalité (b). Le graphique quantile-quantile compare les quantiles de la distribution des résidus à celle d’une distribution normale. On entend par quantile, par exemple, les percentiles des observations. Les quantiles observés sont comparés aux quantiles d’une distribution normale. Lorsque les quantiles observés correspondent à une distribution normale, on devrait voir une droite de \\(1:1\\) à 45\\(^\\circ\\). L’ajout d’une droite théorique à l’aide de qqline( ) permet d’évaluer si les quantiles observés ont une distribution normale. Si la plupart des points sont sur la droite, on peut conclure que les résidus ont une distribution normale. D’ailleurs c’est le cas des résidus de la perte de masse de notre exemple (Figure 3.4a). Le graphique de la figure 3.4b présente un autre échantillon et suggère que les valeurs ne suivent pas la distribution normale. On peut refaire l’exemple pour les résidus provenant du test-\\(t\\) sur l’âge des ours noirs de Montérégie (Figure 3.5: ##on crée un vecteur avec les observations age.ours &lt;- c(18, 13, 17, 2, 8, 14, 8, 5, 9, 2, 8, 7, 7, 3, 19, 21, 20, 11, 7, 16) ##on calcule la moyenne moy.age.ours &lt;- mean(age.ours) ##on calcule les résidus residus.ours &lt;- age.ours - moy.age.ours ##un graphique quantile-quantile qqnorm(residus.ours, main = &quot;Graphique quantile-quantile&quot;, ylab = &quot;Quantiles de l&#39;échantillon&quot;, xlab = &quot;Quantiles théoriques&quot;) ##on ajoute la droite théorique qqline(residus.ours) Figure 3.5: Graphique quantile-quantile pour évaluer la normalité des résidus du test-\\(t\\) de l’âge d’ours de Montérégie. La supposition à l’effet que la moyenne provienne d’une distribution normale de moyennes ne peut être vérifiée formellement. Cette supposition découle du théorème de la limite centrale. Ce dernier indique que les moyennes d’échantillons indépendants tirés d’une même population constituent une distribution normale. 3.1.7 Conclusion Dans ce texte, nous avons abordé la démarche scientifique, ainsi que l’élaboration d’hypothèses scientifiques et d’hypothèses statistiques. Nous avons ensuite présenté deux types d’erreur associées à l’exécution d’un test d’hypothèse: l’erreur de type I et l’erreur de type II. Le seuil de signification (\\(\\alpha\\)) détermine la probabilité de rejeter faussement une hypothèse nulle qui est vraie (erreur de type I), alors que la probabilité de ne pas rejeter une hypothèse nulle qui est fausse est donnée par \\(\\beta\\) (erreur de type II). La puissance est la probabilité de correctement rejeter une hypothèse nulle qui est fausse et est donnée par \\(1 - \\beta\\). Le meilleur moyen d’augmenter la puissance d’un test est d’augmenter la taille d’échantillon et de minimiser la variance. Comme première application d’un test d’hypothèse statistique, nous avons présenté le test-\\(t\\) unilatéral et bilatéral sur un groupe, les suppositions sous-jacentes à ce test, ainsi que des outils permettant de vérifier ces suppositions. Par prédiction, on entend une généralisation d’un phénomène qui permet de prédire le comportement d’une variable d’intérêt pour une condition donnée. Par exemple, si on effectue une étude sur la pression artérielle et la masse corporelle, on pourrait par la suite prédire la pression artérielle pour une masse corporelle de 50 kg.↩︎ "],["les-packages-avec-r.html", "3.2 Les packages avec R", " 3.2 Les packages avec R Toutes les fonctions de R sont regroupées en bibliothèques de fonctions appelées packages. Ces bibliothèques de fonctions permettent de réaliser des tâches particulière regroupées sous un même thème (p. ex., faire des graphiques, travailler avec des données géoréférencées). Par exemple, plusieurs fonctions graphiques sont incluses dans le package graphics, alors que le package base comprend des fonctions de base permettant à R d’agir comme un langage, et que le package stats regroupe plusieurs fonctions pour exécuter des analyses statistiques classiques. Lors de l’installation de R, une série de packages sont installés par défaut. Quelques packages sont activés au démarrage de R, et chacun contient des fonctions de base pour manipuler des objets, importer des fichiers, exécuter des analyses classiques, et créer plusieurs types de graphiques. Dès que l’on désire effectuer des manipulations, des analyses ou des graphiques plus complexes, ou accéder à un jeu de données dans un format moins conventionnel, on doit activer le package qui contient la fonction que l’on désire utiliser. 3.2.1 Activer un package Pour activer un package, il suffit d’utiliser la fonction library( ), en spécifiant le nom du package entre parenthèses. Par exemple, si on veut accéder à une fonction du package MASS, on procéderait comme suit: library(MASS) #pour charger le package MASS Pour connaître les packages déja installés sur notre ordinateur, on utilise la commande library( ), sans spécifier quoi que ce soit entre parenthèses. Autrement, il est aussi possible d’utiliser help.start( ) pour naviguer vers la page web des packages à l’aide d’hyperliens. En consultant la liste des packages sur cette page, on peut voir les fonctions incluses dans chacun des packages installés. 3.2.2 Installer un package Lorsque la fonction que nous voulons utiliser n’est pas incluse dans un des packages installés par défaut, nous devons télécharger le package contenant la fonction d’intérêt à partir du site de CRAN (Comprehensive R Archive Network) ou l’un de ses sites miroirs. Ce site a été décrit dans le premier document sur l’installation de R (Installation de R et choix d’un éditeur) et veuillez y référer pour de plus amples informations. %Une fois téléchargé, le package s’installe automatiquement Le nombre de packages disponibles augmente continuellement. Ces packages contiennent des fonctions créées par des utilisateurs et sont offerts aux utilisateurs à travers la planète afin de réaliser des tâches spécifiques dans R (Figure 3.6, 3.7). En date du 9 septembre 2019, on en comptait plus de 14900. Figure 3.6: Section du site de R et de ses sites miroirs réservée aux packages. Figure 3.7: Page des packages contenant des liens vers la liste complète des packages. La liste des packages par ordre alphabétique permet de visualiser rapidement tous les packages disponibles dans CRAN (Figure 3.8). Figure 3.8: Liste des packages par ordre alphabétique. Lorsqu’on cherche le package qui contient une fonction particulière dont on connaît le nom, par exemple, une fonction mentionnée dans un article scientifique, dans une page web ou par un collègue, on peut faire une recherche directement à l’aide de RSiteSearch( )12 puisque cette fonction va chercher dans les archives de R sur le web.}. Pour trouver une fonction qui réalise une tâche quelconque, on peut également faire une recherche directement sur le site Web http://search.r-project.org/ . Disons que l’on désire importer le fichier vers.xlsx dans R sans avoir à convertir ce fichier Excel en format texte. Nous devrons chercher s’il existe une fonction pour importer des fichiers Excel directement dans R. Pour trouver cette fonction, on écrit \"read Excel files\" dans la fenêtre de recherche (Figure 3.9). Figure 3.9: Résultat de RSiteSearch(‘read Excel files’). Parmi la liste des résultats, on observe que les trois premières entrées sont des fonctions qui permettent d’importer des fichiers Excel. Nous devons tenir compte du et de la auxquels ces entrées correspondent pour choisir un package. Nous attirons votre attention sur la troisième entrée qui réfère au package readxl (Figure 3.9). On peut par la suite aller voir directement dans la liste des packages de R pour le package readxl (Figure 3.10). Cette page contient plusieurs fichiers, dont le manuel (Reference manual) en format pdf. Ce document inclut les pages d’aide de toutes les fonctions de ce package. C’est en inspectant ce document que l’on peut comprendre comment utiliser les fonctions d’un nouveau package. Les sections d’exemples (Examples) illustrent l’utilisation des factions et s’avèrent particulièrement utiles. Figure 3.10: Page du package readxl avec l’archive des fichiers comportant toutes les fonctions codées en R (fichier tar.gz), ainsi que le manuel en pdf Dans notre cas, on remarquera la présence de la fonction read_excel() qui permet d’importer directement un jeu de données stocké dans un fichier Excel. À noter que le jeu de données doit tout de même respecter les conventions pour l’importation des jeux de données, telles que le nom des variables, la présence de données manquantes, le caractère spécifiant la décimale et la structure du fichier (format long). Ainsi, il faudra tout de même choisir la feuille du fichier Excel sur laquelle se trouvent les données. L’installation d’un package s’effectue à l’aide de la fonction install.packages( ) en spécifiant le nom du package d’intérêt entre guillemets à l’aide de l’argument pkgs. Il faut donc être branché au réseau internet lorsqu’on veut télécharger et installer un package sur notre système. On peut aussi spécifier l’adresse du site miroir de R avec l’argument repos, sans quoi, une fenêtre s’ouvrira nous demandant de sélectionner un site afin de télécharger les fichiers du package que l’on veut installer. install.packages(pkgs = &quot;readxl&quot;, repos = &quot;http://probability.ca/cran&quot;) Cette commande lance le téléchargement et l’installation du package readxl à partir du site de miroir de Toronto. Dans certains cas, des fonctions d’un package dépendent de fonctions provenant d’autres packages, c’est ce qu’on appelle la dépendance entre deux packages. La gestion des dépendances est faite automatiquement par R. En d’autres mots, si le package que nous désirons installer dépend d’un autre package, les deux seront installés automatiquement. Une fois installé, le package peut être utilisé à n’importe quel moment. Pour utiliser une fonction d’un package nouvellement installé, il faut tout d’abord activer le package à l’aide de la fonction library( ). library(readxl) Toutes les fonctions du package activé seront alors accessibles. Nous pourrons ensuite importer le fichier vers.xlsx: ##importation du fichier ##attention au bon chemin menant vers ##le fichier vers &lt;- read_excel(&quot;Module_3/data/vers.xlsx&quot;) head(vers) ## # A tibble: 6 × 7 ## Site Superficie Pente Vegetation pH.sol Humide Densite.vers ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 Nashs.Field 3.6 11 Prairie 4.1 FALSE 4 ## 2 Silwood.Bottom 5.1 2 Arable 5.2 FALSE 7 ## 3 Nursery.Field 2.8 3 Prairie 4.3 FALSE 2 ## 4 Rush.Meadow 2.4 5 Pre 4.9 TRUE 5 ## 5 Gunness.Thicket 3.8 0 Chaparral 4.2 FALSE 6 ## 6 Oak.Mead 3.1 2 Prairie 3.9 FALSE 2 3.2.3 Mettre à jour un package Les packages sont mis à jour de temps à autre par leurs auteurs et il est fortement conseillé de vérifier les mises à jour à l’aide de la fonction update.packages( ). Évidemment, il faut être branché à l’internet afin de réaliser cette opération. En utilisant cette commande sans rien spécifier entre parenthèses, R vérifiera la version des packages installés sur notre système et les comparera aux versions les plus récentes des packages dans les sites miroirs. Si les versions des packages sur notre système sont moins récentes que celles sur le site de R, chaque package sera identifié et il faudra indiquer si on désire mettre à jour chacun des packages pours lesquels il existe une mise à jour. Les mises à jour peuvent amener des correctifs à des fonctions déjà présentes ou ajouter de nouvelles fonctions. Par conséquent, il est fortement recommandé de faire la vérification de temps à autre. Une vérification deux à trois fois par année pourrait suffire si vous utilisez des packages établis depuis plusieurs années. Si vous employez des packages développés récemment (version &lt; 1), il est préférable de vérifier les mises à jour un peu plus souvent. 3.2.4 Désactiver un package Lorsque vous avez terminé d’utiliser un package que vous avez activé pendant votre session de travail, il est possible de le désactiver à l’aide de la fonction detach( ). Par exemple, pour désactiver le package readxl qui a été activé plus haut, on exécute la commande : detach(package:readxl) ##une fois le package désactivé, read_excel n&#39;est plus accessible vers &lt;- read_excel(&quot;vers.xlsx&quot;) #donne un message d&#39;erreur Avec près de 15000 packages de R disponibles sur CRAN, il est fort possible de trouver des fonctions ayant des noms identiques dans des packages différents. Cela peut entraîner des conflits entre certains packages lorsqu’ils sont activés lors de la même session. Dans de tels cas, la fonction du package activé en dernier masquera la fonction du même nom du package activé plus tôt pendant la session de travail. Pour éviter des problèmes, il est préférable d’activer uniquement les packages qui sont utilisés pendant une session de R donnée, plutôt que de systématiquement activer une multitude de packages à chaque fois qu’on travaille dans R. 3.2.5 Désinstaller un package En de très rares occasions, nous aurons besoin de désinstaller un package. La fonction remove.packages( ) permet de désinstaller un package spécifié avec l’argument pkgs. remove.packages(pkgs = &quot;readxl&quot;) Une raison possible de désinstaller un package est lorsqu’une mise à jour d’un package n’a pu être réalisée avec succès. Ceci serait indiqué par un message d’erreur lors de la mise à jour. Un changement de dépendances d’un package entre deux versions peut causer une telle erreur. Dans une pareille situation, il suffit de désinstaller l’ancienne version du package à l’aide de remove.packages( ) et d’installer la nouvelle version du package avec install.packages( ). Rappel: il faut être connecté à l’internet pour utiliser la fonction RSiteSearch( )↩︎ "],["exercices-3.html", "3.3 Exercices", " 3.3 Exercices 3.3.1 Question 1 a. Quelle est la différence entre une expérience et une étude d’observation? Réponse L’expérience implique l’assignation aléatoire des traitements aux unités expérimentales et on contrôle toutes les variables, en faisant varier uniquement les variables pour lesquelles on veut tester l’effet. Une étude d’observation implique une sélection aléatoire des unités expérimentales, mais les traitements ne sont pas appliqués aléatoirement (ils ont été appliqués par quelqu’un d’autre). Par exemple, si on s’intéresse à la vitalité des petits commerces dans les villes où des commerces de grande surface se sont installés, on peut sélectionner aléatoirement les villes qui seront prises en compte dans l’étude. Toutefois, la présence ou non d’un commerce de grande surface n’a pas été attribuée aléatoirement. De plus, on n’a aucun contrôle sur plusieurs variables dans l’étude d’observation (p. ex., population de la ville, nombre de petits commerces, distances entre les commerces). b. Quel est le terme utilisé pour désigner la probabilité de rejeter correctement une hypothèse nulle qui devrait être rejetée lors d’une analyse statistique? Réponse C’est la puissance statistique dont il est question ici. c. L’erreur de type I est la probabilité de rejeter par erreur une hypothèse nulle qui est vraie. Vrai ou faux? Réponse Vrai d. Donnez trois moyens pour augmenter la puissance statistique d’une analyse. Réponse Augmenter la taille d’échantillon, réduire la variabilité (en définissant mieux la population statistique d’intérêt) et augmenter le seuil \\(\\alpha\\) sont tous des moyens qui permettent d’augmenter la puissance d’une analyse statistique. e. Donnez deux suppositions du test \\(t\\) sur un seul groupe. Réponse La normalité des résidus et l’indépendance des observations sont les deux principales suppositions à vérifier. La troisième supposition, c’est-à-dire que la moyenne provient d’une distribution normale de moyennes, est un produit dérivé du théorème de la limite centrale. 3.3.2 Question 2 a. Importez le jeu de données temp.csv. Ce dernier contient les données sur la température de l’air pour différentes stations de mesures météorologiques (Site) dans l’Arctique. La colonne Diff.temp est le résultat de la différence entre la température maximale de 2010 et celle de 1950. Réponse ##importer fichier temp &lt;- read.table(file = &quot;Module_3/data/temp.csv&quot;, header = TRUE) ## ou temp &lt;- read.table(file = &quot;Module_3/data/temp.csv&quot;, header = TRUE, sep=&quot;,&quot;) ##on regarde les premières lignes du jeu de données head(temp) ## Site Diff.temp ## 1 site1 2.991231 ## 2 site2 5.526125 ## 3 site3 4.684332 ## 4 site4 8.547139 ## 5 site5 5.467885 ## 6 site6 6.274520 b. On émet l’hypothèse scientifique que le réchauffement climatique a augmenté les températures maximales dans l’Arctique. Émettez des hypothèses statistiques qui permettent de tester l’hypothèse scientifique et spécifiez un seuil de signification (\\(\\alpha\\)). Réponse Puisqu’on émet une hypothèse à l’effet que la température maximale était plus grande en 2010 qu’en 1950, nous nous attendrons à ce que la variable Diff.temp (température maximale 2010 - température maximale 1950) prenne des valeurs positives. Nous avons donc les hypothèses statistiques suivantes : \\(H_0\\): \\(\\mu \\leq 0\\) (la moyenne n’est pas supérieure à 0) \\(H_a\\): \\(\\mu &gt; 0\\) (la moyenne est supérieure à 0) On fixe le seuil de signification, au choix, \\(\\alpha = 0.05\\) c. Effectuez un test d’hypothèse à l’aide du test \\(t\\). Réponse ##on effectue le test t t.out &lt;- t.test(x = temp$Diff.temp, mu = 0, alternative = &quot;greater&quot;) d. Vérifiez les suppositions du test. Utilisez un test formel et une méthode graphique informelle pour vérifier la normalité. Réponse ##on calcule les résidus res &lt;- temp$Diff.temp - mean(temp$Diff.temp) ##si ce package n&#39;est pas installé, on l&#39;installe ##avec install.packages( ) ##une fois installé, on peut charger le package ##on charge le package nortest library(nortest) ad.test(res) ## ## Anderson-Darling normality test ## ## data: res ## A = 0.31088, p-value = 0.5438 ##on fait un graphique quantile-quantile qqnorm(res) ##on ajoute la droite théorique qqline(res) Le test d’Anderson-Darling indique que la supposition de normalité des résidus est respectée (\\(H_0\\) n’est pas rejetée avec un \\(P = 0.5438\\)). Le graphique quantile-quantile suggère également que la supposition de normalité est respectée. e. Interprétez les résultats de l’analyse à la lumière de l’hypothèse scientifique. Réponse ##on regarde le résultat t.out ## ## One Sample t-test ## ## data: temp$Diff.temp ## t = 10.627, df = 59, p-value = 1.256e-15 ## alternative hypothesis: true mean is greater than 0 ## 95 percent confidence interval: ## 4.418407 Inf ## sample estimates: ## mean of x ## 5.242877 On remarque que \\(P &lt; 0.0001\\) et on rejette \\(H_0\\). On peut donc conclure que la moyenne de la variable réponse est supérieure à 0 et que la température maximale en 2010 est significativement supérieure à celle de 1950. Puisque le test est unilatéral, l’intervalle de confiance est borné à l’infini du côté droit (\\(H_a\\): \\(\\mu &gt; 0\\), la moyenne est supérieure à 0). "],["tests-dhypothèse-sur-deux-groupes.html", "Module 4 Tests d’hypothèse sur deux groupes", " Module 4 Tests d’hypothèse sur deux groupes Dans la leçon précédente, nous avons discuté de la démarche scientifique et de l’élaboration d’hypothèses scientifiques et statistiques. Deux types d’erreurs associées à la réalisation d’un test d’hypothèse ont été présentées: l’erreur de type I et l’erreur de type II. Le seuil de signification (\\(\\alpha\\)) correspond à la probabilité de rejeter une hypothèse nulle qui est vraie (erreur de type I), alors que la probabilité de ne pas rejeter une hypothèse nulle qui aurait dû être rejetée est donnée par \\(\\beta\\) (erreur de type II). Nous avons vu le concept de la puissance statistique. Nous avons réalisé un exemple de test d’hypothèse statistique sur un échantillon (groupe) concernant la moyenne à l’aide du test \\(t\\). Ce test peut être unilatéral ou bilatéral. Le test unilatéral cible une seule des deux queues de la distribution alors que le test bilatérale cible les deux queues de la distribution. "],["théories-3.html", "4.1 Théories", " 4.1 Théories 4.1.1 Tests d’hypothèses sur la moyenne de deux groupes indépendants Comme nous l’avons vu précédemment, le but du test d’hypothèse sur un groupe est de déterminer si la moyenne d’une population diffère d’une valeur d’intérêt (test bilatéral) ou si la moyenne est inférieure ou supérieure à une valeur d’intérêt (test unilatéral). Ce même concept s’étend naturellement en présence de deux échantillons ou deux groupes, comme l’illustre l’exemple qui suit. On veut déterminer si les notes des élèves d’une école primaire sont différentes lorsqu’ils ont eu de l’aide aux devoirs pendant l’année comparativement à d’autres élèves n’ayant pas eu cette aide. Pour ce faire, on sélectionne aléatoirement 30 élèves qui recevront l’aide et 30 autres qui ne recevront pas l’aide. Les élèves auront le même enseignant et leurs capacités seront mesurées dans un examen ministériel uniforme valant 40 \\(\\%\\) (Table 4.1). \\(H_0\\): \\(\\mu_{\\mathrm{aide}} = \\mu_{\\mathrm{t\\acute{e}moins}}\\) (test bilatéral) \\(H_a\\): \\(\\mu_{\\mathrm{aide}} \\neq \\mu_{\\mathrm{t\\acute{e}moins}}\\) \\(\\alpha = 0.05\\) Aide aux devoirs Témoins Table 4.1: Notes sur 40 des élèves dans les deux groupes de l’expérience. 36.36 36.10 34.97 19.20 19.31 21.06 37.70 37.22 39.05 21.21 19.41 18.89 35.75 36.32 35.42 18.78 17.66 19.24 35.92 37.55 35.08 20.00 18.21 19.92 36.24 36.90 36.92 20.73 19.88 18.97 36.04 37.86 37.28 19.50 20.88 20.96 37.17 36.52 35.60 20.18 20.15 18.97 35.65 36.03 36.00 20.06 21.88 21.26 37.57 34.66 36.26 20.12 19.21 19.99 36.49 36.22 37.67 19.98 20.76 21.23 On obtient les statistiques descriptives suivantes : \\(\\bar{x}_{aide}\\) = 36.48 &amp; \\(\\bar{x}_{\\mathrm{t\\acute{e}moins}}\\) = 36.48 \\(s^2_{aide}\\) = 0.96 &amp; \\(s^2_{\\mathrm{t\\acute{e}moins}}\\) = 0.96 \\(n_{aide}\\) = 30 &amp; \\(n_{\\mathrm{t\\acute{e}moins}}\\)= 30 On peut utiliser un test \\(t\\) pour déterminer s’il y a une différence entre les moyennes des deux groupes. Dans le cas du test \\(t\\) sur un seul groupe, nous avons l’équation $t = $ où \\(\\bar{x}\\) correspond à la moyenne du groupe et \\(\\mu\\) représente la valeur à laquelle on désire comparer la moyenne. Comme nous l’avons vu à la leçon précédente, le test \\(t\\) sur un groupe suppose la normalité des résidus et l’indépendance des observations. En présence de deux groupes, le test \\(t\\) se calcule comme suit : \\[ t = \\frac{\\bar{x}_{\\mathrm{groupe \\: 1}} - \\bar{x}_{\\mathrm{groupe \\: 2}}}{SE_{\\bar{x}_{\\mathrm{groupe \\: 1}} - \\bar{x}_{\\mathrm{groupe \\: 2}}}} \\] Ici, les moyennes des deux groupes (\\(\\bar{x}_{\\mathrm{groupe \\: 1}}\\), \\(\\bar{x}_{\\mathrm{groupe \\: 2}}\\)) sont comparées entre elles. Le dénominateur, \\(SE_{\\bar{x}_{\\mathrm{groupe \\: 1}} - \\bar{x}_{\\mathrm{groupe \\: 2}}}\\) correspond à l’erreur-type de la différence des moyennes. Voyons maintenant comment calculer cette valeur. La variance de la différence des moyennes équivaut à la somme des variances séparées : \\[ \\sigma^2_{\\bar{x}_{\\mathrm{groupe \\: 1}} - \\bar{x}_{\\mathrm{groupe \\: 2}}} = \\sigma^2_{\\mathrm{groupe \\: 1}} + \\sigma^2_{\\mathrm{groupe \\: 2}} \\] En plus des suppositions de normalité des résidus et de l’indépendance des observations, le test \\(t\\) sur deux groupes indépendants requiert que les variances des deux groupes soient égales (\\(\\sigma^2_{\\mathrm{groupe \\: 1}} = \\sigma^2_{\\mathrm{groupe \\: 2}}\\)). On parle alors d’homogénéité des variances. Il faut donc estimer la variance commune aux deux groupes. La meilleure estimation de cette variance commune consiste à calculer une variance combinée ou “poolée” (pooled variance) : \\[ s^2_{\\mathrm{combin\\acute{e}e}} = \\frac{(n_{\\mathrm{groupe \\: 1}} - 1)s^2_{\\mathrm{groupe \\: 1}} + (n_{\\mathrm{groupe \\: 2}} - 1)s^2_{\\mathrm{groupe \\: 2}}}{n_{\\mathrm{groupe \\: 1}} + n_{\\mathrm{groupe \\: 2}} - 2} \\:. \\] On utilise cette variance pour calculer l’erreur-type des différences des moyennes : \\[ SE_{\\bar{x}_{\\mathrm{groupe \\: 1}} - \\bar{x}_{\\mathrm{groupe \\: 2}}} = \\sqrt{\\frac{s^2_{\\mathrm{combin\\acute{e}e}}}{n_{\\mathrm{groupe \\: 1}}} + \\frac{s^2_{\\mathrm{combin\\acute{e}e}}}{n_{\\mathrm{groupe \\: 2}}}} \\:. \\] Maintenant, reprenons notre exemple sur le succès scolaire des élèves en considérant la variance combinée. On peut effectuer un test \\(t\\) pour comparer les deux groupes d’élèves de l’exemple 4.1. Commençons par calculer la variance combinée qui sera nécessaire pour obtenir le dénominateur du test \\(t\\) sur deux groupes : \\[ s^2_{\\mathrm{combin\\acute{e}e}} = \\frac{(n_{\\mathrm{aide}} - 1)s^2_{\\mathrm{aide}} + (n_{\\mathrm{t\\acute{e}moins}} - 1)s^2_{\\mathrm{t\\acute{e}moins}}}{n_{\\mathrm{aide}} + n_{\\mathrm{t\\acute{e}moins}} - 2} \\] \\[ s^2_{\\mathrm{combin\\acute{e}e}} = \\frac{(30 - 1) \\cdot 0.96 + (30 - 1) \\cdot 0.98}{30 + 30 - 2} \\] \\[ s^2_{\\mathrm{combin\\acute{e}e}} = 0.97 \\] Ensuite calculons l’erreur-type sur la différence des moyennes : \\[ SE_{\\bar{x}_{\\mathrm{aide}} - \\bar{x}_{\\mathrm{t\\acute{e}moins}}} = \\sqrt{\\frac{s^2_{\\mathrm{combin\\acute{e}e}}}{n_{\\mathrm{aide}}} + \\frac{s^2_{\\mathrm{combin\\acute{e}e}}}{n_{\\mathrm{t\\acute{e}moins}}}} \\] \\[ SE_{\\bar{x}_{\\mathrm{aide}} - \\bar{x}_{\\mathrm{t\\acute{e}moins}}} = \\sqrt{\\frac{0.97}{30} + \\frac{0.97}{30}} \\\\ SE_{\\bar{x}_{\\mathrm{aide}} - \\bar{x}_{\\mathrm{t\\acute{e}moins}}} = 0.25 \\] La statistique du test s’obtient ainsi : \\[ t = \\frac{\\bar{x}_{\\mathrm{aide}} - \\bar{x}_{\\mathrm{t\\acute{e}moins}}}{SE_{\\bar{x}_{\\mathrm{aide}} - \\bar{x}_{\\mathrm{t\\acute{e}moins}}}} \\] \\[ t = \\frac{36.48 - 19.92}{0.25}\\\\ t = 65.217 \\] Nous déterminons la probabilité d’observer une valeur \\(t = 65.217\\) dans des populations où \\(H_0\\) est vraie, soit l’énoncé \\(P(\\lvert t_{df = 58} \\rvert \\ge 65.217)\\). On peut consulter la distribution du \\(t\\) de Student avec 58 degrés de liberté (\\(df = n_{\\mathrm{aide}} + n_{\\mathrm{t\\acute{e}moins}} - 2\\)) dans un livre de statistique ou encore avec à l’aide de . On constate que la valeur observée \\(t = 65.217\\) est très peu probable lorsque \\(H_0\\) est vraie, puisque \\(P(\\lvert t_{df = 58} \\rvert \\ge 65.217) &lt; 0.0001\\). On rejette donc \\(H_0\\) et on conclut que les deux moyennes diffèrent, c’est-à-dire que la moyenne des notes des élèves ayant eu l’aide aux devoirs diffère de celle des élèves n’ayant pas eu cette aide. Dans R, il est bien sûr possible de réaliser le tout plus succinctement. Le jeu de données est stocké dans le fichier eleves.txt. Il faudra importer ce fichier avant de pouvoir accéder aux données : eleves &lt;- read.table(file = &quot;Module_4/data/eleves.txt&quot;, header = TRUE) ##premières observations head(eleves) ## Note Type ## 1 36.36 aide ## 2 37.70 aide ## 3 35.75 aide ## 4 35.92 aide ## 5 36.24 aide ## 6 36.04 aide Tout comme pour le test \\(t\\) sur un groupe, la fonction t.test( ) permet de réaliser le test \\(t\\) sur deux groupes indépendants. Dans notre cas, nous utiliserons des arguments additionnels, notamment var.equal = TRUE, qui spécifie la supposition de variances homogènes entre les deux groupes. De plus, nous pourrons aussi utiliser une syntaxe sous forme de formule (c.-à-d., Note \\(\\sim\\) Type) pour indiquer la variable réponse (Note), la variable qui distingue les élèves avec aide aux devoirs et les témoins (Type) et l’argument data. L’argument data permet d’indiquer à R dans quel objet se trouvent les variables employées dans l’analyse. En présence de cet argument, il n’est pas nécessaire d’utiliser le symbole $ à l’intérieur de la fonction t.test( ) – eleves$Note \\(\\sim\\) eleves$Type donnera la même chose que Note \\(\\sim\\) Typ, data = eleves. Cette syntaxe est utilisée dans la plupart des analyses statistiques de R. ##on exécute le test t pour deux groupes t.test(Note ~ Type, data = eleves, var.equal = TRUE) ## ## Two Sample t-test ## ## data: Note by Type ## t = 65.217, df = 58, p-value &lt; 2.2e-16 ## alternative hypothesis: true difference in means between group aide and group temoins is not equal to 0 ## 95 percent confidence interval: ## 16.0556 17.0724 ## sample estimates: ## mean in group aide mean in group temoins ## 36.484 19.920 On remarque que t.test( ) donne le test \\(t\\) bilatéral par défaut, comme c’est le cas pour le test \\(t\\) sur un groupe. Nous pouvons facilement choisir un test unilatéral en modifiant l’argument alternative. Nous verrons un exemple de test unilatéral plus loin dans ce document (exemple 4.4). À noter que la probabilité retournée par t.test( ) est très faible (p.value &lt; 2.2e-16). Par convention, on indique dans un rapport présentant des résultats statistiques que la probabilité est inférieure à une petite valeur, telle que \\(P &lt; 0.0001\\) ou \\(P &lt; 0.001\\). Même si la probabilité est très faible, on n’indique jamais que \\(P = 0\\). Il est erroné d’écrire \\(P = 0\\) puisque cela équivaut à dire que l’événement est impossible. Il faut distinguer entre impossible (ne se produira jamais) et peu probable (peut se produire avec une très faible probabilité). Par analogie, la probabilité de gagner à une loterie est peut-être faible (\\(P &lt; 0.0001\\)) mais non nulle si on a acheté un billet. Par contre, si on n’achète aucun billet, on n’a aucune chance de gagner et, dans ce cas, on peut écrire \\(P = 0\\). 4.1.1.1 Suppositions Les suppositions ou conditions à respecter pour le test \\(t\\) à deux groupes sont les suivantes : l’indépendance des observations: les deux échantillons sont indépendants et les observations proviennent d’un échantillonnage aléatoire; la normalité des résidus: les deux groupes proviennent de populations normales et leurs erreurs suivent une distribution normale; l’homogénéité des variances: les deux groupes ont la même variance. 4.1.1.1.1 Indépendance des observations Le meilleur moyen de respecter cette condition est d’utiliser un design complètement aléatoire afin de récolter les données. Comme nous l’avons vu dans la leçon précédente, des mesures répétées sur les mêmes unités avant et après un traitement ne constituent pas des observations indépendantes. 4.1.1.1.2 Normalité des résidus Afin de vérifier la normalité des résidus, nous pouvons utiliser les mêmes diagnostics que ceux présentés pour le test \\(t\\) à un groupe. Les tests formels ou les méthodes graphiques peuvent nous aider à détecter des déviations de cette condition. Dans notre exemple sur le succès scolaire des élèves, nous pourrions procéder comme suit : ##on crée un sous-jeu de données pour les élèves ayant eu l&#39;aide aux devoirs eleves.aide &lt;- eleves[eleves$Type == &quot;aide&quot;, &quot;Note&quot;] eleves.aide ## [1] 36.36 37.70 35.75 35.92 36.24 36.04 37.17 35.65 37.57 36.49 36.10 37.22 36.32 37.55 36.90 37.86 36.52 ## [18] 36.03 34.66 36.22 34.97 39.05 35.42 35.08 36.92 37.28 35.60 36.00 36.26 37.67 ##on crée un sous-jeu de données les élèves témoins eleves.temoins &lt;- eleves[eleves$Type == &quot;temoins&quot;, &quot;Note&quot;] eleves.temoins ## [1] 19.20 21.21 18.78 20.00 20.73 19.50 20.18 20.06 20.12 19.98 19.31 19.41 17.66 18.21 19.88 20.88 20.15 ## [18] 21.88 19.21 20.76 21.06 18.89 19.24 19.92 18.97 20.96 18.97 21.26 19.99 21.23 ##résidus du groupe avec aide aux devoir res.aide &lt;- eleves.aide - mean(eleves.aide) ##résidus du groupe témoins res.temoins &lt;- eleves.temoins - mean(eleves.temoins) ##on combine les résidus des deux groupes res.combo &lt;- c(res.aide, res.temoins) ##test formel Anderson-Darling library(nortest) ad.test(res.combo) ## ## Anderson-Darling normality test ## ## data: res.combo ## A = 0.3122, p-value = 0.5408 ##graphique quantile-quantile qqnorm(res.combo, ylab = &quot;Quantiles observées&quot;, xlab = &quot;Quantiles théoriques&quot;, main = &quot;Graphique quantile-quantile&quot;) qqline(res.combo) Figure 4.1: Graphique quantile-quantile à partir des résidus du test \\(t\\) sur deux groupes indépendants effectué sur les données du succès scolaire des élèves. Les diagnostics suggèrent que la supposition de normalité des résidus est bien respectée (Figure. 4.1}). En effet, le résultat du test de Anderson-Darling (p.value = 0.5408) ne permet pas de rejeter l’hypothèse nulle selon laquelle les résidus suivent une distribution normale avec un seuil \\(\\alpha = 0.05\\). De plus, les quantiles des observations s’alignent bien avec les quantiles théoriques d’une distribution normale. 4.1.1.1.3 Homogénéité de la variance Le test \\(t\\) sur deux groupes indépendants suppose que les deux groupes comparés ont des variances égales. C’est ce qu’on appelle la supposition d’homogénéité de la variance, aussi appelée homoscédasticité (homogeneity of variance, homoscedasticity, homoskedasticity). Lorsque deux groupes ont des variances inégales, nous dirons que l’homogénéité de la variance n’est pas respectée, ou de façon équivalente, que les variances sont hétérogènes (heterogeneous variances, heteroscedasticity, heteroskedasticity). Cette supposition est très importante et il est primordial de la vérifier. On peut évaluer si cette supposition est valide à l’aide de méthodes formelles telles que le test de Fligner-Killeen (fligner.test( )), de Bartlett (bartlett.test( )) ou celui de Levene (leveneTest( )) du package car. Toutefois, ces tests formels peuvent être sensibles à la non-normalité, c’est-à-dire que le rejet de l’hypothèse nulle (\\(H_0\\) : homogénéité de la variance) peut être dû à des déviations de la normalité plutôt qu’à l’hétérogénéité de la variance. Ainsi, nous utiliserons plutôt des méthodes graphiques pour diagnostiquer les problèmes d’hétérogénéité de la variance. La méthode graphique la plus simple consiste à observer le patron de la variance à l’aide des résidus. Puisque nous n’avons que deux groupes, nous pouvons utiliser un diagramme de boîtes et moustaches (boxplot). Si la condition d’homogénéité de la variance est respectée, nous observerons deux boîtes de même hauteur sur le graphique. ##on met les résidus dans le jeu de données eleves$res.combo &lt;- res.combo ##on crée le graphique boxplot(res.combo ~ Type, data = eleves) Figure 4.2: Diagramme de boîtes et moustaches à partir des résidus du test \\(t\\) sur deux groupes indépendants effectué sur les données du succès scolaire des élèves. On voit que les boîtes des deux groupes sont très semblables (la distance interquartile est similaire). On peut donc procéder avec le test \\(t\\) (Figure 4.2). Dans d’autres cas, les variances des deux groupes sont différentes et la supposition d’homogénéité de la variance n’est pas respectée (Figure 4.3). Lorsqu’il y a un doute sur l’interprétation du diagramme, il vaut mieux utiliser des tests qui ne nécessitent pas l’homogénéité de la variance (voir la prochaine section). Figure 4.3: Diagrammes de boîtes et moustaches sur les résidus de groupes avec variances inégales. On remarque que les boîtes ont des hauteurs différentes (distance interquartile différente). Dans chaque diagramme, le groupe 1 varie moins que le groupe 2. 4.1.1.1.4 Robustesse du test t Le test \\(t\\) est robuste aux déviations, particulièrement si les deux groupes sont de tailles identiques et si on teste des hypothèses bilatérales. La robustesse du test augmente avec la taille de l’échantillon. Lorsque la supposition d’homogénéité de la variance ne peut être respectée, une version modifiée du test \\(t\\) peut être utilisée, le test \\(t\\) de Welch (Welch’s \\(t\\)-test). C’est d’ailleurs ce test \\(t\\) qui est exécuté par défaut, avec t.test( ) (c.-à-d., l’argument var.equal prend la valeur FALSE par défaut). L’exemple suivant illustre ce qu’on obtient avec cette modification du test \\(t\\). Le test \\(t\\) de Welch est une version modifiée du test \\(t\\) classique et est adapté aux situations de variances hétérogènes. Alors que le test \\(t\\) utilise un terme de variance combinée, le test \\(t\\) de Welch inclut les variances des deux groupes dans le calcul de l’erreur-type sur la différence des moyennes : \\[ SE_{\\bar{x}_{\\mathrm{groupe \\: 1}} - \\bar{x}_{\\mathrm{groupe \\: 2}}} = \\sqrt{\\frac{s^2_{\\mathrm{groupe \\: 1}}}{n_{\\mathrm{groupe \\: 1}}} + \\frac{s^2_{\\mathrm{groupe \\: 2}}}{n_{\\mathrm{groupe \\: 2}}}} \\] Le calcul des degrés de liberté est plus complexe avec le test \\(t\\) de Welch : \\[ df = \\frac{\\left(\\frac{s^2_{\\mathrm{groupe \\: 1}}}{n_{\\mathrm{groupe \\: 1}}} + \\frac{s^2_{\\mathrm{groupe \\: 2}}}{n_{\\mathrm{groupe \\: 2}}}\\right)}{\\frac{\\left(\\frac{s^2_{\\mathrm{groupe \\: 1}}}{n_{\\mathrm{groupe \\: 1}}}\\right)^2}{n_{\\mathrm{groupe \\: 1}} - 1} + \\frac{\\left(\\frac{s^2_{\\mathrm{groupe \\: 2}}}{n_{\\mathrm{groupe \\: 2}}}\\right)^2}{n_{\\mathrm{groupe \\: 2}} - 1}} \\: \\] On peut comparer la différence entre les résultats du test \\(t\\) de Student et ceux du test \\(t\\) de Welch concernant les données relatives au succès scolaire des élèves : ##test t de Student t.test(Note ~ Type, data = eleves, var.equal = TRUE) ## ## Two Sample t-test ## ## data: Note by Type ## t = 65.217, df = 58, p-value &lt; 2.2e-16 ## alternative hypothesis: true difference in means between group aide and group temoins is not equal to 0 ## 95 percent confidence interval: ## 16.0556 17.0724 ## sample estimates: ## mean in group aide mean in group temoins ## 36.484 19.920 ##test t de Welch t.test(Note ~ Type, data = eleves, var.equal = FALSE) ## ## Welch Two Sample t-test ## ## data: Note by Type ## t = 65.217, df = 57.993, p-value &lt; 2.2e-16 ## alternative hypothesis: true difference in means between group aide and group temoins is not equal to 0 ## 95 percent confidence interval: ## 16.0556 17.0724 ## sample estimates: ## mean in group aide mean in group temoins ## 36.484 19.920 On constate que les résultats des tests sont pratiquement les mêmes puisque la supposition d’homogénéité de la variance est respectée. Seuls les degrés de libertés diffèrent légèrement. On effectue une expérience sur le temps passé à regarder son téléphone portable pour des étudiants de 19 à 23 ans dans deux situations distinctes : dîner avec des étudiants du même département et dîner avec des étudiants de départements différents. On recrute 30 volontaires que l’on assigne aléatoirement aux deux situations, à raison de 15 individus dînant avec des compères et 15 individus dînant avec des étudiants inconnus d’une autre discipline. Le temps que chaque volontaire passe à regarder son téléphone pendant le dîner de 1h30 est noté. À la fin de l’expérience, on obtient les résultats du tableau 4.2. Inconnus Compères Table 4.2: Temps en minutes passé sur son téléphone portable de différents individus dînant avec des compères ou des étudiants inconnus de d’autres disciplines. 27.05083 13.000143 23.524914 20.39442 35.88428 12.49332 14.98468 5.153957 4.428985 13.75534 35.78062 33.20128 26.18627 27.647381 9.389878 30.06083 24.73449 26.80185 11.92027 18.461213 19.881456 34.20787 42.13944 27.49096 16.49150 5.271710 13.398200 23.95501 18.36979 40.77970 Les données se trouvent dans le fichier temps.txt. Puisqu’on s’attend à ce que les individus dînant avec des compères passent plus de temps sur leurs téléphones portables que ceux dînant avec des inconnus (car ils sont désinhibés en présence de leurs compères et moins polis), nous effectuerons un test unilatéral. \\(H_0\\): \\(\\mu_{\\mathrm{inconnus}} \\geq \\mu_{\\mathrm{comp\\grave{e}res}}\\) (test unilatéral) \\(H_a\\): \\(\\mu_{\\mathrm{inconnus}} &lt; \\mu_{\\mathrm{comp\\grave{e}res}}\\) On importe le jeu de données : temps &lt;- read.table(&quot;Module_4/data/temps.txt&quot;, header = TRUE) ##premières observations head(temps) ## Temps Accompagnateurs ## 1 27.05083 inconnus ## 2 14.98468 inconnus ## 3 26.18627 inconnus ## 4 11.92027 inconnus ## 5 16.49150 inconnus ## 6 13.00014 inconnus ##dernières observations tail(temps) ## Temps Accompagnateurs ## 25 18.36979 comperes ## 26 12.49332 comperes ## 27 33.20128 comperes ## 28 26.80184 comperes ## 29 27.49096 comperes ## 30 40.77970 comperes On peut effectuer le test \\(t\\) de Student pour tester notre hypothèse : ##temps pour dîner avec inconnus inconnus &lt;- temps[temps$Accompagnateurs == &quot;inconnus&quot;, &quot;Temps&quot;] ##temps pour dîner avec compères comperes &lt;- temps[temps$Accompagnateurs == &quot;comperes&quot;, &quot;Temps&quot;] ##test unilatéral avec x plus grand que y t.out &lt;- t.test(x = comperes, y= inconnus, data = temps, var.equal = TRUE, alternative = &quot;greater&quot;) t.out ## ## Two Sample t-test ## ## data: comperes and inconnus ## t = 3.8997, df = 28, p-value = 0.0002747 ## alternative hypothesis: true difference in means is greater than 0 ## 95 percent confidence interval: ## 6.887848 Inf ## sample estimates: ## mean of x mean of y ## 28.00328 15.78609 Toutefois, avant de se lancer dans l’interprétation de l’analyse, il faut vérifier les suppositions. ##calculer les résidus - groupe inconnus inconnus.res &lt;- inconnus - mean(inconnus) ##calculer les résidus - groupe comperes comperes.res &lt;- comperes - mean(comperes) ##combiner résidus dans jeu de données temps$Res &lt;- c(inconnus.res, comperes.res) ##vérifier normalité avec test Anderson-Darling ad.test(temps$Res) ## ## Anderson-Darling normality test ## ## data: temps$Res ## A = 0.24703, p-value = 0.732 ##combiner graphique dans une fenêtre par(mfrow = c(1, 2)) ##graphique quantile-quantile qqnorm(temps$Res, ylab = &quot;Quantiles observés&quot;, xlab = &quot;Quantiles théoriques&quot;, main = &quot;Graphique quantile-quantile&quot;) qqline(temps$Res) ##ajouter lettre a text(x = -2, y = 14.5, labels = &quot;a&quot;, cex = 1.2) ##diagramme de boîtes et moustaches - homoscédasticité boxplot(Res ~ Accompagnateurs, data = temps) ##ajouter lettre b text(x = 0.5, y = 14.5, labels = &quot;b&quot;, cex = 1.2) Figure 4.4: Graphique quantile-quantile sur les résidus du test \\(t\\) sur deux groupes indépendants sur les données de temps passés par des étudiants sur leurs téléphones portables (a) et diagramme de boîtes et moustaches pour vérifier la supposition d’homogénéité de la variance à partir des résidus du test \\(t\\) sur deux groupes indépendants avec avec les mêmes données (b). Le test de normalité d’Anderson-Darling suggère que la supposition de normalité est respectée et il en va de même avec le graphique quantile-quantile (Figure 4.4a). La figure 4.4b ne montre pas de signes d’hétérogénéité de la variance. On peut procéder à l’inteprétation des résultats. Puisque \\(P(t_{df = 28}\\) 3.9) = 2.7^{-4}, on rejette \\(H_0\\) et on conclut que le temps passé à regarder son téléphone portable est plus grand lorsque les étudiants dînent avec leurs compères plutôt qu’avec des inconnus. Autrement dit, la valeur de \\(t\\) observée est beaucoup plus extrême que ce qui est attendu selon \\(H_0\\). Finalement, on peut illustrer le résultat dans un graphique de la moyenne de chaque groupe accompagnée de son intervalle de confiance respectif (Figure 4.5). ##moyenne des groupes moy.inconnus &lt;- mean(inconnus) moy.comperes &lt;- mean(comperes) ##SD des groupes sd.inconnus &lt;- sd(inconnus) sd.comperes &lt;- sd(comperes) ##n des groupes n.inconnus &lt;- length(inconnus) n.comperes &lt;- length(comperes) ##SE des moyennes SE.inconnus &lt;- sd.inconnus/sqrt(n.inconnus) SE.comperes &lt;- sd.comperes/sqrt(n.comperes) ##IC&#39;s à 95% IC.inf95.inconnus &lt;- moy.inconnus - qt(p = 0.025, df = n.inconnus - 1) * SE.inconnus IC.sup95.inconnus &lt;- moy.inconnus + qt(p = 0.025, df = n.inconnus - 1) * SE.inconnus IC.inf95.comperes &lt;- moy.comperes - qt(p = 0.025, df = n.comperes - 1) * SE.comperes IC.sup95.comperes &lt;- moy.comperes + qt(p = 0.025, df = n.comperes - 1) * SE.comperes ##graphique plot(y = 0, x = 0, xlab = &quot;Accompagnateurs&quot;, ylab = &quot;Temps passé sur son téléphone (minutes)&quot;, main = &quot;Moyennes ± IC à 95%&quot;, ylim = range(c(IC.inf95.inconnus, IC.inf95.comperes, IC.sup95.inconnus, IC.sup95.comperes)), xlim = c(0, 1), xaxt = &quot;n&quot;) ##ajout de l&#39;axe des x axis(side = 1, at = c(0.3, 0.75), labels = c(&quot;Inconnus&quot;, &quot;Compères&quot;)) ##ajout des points points(y = c(moy.inconnus, moy.comperes), x = c(0.3, 0.75)) # ajout des barres d&#39;erreur arrows(x0 = c(0.3, 0.75), x1 = c(0.3, 0.75), y0 = c(IC.inf95.inconnus, IC.inf95.comperes), y1 = c(IC.sup95.inconnus, IC.sup95.comperes), length = 0.05, code = 3, angle = 90) Figure 4.5: Moyennes du temps passé sur leurs téléphones par des étudiants dînant accompagnés de compères et d’inconnus. Les barres d’erreurs correspondent à des intervalles de confiance à 95%. 4.1.2 Tests d’hypothèse pour deux groupes appariés Dans certains dispositifs d’échantillonnage, on retrouve des données qui viennent en paires. On dira que les données sont appariées, c’est-à-dire que chaque observation d’un groupe est liée avec celle d’un autre groupe. Par exemple, on prend le pouls de jumeaux chez qui on a administré deux médicaments (un jumeau a le traitement 1, le deuxième a le traitement 2); on détermine l’asymétrie chez une espèce de moustique et on mesure la longueur de l’aile gauche et de l’aile droite. L’aile gauche d’un moustique constitue une observation du premier groupe alors que son aile droite est la donnée appariée qui fait partie du second groupe; on mesure la croissance des mêmes arbres avant et après l’ajout d’un fertilisant. La première mesure de croissance d’un arbre constitue donc une donnée du premier groupe alors que la deuxième mesure du même arbre est la donnée appariée qui fait partie du second groupe. Dans chaque exemple ci-dessus, on a des données sous forme de paires. Un dispositif avec des éléments appariés peut être une bonne stratégie pour réduire la variabilité de ce qu’on désire mesurer. Il réduit la variance entre deux observations d’une même paire. Ce dispositif est particulièrement utile lorsque la variabilité entre les individus (paires) est grande. On reconnaît que les paires diffèrent entre elles, mais la variabilité intrapaire (c.-à-d., à l’intérieur d’une même paire) est moindre. On ne peut pas considérer des mesures appariées comme indépendantes les unes des autres et cette structure doit être prise en compte lors de l’analyse. En d’autres mots, il est inapprorié de considérer ces données comme provenant de deux groupes indépendants. Par conséquent, on ne peut pas faire abstraction de cette structure et utiliser le test \\(t\\) sur deux groupes qui sont indépendants. En présence de données appariées, nous utilisons plutôt le test \\(t\\) pour données appariées (paired \\(t\\)-test). Ce test est réalisé sur les différences entre les groupes et il correspond à un test \\(t\\) sur un groupe. Illustrons avec un exemple. Dans une étude d’impact environnementale, on vise à évaluer l’effet des eaux usées sur le nombre d’espèces de poissons dans différents cours d’eau. Pour ce faire, on sélectionne 16 cours d’eau dans lesquels se déversent des eaux usées. On échantillonne les poissons en amont (avant la décharge) et en aval (après la décharge) des sources d’eaux usées. Tous les cours d’eau sont visités à la même période de l’année. Les données se trouvent dans le fichier poissons.txt. poissons &lt;- read.table(&quot;Module_4/data/poissons.txt&quot;, header = TRUE) head(poissons) ## Aval Amont ## 1 20 23 ## 2 15 16 ## 3 10 10 ## 4 5 4 ## 5 20 22 ## 6 15 15 tail(poissons) ## Aval Amont ## 11 10 11 ## 12 5 5 ## 13 20 22 ## 14 15 14 ## 15 10 10 ## 16 5 6 On peut utiliser un test \\(t\\) pour données appariées pour déterminer si le nombre d’espèces en aval est inférieure à celui en amont des décharges d’eaux usées. Dans cet exemple, nous définissons la différence du nombre d’espèces comme étant Aval - Amont. \\(H_0\\): \\(\\mu_{\\mathrm{diff\\acute{e}rence}} \\geq 0\\) (test unilatéral) \\(H_a\\): \\(\\mu_{\\mathrm{diff\\acute{e}rence}} &lt; 0\\) À noter que le choix de calcul de la différence est totalement arbitraire et on aurait pu calculer plutôt . Le cas échéant, il aurait fallu modifier \\(H_0\\) à \\(\\mu_{\\mathrm{diff\\acute{e}rence}} \\leq 0\\) et \\(H_a\\) à \\(\\mu_{\\mathrm{diff\\acute{e}rence}} &gt; 0\\), puisqu’une valeur positive de la différence est conforme à notre prédiction que le nombre d’espèces est plus élevé en amont. Dans tous les cas, \\(H_a\\) doit être conforme à la prédiction associée à notre hypothèse scientifique. Le test \\(t\\) pour données appariées s’effectue sur la différence entre les paires d’observations de chaque groupe : poissons$Diff &lt;- poissons$Aval - poissons$Amont head(poissons) ## Aval Amont Diff ## 1 20 23 -3 ## 2 15 16 -1 ## 3 10 10 0 ## 4 5 4 1 ## 5 20 22 -2 ## 6 15 15 0 mean.diff &lt;- mean(poissons$Diff) mean.diff ## [1] -0.875 SD &lt;- sd(poissons$Diff) SD ## [1] 1.147461 SE &lt;- SD/sqrt(nrow(poissons)) SE ## [1] 0.2868652 t.poissons &lt;- t.test(poissons$Diff, alternative = &quot;less&quot;) On sait que : \\(\\bar{x}_{\\mathrm{diff\\acute{e}rence}} =\\) -0.9 \\(SD_{\\mathrm{diff\\acute{e}rence}} =\\) 1.15 \\(SE_{\\mathrm{diff\\acute{e}rence}} =\\) 0.29 On obtient ainsi : \\[t = \\frac{\\bar{x}_{\\mathrm{diff\\acute{e}rence}} - \\mu_{H_{0}}}{SE_{\\mathrm{diff\\acute{e}rence}}}\\] \\[t = \\frac{-0.9 - 0}{0.29}\\] \\[t = -3.05\\] On peut effectuer rapidement le test dans R à l’aide de t.test( ) : ##test t à un groupe sur la différence t.test(poissons$Diff, alternative = &quot;less&quot;) ## ## One Sample t-test ## ## data: poissons$Diff ## t = -3.0502, df = 15, p-value = 0.00405 ## alternative hypothesis: true mean is less than 0 ## 95 percent confidence interval: ## -Inf -0.3721108 ## sample estimates: ## mean of x ## -0.875 Vous aurez remarqué que le test \\(t\\) pour données appariées n’est nul autre qu’un test \\(t\\) sur un seul groupe (voir Leçon 3). Ce groupe est constitué des différences entre les paires d’observations. Le test \\(t\\) pour données appariées doit respecter les mêmes suppositions que celles du test \\(t\\) à un groupe, puisque le test \\(t\\) pour données appariées est un test \\(t\\) effectué sur un seul groupe. On peut aussi fournir les données brutes de chaque paire à la fonction t.test( ), mais il est très important d’inclure l’argument paired = TRUE. ##même test t, mais utilisant syntaxe de formule ##et l&#39;argument paired = TRUE t.test(poissons$Aval, poissons$Amont, paired = TRUE, alternative = &quot;less&quot;) ## ## Paired t-test ## ## data: poissons$Aval and poissons$Amont ## t = -3.0502, df = 15, p-value = 0.00405 ## alternative hypothesis: true mean difference is less than 0 ## 95 percent confidence interval: ## -Inf -0.3721108 ## sample estimates: ## mean difference ## -0.875 ##calcul des résidus res.diff &lt;- poissons$Diff - mean(poissons$Diff) ##graphique quantile-quantile qqnorm(res.diff, ylab = &quot;Quantiles observés&quot;, xlab = &quot;Quantiles théoriques&quot;, main = &quot;Graphique quantile-quantile&quot;) qqline(res.diff) Figure 4.6: Vérification de la normalité à partir des résidus des données appariées sur le nombre d’espèces de poissons. La vérification de la normalité indique que les résidus ont une distribution normale (Figure 4.6). On conclut qu’il est peu probable d’observer une valeur \\(t\\) de -3.05 ou plus extrême (ici, plus faible étant donné notre hypothèse alternative) avec de liberté lorsque \\(H_0\\) est vraie. Effectivement, \\(P(t_{15} \\leq -3.05) = 0.004\\). On rejette \\(H_0\\) et on déclare dans notre rapport que le nombre espèces de poissons est significativement plus faible en aval qu’en amont d’une décharge d’eaux usées. On peut également formuler la conclusion en déclarant que le nombre d’espèces de poissons est significativement plus élevée en amont qu’en aval d’une décharge d’eaux usées dans notre aire d’étude. Finalement, on peut créer un graphique pour illustrer les résultats. Puisque les deux groupes sont appariés et que le test \\(t\\) pour deux groupes appariés est en fait un test \\(t\\) réalisé sur la différence entre les paires, on peut présenter la moyenne de la différence entre les paires et son intervalle de confiance à 95% (Figure 4.7). ##degrés de liberté résiduels res.df &lt;- nrow(poissons) - 1 ##IC à 95% inf95 &lt;- mean.diff - qt(p = 0.025, df = res.df) * SE sup95 &lt;- mean.diff + qt(p = 0.025, df = res.df) * SE ##graphique plot(x = 0.5, y = mean.diff, ylab = &quot;Moyenne de différence entre les groupes&quot;, xlab = &quot;Analyse réalisée sur le nombre d&#39;espèces (aval - amont)&quot;, main = &quot;Moyenne ± IC à 95%&quot;, ylim = range(c(inf95, sup95)), xaxt = &quot;n&quot;) ##ajout d&#39;IC à 95% segments(x0 = 0.5, x1 = 0.5, y0 = inf95, y1 = sup95) Figure 4.7: Moyenne de la différence du nombre d’espèces de poissons (aval - amont) et intervalle de confiance à 95%. Dans ce cas particulier, on remarque que la figure n’amène pas beaucoup plus d’information que de présenter la moyennne et l’intervalle de confiance directement dans le texte: \\(\\bar{x}_{\\mathrm{Diff}} = -0.88\\), \\(IC\\) à 95%: (-1.49, -0.26). 4.1.3 Transformations Parfois, les suppositions de normalité des résidus ou d’homogénéité de la variance ne peuvent pas être respectées. Dans de tels cas, il s’avère utile de transformer les données brutes. Dans cette section, nous verrons quelques transformations communes utilisées en statistiques. 4.1.3.1 Transformation logarithmique La transformation logarithmique (log transformation) peut permettre d’arriver à la normalité ou d’homogénéiser les variances dans certains cas : \\[ X^\\prime = \\log(X) \\\\ \\mathrm{ou} \\\\ X^\\prime = \\log(X + 1) \\:, \\] où \\(X\\) correspond à la variable originale et \\(X^\\prime\\) représente la nouvelle variable résultant de cette transformation sur chacune des valeurs originales. On ajoute une constante à cette transformation lorsque certaines des valeurs de la variable originale sont 0, puisque le logarithme n’est pas défini pour de telles valeurs. Bien qu’il soit possible d’utiliser n’importe quelle base pour le logarithme, la base \\(e\\) (log népérien, naturel) et la base 10 sont les plus fréquemment utilisées. En présence d’hétéroscédasticité, cette transformation peut rendre les variances homogènes (Figure 4.8). Cette transformation influence à la fois l’homogénéité de la variance et la normalité des résidus. Si les résidus ont déja une distribution normale, mais que les variances ne sont pas homogènes, il se peut que la transformation rétablisse l’homogénéité de la variance mais que la normalité des résidus ne soit plus rencontrée. Figure 4.8: Utilisation de la transformation logarithmique pour homogénéiser les variances de deux groupes. Les données brutes présentent des variances hétérogènes (a) puisque le groupe 1 varie moins que le groupe 2. L’application du log naturel a homogénéisé les variances (b), puisque la hauteur des deux boîtes est très semblable. Dans R, on peut facilement appliquer la transformation logarithmique à l’aide de la fonction log( ) sur une variable existante, comme celle du temps passé sur son téléphone vue plus tôt : ##transformation log sur Temps passé sur téléphone temps$log.Temps &lt;- log(temps$Temps) head(temps) ## Temps Accompagnateurs Res log.Temps ## 1 27.05083 inconnus 11.2647400 3.297718 ## 2 14.98468 inconnus -0.8014168 2.707028 ## 3 26.18627 inconnus 10.4001760 3.265235 ## 4 11.92027 inconnus -3.8658228 2.478240 ## 5 16.49150 inconnus 0.7054111 2.802845 ## 6 13.00014 inconnus -2.7859490 2.564960 4.1.3.2 Transformation racine carrée La transformation racine carrée (square-root transformation) peut s’avérer utile lorsque les données sont sous forme de fréquences. Les fréquences sont constituées de valeurs entières (p. ex., 0, 10, 21) et ne peuvent prendre de valeurs décimales. Par exemple, dans une étude sur le nombre de naissances dans différents hôpitaux, on peut observer 3 naissances, ou encore 2 naissances, 1 naissances ou aucune, mais il est impossible d’observer 2.5 naissances dans un hôpital. De telles données suivent souvent une distribution autre que la distribution normale, comme la distribution de Poisson13. Avec des fréquences, les variances des groupes sont souvent proportionnelles aux moyennes14 et la transformation peut homogénéiser les variances. Il existe différentes versions de la transformation racine carrée, chacune utilisant différentes constantes, notamment : \\[ X^\\prime = \\sqrt{X + 0.5} \\\\ \\mathrm{et} \\\\ X^\\prime = \\sqrt{X + \\frac{3}{8}} \\:. \\] La transformation racine carrée se réalise à l’aide de sqrt( ) : ##on crée une variable de fréquences provenant ##d&#39;une distribution de Poisson avec ##une moyenne de 2.5 freqs &lt;- rpois(n = 60, lambda = 2.5) ##transformation racine carrée sur des fréquences freqs.transf &lt;- sqrt(freqs) ## On représente les deux distributions dans une même figure par(mfrow = c(1, 2)) ## Distribution originale hist(freqs, ylim = c(0, 25)) ## Distribution transformée hist(freqs.transf, ylim = c(0, 25)) Figure 4.9: Distribution de Poisson avec une moyenne de 2.5. Fréquences originales (gauche) et fréquences transformées suivant la transformation racine carrée (droite). 4.1.3.3 Transformation arcsinus Les données sous formes de pourcentages et de proportions suivent fréquemment une distribution binomiale plutôt que normale. Alors que la distribution normale est définie entre l’intervalle \\([-\\infty, +\\infty]\\), les pourcentages et les proportions sont limités dans l’intervalle \\([0, 100]\\) ou \\([0, 1]\\). Il en résulte que les données sous forme de pourcentages et de proportions ont plus de valeurs aux extrémités de l’intervalle (0 – 30 %, 70 – 100 %) qu’une distribution normale. Pour pallier ce problème, la transformation arcsinus (arcsine transformation, angular transformation) peut être utile: \\[ X^\\prime = \\arcsin\\sqrt{X} \\\\ \\mathrm{ou} \\\\ X^\\prime = \\sqrt{(n + \\frac{1}{2})} \\arcsin \\sqrt{\\frac{X + \\frac{3}{8}}{n + \\frac{3}{4}}} \\: . \\] À noter qu’il faut ramener les pourcentages en proportions avant d’utiliser la transformation arcsinus (0.1 et 0.04, au lieu de 10 % et 4 %). Après la transformation, les données sont exprimées en radians (\\(\\frac{180^\\circ}{2\\pi}\\)). Dans R, on exécute la transformation arcsinus comme suit: ##on crée une série de proportions ##à partir d&#39;une distribution bêta props &lt;- rbeta(n = 60, shape1 = 5, shape2 = 1) ##transformation arcsinus sur les proportions props.transf &lt;- asin(sqrt(props)) ## On représente les deux distributions dans une même figure par(mfrow = c(1, 2)) ## Distribution originale hist(props) ## Distribution transformée hist(props.transf) Figure 4.10: Distribution bêta. Proportions originales (gauche) et proportions transformées suivant la transformation arcsinus (droite). 4.1.3.4 Autres transformations La transformation réciproque ou inverse (reciprocal transformation) peut être utilisée lorsque les variances des groupes sont proportionnelles au carré des moyennes : \\[ X^\\prime &amp;=&amp; \\frac{1}{X} \\\\ \\mathrm{ou} \\\\ X^\\prime &amp;=&amp; \\frac{1}{X + 1} \\: . \\] Toutefois, il faut rester vigilant dans l’interprétation des résultats lorsqu’on a appliqué la transformation inverse. Le signe sera inversé. Par exemple, si on obtient après la transformation inverse que la différence du groupe 1 est plus grande que celle du groupe 2, cela implique que pour les données brutes, c’est en effet le contraire qui se produit. La transformation inverse s’obtient de la manière suivante dans R : ##transformation inverse sur le Temps passé sur son téléphone temps$inv.Temps &lt;- 1/(temps$Temps) head(temps) ## Temps Accompagnateurs Res log.Temps inv.Temps ## 1 27.05083 inconnus 11.2647400 3.297718 0.03696744 ## 2 14.98468 inconnus -0.8014168 2.707028 0.06673484 ## 3 26.18627 inconnus 10.4001760 3.265235 0.03818795 ## 4 11.92027 inconnus -3.8658228 2.478240 0.08389072 ## 5 16.49150 inconnus 0.7054111 2.802845 0.06063729 ## 6 13.00014 inconnus -2.7859490 2.564960 0.07692223 La transformation au carré est parfois utile lorsque les variances diminuent alors que les moyennes augmentent, ou lorsque la distribution est allongée vers la gauche: \\[ X^\\prime = X^2 \\end{equation*} \\] On exécute la transformation au carré ainsi: ##transformation au carré sur le Temps passé sur son téléphone temps$carre.Temps &lt;- (temps$Temps)^2 head(temps) ## Temps Accompagnateurs Res log.Temps inv.Temps carre.Temps ## 1 27.05083 inconnus 11.2647400 3.297718 0.03696744 731.7475 ## 2 14.98468 inconnus -0.8014168 2.707028 0.06673484 224.5405 ## 3 26.18627 inconnus 10.4001760 3.265235 0.03818795 685.7207 ## 4 11.92027 inconnus -3.8658228 2.478240 0.08389072 142.0928 ## 5 16.49150 inconnus 0.7054111 2.802845 0.06063729 271.9697 ## 6 13.00014 inconnus -2.7859490 2.564960 0.07692223 169.0037 Il existe d’autres familles de transformation afin de respecter les conditions d’utilisation des tests, mais celles que nous avons présentées plus haut sont les plus fréquemment utilisées. Malgré toutes ces possibilités de transformation des données, certaines variables ne peuvent être amenées à respecter les suppositions. Le cas type est celui où la variable contient une majorité de 0’s. Par exemple, lors d’une étude d’observation, on dénombre les arrestations pour vol à main armée dans 60 villages durant les 10 dernières années, et on observe que la majorité des villages n’ont eu aucun vol. Peu importe le type de transformation que nous utiliserons pour ce type de données, il y aura toujours plus de valeurs dans les queues de la distribution que ce qui est prédit par la distribution normale. Lorsqu’il est impossible de respecter les conditions d’utilisation d’un test d’hypothèse, il est préférable d’éviter de réaliser l’analyse et d’opter pour une analyse alternative. Ceci est particulièrement vrai lorsque la supposition d’homogénéité de la variance n’est pas respectée. 4.1.4 Conclusion Dans ce texte, nous avons présenté des variantes du test \\(t\\) de Student pour comparer les données de deux groupes indépendants ainsi que pour des données appariées. Les suppositions sous-jacentes à ces analyses ont été énoncées, et plusieurs diagnostics formels et informels permettant de vérifier le respect de ces suppositions ont été illustrés. Nous avons également vu une modification du test \\(t\\), celle de Welch, qui est appropriée lorsque les variances des groupes ne sont pas homogènes. Une série de transformations de données ont été présentées afin d’arriver à respecter les suppositions de normalité et d’homogénéité des variances. La distribution de Poisson est une distribution théorique qui définit les événements rares. Cette distribution ne possède qu’un seul paramètre estimé: la moyenne. Cette distribution théorique est utilisée dans certaines analyses, comme la régression de Poisson.↩︎ Rappelons que la variance et la moyenne sont deux paramètres indépendants dans la distribution normale, ce qui n’est pas le cas avec la distribution de Poisson.↩︎ "],["laboratoire.html", "4.2 Laboratoire", " 4.2 Laboratoire 4.2.1 Question 1 a. Lequel des éléments suivants n’est pas associé à une supposition du test \\(t\\) sur deux groupes indépendants? Normalité des résidus. Indépendance des observations. Hétérogénéité de la variance. Homogénéité de la variance. b. Le test \\(t\\) pour données appariées suppose que les variances sont homogènes. Vrai ou faux? c. Le test \\(t\\) de Welch est le plus approprié lorsqu’on ne peut respecter la supposition de normalité des résidus. Vrai ou faux? 4.2.2 Question 2 On effectue une expérience sur l’efficacité d’un médicament qui améliore le temps de récupération après un effort physique. On sélectionne aléatoirement 50 étudiants à la TÉLUQ. On prépare deux séries de comprimés. La première série de comprimés contient uniquement du sucre (placebo), alors que la deuxième série contient le médicament. Pendant l’expérience, chaque individu reçoit deux comprimés, l’un de sucre, et l’autre contenant le médicament. On détermine aléatoirement l’ordre dans lequel chaque individu reçoit les comprimés. On administre ensuite un des deux comprimés à chaque individu et on mesure son temps de récupération en minutes après qu’il a couru sur un tapis roulant pendant 10 minutes à une vitesse de 10 km/h. Le jour suivant, on administre l’autre comprimé en suivant le même protocole. a. Importez le jeu de données recup.txt qui contient les données de cette expérience. On émet l’hypothèse que le temps de récupération avec le placebo est plus long qu’avec le médicament. b. Choisissez le test \\(t\\) approprié pour analyser les données et justifiez votre choix. c. Énoncez les hypothèses nulle et alternative de votre analyse statistique (\\(H_o\\), \\(H_a\\)). d. Vérifiez les suppositions du test que vous avez choisi. e. Effectuez l’analyse du test \\(t\\) choisi avec R. f. Interprétez les résultats. "],["analyse-de-fréquences-et-test-du-khi-carré-chi2.html", "Module 5 Analyse de fréquences et test du khi carré (\\(\\chi^2\\))", " Module 5 Analyse de fréquences et test du khi carré (\\(\\chi^2\\)) Dans la leçon précédente, nous avons illustré des variantes du test \\(t\\) de Student pour comparer les données de deux groupes indépendants ainsi que pour des données appariées. Nous avons présenté les suppositions sous-jacentes à ces analyses ainsi que plusieurs diagnostics formels et informels permettant de vérifier le respect de ces suppositions. Dans tous les cas, les variables réponses étaient numériques avec des valeurs décimales, comme la hauteur, le temps et la masse. Pour ce genre de données, la décimale représente une valeur observable: une hauteur de 193.24 cm a un sens. Toutefois, les fréquences, comme celles provenant du décompte d’événements d’intérêt prennent uniquement des valeurs entières (discrètes). Par exemple, si nous étudions le nombre de personnes atteintes d’une maladie ou le nombre survivant à cette maladie, nous ne pouvons pas observer 12.23 événements. Nous en observerons plutôt 12 ou 13. Ce type de données fera l’objet de cette leçon. "],["théories-4.html", "5.1 Théories", " 5.1 Théories 5.1.1 Analyse de fréquences : le test du khi carré (\\(\\chi^2\\)) Les entiers constituent un type de données qui mène à des analyses particulières. Dans certains cas, on récolte des données de fréquences (ou nombre d’occurrences) dans différentes catégories (p. ex., le sexe) et on désire savoir si la proportion des fréquences diffère selon les catégories. Lorsqu’elles proviennent d’un dispositif complètement aléatoire, les fréquences peuvent être analysées avec le test du khi carré (\\(\\chi^2\\), chi-squared test). Ce genre de test est parfois appelé test d’ajustement ou test d’adéquation (goodness-of-fit test), car il compare les fréquences observées aux fréquences prédites conformes à \\(H_0\\). On s’intéresse aux habitudes d’utilisation de différents modes de transport chez les étudiants de la TÉLUQ. Pour ce faire, on réalise une étude d’observation. On décide a priori d’échantillonner aléatoirement 500 personnes sur la liste des étudiants inscrits à la TÉLUQ. Chaque personne sélectionnée indique le mode de transport qu’elle utilise pour parcourir les plus longues distances dans ses déplacements du lundi au vendredi, parmi les quatre choix suivants: 1) à pied, 2) en transport en commun (autobus, métro, train), 3) à vélo, ou 4) en voiture (véhicule personnel, taxi, autopartage). La variable mode de transport définit les quatre catégories. On utilise souvent le terme variable catégorique pour désigner ce type de variable. Nous dressons un tableau avec les résultats suivants (Table 5.1) : Table 5.1: Habitudes d’utilisation de différents modes de transports dans un échantillon de 500 individus à la TÉLUQ. À pied Transport en commun Vélo Voiture TOTAL 139 146 117 98 500 5.1.1.1 La distribution du khi carré (\\(\\chi^2\\)) Le test du khi carré permet de comparer des fréquences observées par rapport aux fréquences prédites selon une hypothèse nulle que l’on veut tester. Ce test du khi carré porte le nom d’une distribution statistique continue, celle du khi carré (\\(\\chi^2\\)). Cette distribution, tout comme la distribution normale et celle du \\(t\\) de Student, est définie par une fonction de densité. La fonction de densité du khi carré est \\[ f(x \\vert \\nu) = \\frac{\\left(\\frac{1}{2}\\right)^{\\frac{\\nu}{2}}}{\\Gamma \\left(\\frac{\\nu}{2}\\right)} x^{\\left(\\frac{\\nu}{2} - 1\\right)} e^{-\\frac{1}{2}x} \\] Malgré son apparence un peu intimidante, cette fonction de densité comporte un seul paramètre, \\(\\nu\\), qui correspond aux degrés de liberté. Le symbole \\(\\Gamma\\) représente la fonction gamma, qui comporte une intégrale (dans R, elle s’obtient avec gamma( )). La fonction de densité du khi carré est définie pour les valeurs &gt; 0. En mots, cette fonction nous donne la densité de probabilité associée à \\(X = x\\) pour un nombre de degrés de liberté donné. La forme de la distribution du khi carré dépend uniquement des degrés de liberté (Figure 5.1). Dans R, on peut obtenir la densité de probabilité du khi carré associée à une certaine valeur de \\(x\\) à l’aide de dchisq( ). La probabilité cumulative s’obtient à l’aide de pchisq( ) et la fonction quantile à l’aide de qchisq( ). Les valeurs obtenues par le test du khi carré sont comparées à cette distribution afin d’évaluer s’il est fréquent d’observer de telles valeurs avec un nombre de degrés de liberté donné lorsque \\(H_0\\) est vraie. Figure 5.1: Allure de la distribution du khi carré avec différents degrés de liberté. 5.1.1.2 Le test du khi carré (\\(\\chi^2\\)) Le test du khi carré est basé sur une comparaison des fréquences observées par rapport à des fréquences attendues si l’hypothèse nulle est vraie. On calcule facilement la statistique du khi carré à l’aide de : \\[ \\chi^2 = \\sum_{i=1}^k \\frac{(f_i - \\hat{f_i})^2}{\\hat{f_i}} \\] où \\(k\\) correspond au nombre de catégories, \\(f_i\\) donne la fréquence observée dans la catégorie \\(i\\) et \\(\\hat{f_i}\\) donne la fréquence théorique pour la catégorie \\(i\\) si \\(H_0\\) est vraie. On compare la valeur obtenue du khi carré à la distribution théorique du khi carré correspondant à \\(k - 1\\) degrés de liberté (\\(P(\\chi^2_{\\alpha, k - 1} \\geq \\chi^2_{observ\\acute{e}})\\)). Si cette valeur est plus grande que ce que l’on devrait observer lorsque \\(H_0\\) est vraie pour un seuil \\(\\alpha\\) et des degrés de liberté donnés, on rejette \\(H_0\\). Une des propriétés de la statistique du khi carré pour un degré de liberté et un seuil \\(\\alpha\\) donné (\\(\\chi^2_{\\alpha, 1}\\)) est qu’il correspond au carré du \\(z\\) bilatéral15, et au carré du \\(t\\) bilatéral pour un même \\(\\alpha\\) et pour un degré de liberté qui tend vers l’infini: \\[ \\chi^2_{\\alpha, 1} = z^2_{\\alpha(2)} = t^2_{\\alpha(2), \\infty} \\] — Reprenons notre exemple sur l’étude d’observation des personnes inscrites à la TÉLUQ. On veut maintenant déterminer si les modes de transport sont utilisés avec la même fréquence dans un ratio \\(\\mathrm{\\grave{a}\\:pied}:\\mathrm{transport\\:en\\:commun}:\\mathrm{v\\acute{e}lo}:\\mathrm{voiture}\\) de \\(1:1:1:1\\). Nous pouvons énoncer les hypothèses statistiques suivantes: \\(H_0\\): Le ratio \\(\\mathrm{\\grave{a}\\:pied}:\\mathrm{transport\\:en\\:commun}:\\mathrm{v\\acute{e}lo}:\\mathrm{voiture}\\) est de \\(1:1:1:1\\). \\(H_a\\): Le ratio \\(\\mathrm{\\grave{a}\\:pied}:\\mathrm{transport\\:en\\:commun}:\\mathrm{v\\acute{e}lo}:\\mathrm{voiture}\\) n’est pas de \\(1:1:1:1\\). \\(\\alpha = 0.05\\) On peut appliquer le test du khi carré pour tester ces hypothèses : \\[ \\chi^2 = \\sum_{i=1}^k \\frac{(f_i - \\hat{f_i})^2}{\\hat{f_i}} \\\\ \\] On connaît déjà les \\(f_i\\) qui sont les fréquences récoltées dans chaque catégorie (tableau @ref(tab:mode.transport)). Pour ce qui est des fréquences théoriques (\\(\\hat{f_i}\\)), on doit les obtenir selon l’hypothèse nulle que l’on a spécifiée. Dans notre exemple, puisque \\(H_0\\) s’intéresse à un ratio \\(1:1:1:1\\), la fréquence théorique dans chaque catégorie s’obtient en divisant le total des fréquences par le nombre de catégories (c.-à-d., \\(\\hat{f_i} = \\frac{1}{4}(500) = 125\\)). On peut ensuite résoudre : \\[ \\chi^2 = \\frac{(139 - 125)^2}{125} + \\frac{(146 - 125)^2}{125} + \\frac{(117 - 125)^2}{125} + \\frac{(98 - 125)^2}{125} \\] \\[ \\chi^2 = 11.44 \\] Puisque nous avons quatre catégories, le test du khi carré a \\(k - 1 = 4 - 1 = 3\\) degrés de liberté. On observe que la valeur de \\(P(\\chi^2_{\\alpha, k - 1} \\geq 11.44) = 0.0096\\): ##on détermine la probabilité cumulative 1 - pchisq(q = 11.44, df = 3, lower.tail = TRUE) ## [1] 0.009569722 ##même chose avec lower.tail = FALSE pchisq(q = 11.44, df = 3, lower.tail = FALSE) ## [1] 0.009569722 Dans R, on peut exécuter rapidement le test du khi carré avec la fonction chisq.test( ). On doit fournir les fréquences à l’argument x, suivi du type d’hypothèse nulle que l’on veut tester à l’aide de l’argument p. Ici, puisque \\(H_0\\) spécifie qu’on devrait avoir autant de fréquences dans les quatre groupes, on indique p = c(0.25, 0.25, 0.25, 0.25) pour avoir le quart du total des fréquences dans chaque groupe. On peut écrire : ##on crée un vecteur de fréquences ratio &lt;- c(139, 146, 117, 98) ##on peut ajouter les étiquettes (optionnel) names(ratio) &lt;- c(&quot;À Pied&quot;, &quot;Transport en commun&quot;, &quot;Vélo&quot;, &quot;Voiture&quot;) ratio ## À Pied Transport en commun Vélo Voiture ## 139 146 117 98 ##on effectue le khi carré out &lt;- chisq.test(x = ratio, p = c(0.25, 0.25, 0.25, 0.25)) out ## ## Chi-squared test for given probabilities ## ## data: ratio ## X-squared = 11.44, df = 3, p-value = 0.00957 On rejette \\(H_0\\) car \\(P \\leq \\alpha\\) (0.05) et on conclut que le ratio \\(\\mathrm{\\grave{a}\\:pied}:\\mathrm{transport\\:en\\:commun}:\\mathrm{v\\acute{e}lo}:\\mathrm{voiture}\\) n’est pas de \\(1:1:1:1\\) à la TÉLUQ. On constate d’ailleurs que le mode de transport par voiture est sous représenté par rapport aux autres catégories et que les étudiants se déplacent davantage à pied et en transport en commun que ce qui est attendu selon \\(H_0\\). 5.1.1.2.1 Trouver les différences significatives L’analyse réalisée précédemment brosse un portrait général des différences entre les fréquences. Le rejet de \\(H_0\\) ouvre la porte à une étude plus poussée des données. L’identification des différences significatives entre les fréquences observées est possible grâce aux résidus de Pearson. Le résidu de chaque fréquence observée se calcule à l’aide de l’équation suivante : \\[ r_i = \\frac{f_i - \\hat{f_i}}{\\sqrt{\\hat{f_i}}} \\] Les fréquences ayant un résidu inférieur à -2 ou supérieur à 2 sont considérées comme significativement différentes de leur fréquence théorique. Les résidus de Pearson sont calculés par la fonction chisq.test(). On peut les obtenir en ajoutant l’opérateur $ et la composante residuals à l’objet où sont sauvegardés les résultats. ##on extrait les résidus de Pearson des résultats du test out$residuals ## À Pied Transport en commun Vélo Voiture ## 1.2521981 1.8782971 -0.7155418 -2.4149534 On remarque que seul le résidu de l’utilisation de la voiture est significatif. La valeur négative indique que cette fréquence est significativement inférieure à la fréquence théorique. 5.1.1.3 Conditions d’utilisation 5.1.1.3.1 Utilisation des fréquences Certaines conditions sont nécessaires à l’application correcte du test du khi carré. Ce dernier utilise des fréquences (des entiers) comme données brutes. Le test permet de déterminer si les proportions des fréquences observées sont conformes à l’hypothèse nulle. On ne doit jamais utiliser seulement que des proportions (p.ex, 0.3 et 0.7) pour réaliser l’analyse, car la taille de l’échantillon affecte aussi la valeur du khi carré et le résultat du test. 5.1.1.3.2 Indépendance des observations Le test du khi carré suppose que les données sont récoltées selon un dispositif complètement aléatoire et qu’il n’y a pas de structure dans les données telle que la non-indépendance imposée par l’échantillonnage. On peut illustrer la non-indépendance par un bref exemple où on s’intéresse à l’occurrence de vers solitaires dans des paquets de bœuf haché contaminé soumis à 2 temps de cuisson. On pourrait utiliser 2 morceaux du même paquet pour tester les 2 temps de cuisson, cependant ces morceaux ne seraient pas considérés comme indépendants. Pour une parfaite indépendance, il faudrait utiliser un seul morceau de viande pour chaque paquet choisi aléatoirement 5.1.1.3.3 Faibles fréquences théoriques De faibles fréquences théoriques (\\(\\hat{f_i}\\))16 entraînent un biais du \\(\\chi^2\\). En effet, de trop faibles fréquences théoriques surestiment la valeur du \\(\\chi^2\\) et, par conséquent, augmentent l’erreur de type I. Certains statisticiens suggèrent que le test du \\(\\chi^2\\) est approprié uniquement pour les cas où aucune fréquence théorique n’est inférieure à 3 et où pas plus de 20 % des fréquences théoriques sont inférieures à 5. D’autres sont plus stricts et recommandent de n’avoir aucune fréquence théorique inférieure à 5. Afin d’éviter des problèmes, il est préférable de viser une bonne taille d’échantillon, notamment en s’assurant d’avoir au moins 5 fois plus d’observations que de cellules dans le tableau. En d’autres mots, si on a deux cellules comme dans le tableau de l’exemple 5.2 (homme et femme), on veut au moins un total de 10 individus échantillonnés aléatoirement dans la population d’intérêt. Continuons avec un exemple sur la sélection d’habitat qui applique le \\(\\chi^2\\) en écologie animale. On s’intéresse à la sélection d’habitat par des orignaux (Alces alces). Pendant deux années, on suit 117 individus munis de colliers émetteurs et on note leur utilisation de différents habitats dans une aire d’étude. À l’aide d’un système d’information géoréférencé, on calcule la proportion des quatre habitats qui constituent l’aire d’étude. On veut savoir si les orignaux sélectionnent un habitat en particulier par rapport à la disponibilité de chaque habitat dans l’aire d’étude. En d’autres mots, on détermine si certains types d’habitats rares sont plus utilisés que des habitats plus communs, ce qui indiquera une réelle préférence d’habitat. Le tableau 5.2 présente ce jeu de données. Table 5.2: Sélection d’habitat par des Orignaux selon la disponibilité d’habitat dans une aire d’étude. Habitat Disponibilité Fréquence jeune coupe forestière 0.340 25 vieille coupe forestière 0.101 22 marais et étang 0.104 30 forêt non-coupée 0.455 40 L’objectif est de déterminer si certains habitats sont surutilisés ou sous-utilisés par les orignaux. On émet les hypothèses statistiques suivantes : \\(H_0\\): L’utilisation des habitats est proportionnelle à la disponibilité de chacun des habitats (\\(0.340 : 0.101 : 0.104 : 0.455\\)). \\(H_a\\): L’utilisation des habitats n’est pas proportionnelle à la disponibilité de chacun des habitats (certains habitats sont surutilisés ou sous-utilisés). \\(\\alpha = 0.05\\) On obtient les fréquences théoriques (\\(\\hat{f_i}\\)) à l’aide des proportions de disponibilité de chaque habitat : \\[\\hat{f_1} = 0.340 \\cdot 117 = 39.78\\] \\[\\hat{f_2} = 0.101 \\cdot 117 = 11.82\\] \\[\\hat{f_3} = 0.104 \\cdot 117 = 12.17\\] \\[\\hat{f_4} = 0.455 \\cdot 117 = 53.23\\] Ce qui nous permet ensuite de calculer le \\(\\chi^2\\): \\[ \\chi^2 = \\sum_{i=1}^k \\frac{(f_i - \\hat{f_i})^2}{\\hat{f_i}} \\chi^2 = \\frac{(25 - 39.78)^2}{39.78} + \\frac{(22 - 11.82)^2}{11.82} + \\frac{(30 - 12.17)^2}{12.17} + \\frac{(40 - 53.23)^2}{53.23} \\] \\[ \\chi^2 = 43.689 \\] Avec \\(k - 1 = 4 - 1 = 3\\) degrés de liberté, on trouve que \\(P(\\chi^2_{0.05, 3} \\geq 43.689) &lt; 0.0001\\). Avec R, le tout s’obtient facilement à l’aide de : ##on crée un vecteur de fréquences select &lt;- c(25, 22, 30, 40) ##on ajoute les noms des habitats names(select) &lt;- c(&quot;jeune_coupe&quot;, &quot;vieille_coupe&quot;, &quot;marais_et_etang&quot;, &quot;foret_non_coupee&quot;) ##on effectue le test du khi carré ##important de spécifier les bonnes valeurs pour H0 out.select &lt;- chisq.test(select, p = c(0.340, 0.101, 0.104, 0.455)) out.select ## ## Chi-squared test for given probabilities ## ## data: select ## X-squared = 43.689, df = 3, p-value = 1.757e-09 On conclut qu’il est peu probable d’observer une valeur de \\(\\chi^2 \\geq 43.689\\) avec 3 degrés de liberté dans une population où l’utilisation de l’habitat par les orignaux est proportionnelle à la disponibilité de l’habitat. Effectivement, on rejette \\(H_0\\). Certains types d’habitat sont sous-utilisés et d’autres surutilisés par rapport à leur disponibilité dans l’aire d’étude. De façon équivalente, on peut dire que l’utilisation de différents habitats par les Orignaux ne dépend pas seulement des disponibilités de ces habitats. Le rejet de \\(H_0\\) permet une analyse plus poussée à l’aide des résidus de Pearson. ##on extrait les résidus de Pearson des résultats du test out.select$residuals ## jeune_coupe vieille_coupe marais_et_etang foret_non_coupee ## -2.343376 2.962253 5.111995 -1.813950 On constate que les vieilles coupes forestières ainsi que les marais et les étangs sont significativement plus utilisés que ce qui est prédit selon leur disponibilité, alors que les jeunes coupes forestières sont significativement moins fréquentées. Le résultat obtenu pour la forêt non-coupée est plus nuancé. Le résidu associé à cet habitat est près du seuil de signification, soit -2. De plus, la différence entre la fréquence observée et théorique est similaire à ce que l’on observe chez d’autres habitats. C’est ici que les compétences de spécialistes deviennent pertinentes. On pourrait alors conclure que la sous-utilisation de cet habitat est biologiquement significative même si le résidu (-1.81295) est légèrement inférieur au seuil de signification statistique de -2. 5.1.1.3.4 Correction de continuité Les calculs à partir de la formule du \\(\\chi^2\\) approximent bien la distribution théorique du khi carré, sauf dans le cas où il n’y a qu’un seul degré de liberté (\\(df = 1\\)). Dans de tels cas, on peut utiliser la correction de continuité de Yates pour réduire le biais : \\[\\chi^2_{corr} = \\sum_{i=1}^k \\frac{(\\vert f_i - \\hat{f_i}\\vert - 0.5)^2}{\\hat{f_i}}\\] Le prochain exemple illustre justement l’application de la correction de continuité pour un \\(\\chi^2\\) avec 1 degré de liberté. Dans une expérience en génétique, on veut savoir si un échantillon constitué de 100 plantes provient d’une population où le ratio entre le nombre de fleurs jaunes et celui de fleurs vertes est de \\(3 : 1\\). Après avoir récolté 100 plantes, on remarque que 84 plantes ont des fleurs jaunes et 16 plantes ont des fleurs vertes. On émet les hypothèses statistiques suivantes : \\(H_0\\): L’échantillon provient d’une population avec un ratio fleurs \\(\\mathrm{jaunes : fleurs}\\) vertes de \\(3 : 1\\). \\(H_a\\): L’échantillon ne provient pas d’une population avec un ratio fleurs \\(\\mathrm{jaunes : fleurs}\\) vertes de \\(3 : 1\\). \\(\\alpha = 0.05\\) Puisqu’on veut tester un ratio \\(3 : 1\\), les fréquences théoriques conformes à l’hypothèse nulle seront obtenues en multipliant le nombre total de fréquences par \\(0.75(3/4)\\) et \\(0.25(1/4)\\) pour les fleurs jaunes et les fleurs vertes, respectivement. On peut réaliser l’analyse dans R : ##on crée une matrice avec les fréquences freq.fleurs &lt;- matrix(data = c(84, 16), nrow = 1) ##on indique H0 H0.prop &lt;- c(0.75, 0.25) ##on effectue le test du khi carré khi2 &lt;- chisq.test(x = freq.fleurs, p = H0.prop) khi2 ## ## Chi-squared test for given probabilities ## ## data: freq.fleurs ## X-squared = 4.32, df = 1, p-value = 0.03767 On remarque que le test a un degré de liberté. Il faut donc appliquer la correction de continuité de Yates à notre valeur comme suit : ##khi carré original khi.orig &lt;- sum(((khi2$observed - khi2$expected)^2)/ khi2$expected) khi.orig ## [1] 4.32 ##khi carré corrigé pour continuité khi.corr &lt;- sum(((abs(khi2$observed - khi2$expected) - 0.5)^2)/ khi2$expected) khi.corr ## [1] 3.853333 ## P-value du khi carré corrigé P&lt;- pchisq(khi.corr, df = 1, lower.tail = FALSE) P ## [1] 0.04964723 Avec le \\(\\chi^2\\) corrigé pour la continuité (3.853 comparé à 4.32 pour le \\(\\chi^2\\) non corrigé), on rejette tout de même \\(H_0\\) (\\(P = 0.0496\\)) et on conclut que l’échantillon ne provient pas d’une population où il y a un ratio de trois fleurs jaunes pour une fleur verte (\\(3 : 1\\)). Lorsque les valeurs de \\(P\\) se rapprochent du seuil \\(\\alpha = 0.05\\), on peut mentionner que le résultat est marginalement significatif, ce qui souligne au lecteur la moins grande fiabilité du résultat. 5.1.2 Tableau de contingence Le tableau de contingence (contingency table) est un tableau à deux dimensions où on note, à la suite d’une étude d’observation ou d’une expérience, la fréquence d’occurrence d’un événement par rapport à deux variables catégoriques (p. ex., sexe, classes de taille). Le tableau de contingence se prête à un test d’indépendance entre les deux variables catégoriques. Autrement dit, le tableau de contingence permet de tester l’hypothèse que les fréquences d’occurrence d’un événement pour une variable sont indépendantes des fréquences pour les catégories de l’autre variable. Le test du \\(\\chi^2\\) s’applique naturellement au tableau de contingence. On veut savoir si la couleur des yeux est indépendante de la couleur des cheveux chez des humains d’origine septentrionale. On sélectionne aléatoirement 114 individus au Canada et, pour chacun, on note la couleur des yeux et la couleur des cheveux. Les données sont illustrées dans le tableau 5.3. Notons ici qu’il y a deux variables catégoriques: la couleur des cheveux (deux niveaux: pâle, foncé) et la couleur des yeux (deux niveaux: bleu, brun). Nous reprendrons plus tard cet exemple. Table 5.3: Tableau de contingence selon la couleur des cheveux et des yeux. Pâle Foncé Bleu 38 11 Brun 14 51 5.1.2.1 Formulation des hypothèse nulles La formulation des hypothèses nulles à partir d’un tableau de contingence diffère légèrement de ce que nous avons vu pour le test du khi carré d’ajustement. On peut formuler l’hypothèse nulle en termes d’indépendance d’une variable catégorique par rapport à une deuxième variable catégorique. L’hypothèse nulle peut aussi être formulée en termes de proportion d’un niveau d’une variable catégorique par rapport à une deuxième variable catégorique. Les deux types de formulation sont illustrés dans le prochain exemple. On continue l’exemple débuté plus haut. Nous pouvons formuler les hypothèses statistiques pour les données du tableau de contingence de deux manières différentes. Formulation en termes d’indépendance : \\(H_0\\) (indépendance): La couleur des yeux est indépendante de la couleur des cheveux. \\(H_a\\) (non-indépendance): La couleur des yeux n’est pas indépendante de la couleur des cheveux. Formulation en termes de proportions : \\(H_0\\): La proportion des individus avec des yeux bleus ne diffère pas selon la couleur des cheveux. \\(H_a\\): La proportion des individus avec des yeux bleus diffère selon la couleur des cheveux17. 5.1.2.2 Fréquences théoriques L’obtention des fréquences théoriques est un peu plus compliquée pour les tableaux de contingence, mais suit la même logique que pour le \\(\\chi^2\\) d’ajustement qui lui n’a qu’une seule variable catégorique. L’équation suivante nous donne les \\(\\hat{f_{ij}}\\) conformes à l’hypothèse nulle: \\[\\hat{f_{ij}} = \\frac{R_i \\times C_j}{G}\\] On applique le calcul des fréquences théoriques aux données de l’échantillon de couleur des yeux et des cheveux. Le tableau @ref(tab:fi.hat) montre les fréquences observées avec les fréquences théoriques entre parenthèses. Pour illustrer, la fréquence théorique des individus avec des cheveux pâles et des yeux bleus s’obtient: \\[\\hat{f_{ij}} = \\frac{R_i \\times C_j}{G} \\] \\[\\hat{f}_{bleu-p\\hat{a}le} = \\frac{(38 + 11) \\times (38 + 14)}{(38 + 11 + 14 + 51)} \\] \\[\\hat{f}_{bleu-p\\hat{a}le} = \\frac{49 \\times 52}{114} \\] \\[\\hat{f}_{bleu-p\\hat{a}le} = 22.35 \\] Table 5.4: Fréquences observées et théoriques (entre parenthèses) et totaux marginaux selon la couleur des cheveux et des yeux. Pâle Foncé Somme des rangées Bleu 38 (22.35) 11 (26.65) 49 Brun 14 (29.65) 51 (35.35) 65 Somme des colonnes 52 62 114 Avant d’aller plus loin, allons voir les détails du calcul du \\(\\chi^2\\) d’indépendance. 5.1.2.3 Calcul du \\(\\chi^2\\) d’indépendance Le calcul du \\(\\chi^2\\) pour des données d’un tableau de contingence donne la somme des valeurs partielles pour toutes les cellules du tableau : \\[\\chi^2 = \\sum_{i=1}^r \\sum_{j=1}^c \\frac{(f_{ij} - \\hat{f}_{ij})^2}{\\hat{f}_{ij}} \\] où \\(r\\) correspond au nombre total de rangées et \\(c\\) au nombre total de colonnes du tableau de contingence, \\(f_{ij}\\) correspond à la fréquence observée pour la rangée \\(i\\) et la colonne \\(j\\) du tableau de contingence, alors que \\(\\hat{f_{ij}}\\) représente la fréquence théorique de la rangée \\(i\\) et colonne \\(j\\) du tableau de contingence. On calcule des degrés de liberté avec \\(df = (r - 1)(c - 1)\\). En poursuivant l’exemple de la couleur des yeux et des cheveux, on peut calculer le \\(\\chi^2\\) : \\[\\chi^2 = \\sum_{i=1}^r \\sum_{j=1}^c \\frac{(f_{ij} - \\hat{f}_{ij})^2}{\\hat{f}_{ij}} \\] \\[ \\chi^2 = \\frac{(38 - 22.35)^2}{22.35} + \\frac{(14 - 29.65)^2}{29.65} + \\frac{(11 - 26.65)^2}{26.65} + \\frac{(51 - 35.35)^2}{35.35}\\] \\[\\chi^2 = 35.334\\] Avec cette valeur de \\(\\chi^2\\), nous avons \\(df = (r - 1)(c - 1) = (2 - 1)(2 - 1) = 1\\) degré de liberté. Puisque nous avons un seul degré de liberté nous appliquons la correction de Yates pour la continuité : \\[\\chi^2 = \\sum_{i=1}^r \\sum_{j=1}^c \\frac{(\\vert f_{ij} - \\hat{f}_{ij}\\vert - 0.5)^2}{\\hat{f}_{ij}}\\] Ici, nous obtenons 33.112 avec la correction de Yates et une valeur de \\(P(\\chi^2_{0.05, 1} \\geq 33.112) &lt; 0.0001\\). On rejette \\(H_0\\) et on conclut que la couleur des cheveux n’est pas indépendante de la couleur des yeux chez des individus d’origine septentrionale résidant au Canada. À l’aide de R, on procède ainsi : ##on assemble les données dans une matrice freq.obs &lt;- matrix(data = c(38, 11, 14, 51), nrow = 2, ncol = 2, byrow = TRUE) ##on ajoute des étiquettes aux colonnes et rangées ##ceci est facultatif colnames(freq.obs) &lt;- c(&quot;Pale&quot;, &quot;Fonce&quot;) rownames(freq.obs) &lt;- c(&quot;Bleu&quot;, &quot;Brun&quot;) ##analyse sans correction pour continuité out.tab &lt;- chisq.test(x = freq.obs, correct = FALSE) out.tab ## ## Pearson&#39;s Chi-squared test ## ## data: freq.obs ## X-squared = 35.334, df = 1, p-value = 2.778e-09 ##analyse avec correction pour continuité out.tab.corr &lt;- chisq.test(x = freq.obs, correct = TRUE) out.tab.corr ## ## Pearson&#39;s Chi-squared test with Yates&#39; continuity correction ## ## data: freq.obs ## X-squared = 33.112, df = 1, p-value = 8.7e-09 Bien que chisq.test( ) possède un argument correct qui prend la valeur TRUE par défaut, il n’applique la correction de continuité de Yates que sur les tableaux de contingence de 2 x 2. Nous n’aurions pas pu appliquer cette correction directement avec chisq.test( ) sur le test du \\(\\chi^2\\) d’ajustement pour les données de fleurs jaunes et vertes. Puisque le test de khi-carré révèle une dépendance entre la couleur des cheveux et des yeux, il devient pertinent d’identifier les fréquences qui diffèrent significativement entre elles et dans quelle direction ces différences se trouvent. Les résidus de Pearson sont les suivant : ##on extrait les résidus de Pearson des résultats du test out.tab.corr$residuals ## Pale Fonce ## Bleu 3.310112 -3.031437 ## Brun -2.873982 2.632024 L’étude des résidus indique que les fréquences diffèrent significativement puisqu’elles sont inférieures à -2 ou supérieures à 2. On peut conclure que les individus au teint pâle avec des yeux bleus sont plus fréquents que la fréquence attendue (ou théorique). Il en est de même pour les individus dont le teint est foncé avec des yeux bruns. La tendance inverse est observée chez les personnes avec le teint pâle et les yeux bruns et celles au teint foncé avec les yeux bleus. 5.1.3 Alternative au test du \\(\\chi^2\\) Il existe des alternatives au test du khi carré, selon le type de fréquences récoltées. Les prochaines lignes mentionnent quelques tests à titre indicatif. Le test \\(G\\) (G-test) s’applique dans les mêmes conditions que le test du khi carré. Le test exact de Fisher (Fisher’s exact test, fisher.test( ) peut s’avérer utile, lorsqu’on ne peut pas rencontrer les suppositions du test du \\(\\chi^2\\) ou du test \\(G\\), notamment, des fréquences théoriques (\\(\\hat{f}_{ij}\\)) &lt; 5. Le test exact de Fisher est basé sur la distribution hypergéométrique et s’applique surtout sur des tableaux de contingence de 2 x 2. Le test binomial (binomial test) est une autre alternative au test du \\(\\chi^2\\) qui permet de tester si les proportions diffèrent dans un tableau 2 x 2 (prop.test( )) ou si une seule proportion diffère d’une valeur spécifique (binom.test( )). Si les catégories sont ordinales (p. ex., petit, moyen, grand; jeune, mature, vieux), on peut utiliser le test de Kolmogorov-Smirnov modifié pour les données discrètes. Le cas échéant, le test de Kolmogorov-Smirnov sera préférable et plus puissant que le \\(\\chi^2\\) qui ne tient pas compte de l’ordre des catégories. Toutefois, ce dernier n’était pas disponible dans R pour les données discrètes ordinales lors de l’écriture de ce document, c’est-à-dire, ks.test( ) est un test d’ajustement uniquement pour les données continues. Dans d’autres cas, on peut modéliser les fréquences directement à l’aide de modèles log-linéaires (log-linear models) ou des probabilités à l’aide de régressions logistiques (logistic regression) en fonction d’une série de variables catégoriques ou numériques. Ces dernières approches sont des modèles linéaires généralisés que l’on peut réaliser à l’aide de glm( ). 5.1.4 Le paradoxe de Simpson Lorsqu’on analyse des fréquences, il faut être conscient du paradoxe de Simpson (Simpson’s paradox), qui se traduit par une inversion des relations dues à l’effet d’une variable qui n’a pas été considérée dans l’analyse. Voici une illustration de ce paradoxe. Considérons la survie de souris exposées à un traitement (400 souris) comparé à un témoin (120 souris) (Table 5.5). Table 5.5: Effet d’un traitement sur la survie d’individus. Témoin Traitement Vivant 60 200 Mort 60 200 D’après ce tableau, on serait porté à conclure que la survie des souris est indépendante du traitement (\\(60/120\\) vs \\(200/400\\) = 0.5 vs 0.5). Toutefois, en stratifiant l’analyse par sexe, on obtient un résultat différent. Le tableau des mâles suggère que le traitement augmente la survie (Table 5.6, \\(14/60\\) vs \\(60/70\\) = 0.23 vs 0.85). Table 5.6: Effet d’un traitement sur la survie des mâles. Témoin Traitement Vivant 14 60 Mort 46 10 Le tableau des femelles, quant à lui, suggère que le traitement diminue la survie (Table 5.7, \\(46/60\\) vs \\(140/330\\) = 0.77 vs 0.42). Dans cet exemple, les analyses réalisées séparément par sexe mènent à des conclusions différentes. Dans ce cas-ci, faire abstraction du sexe dans l’analyse mène au paradoxe de Simpson et à une conclusion erronée. Il faut songer attentivement avant de se lancer dans l’analyse de fréquences et il est important de considérer toutes les variables pouvant influencer les résultats. Un moyen de prévenir le paradoxe de Simpson est de mieux cibler la population statistique d’intérêt en échantillonnant une partie de la population bien précise (p. ex., âge, type d’habitat). Une autre approche consiste à stratifier l’analyse selon une ou plusieurs variables (p. ex., le sexe, la taille). Table 5.7: Effet d’un traitement sur la survie des femelles. Témoin Traitement Vivant 46 140 Mort 14 190 5.1.5 Conclusion Cette leçon a présenté les rudiments de l’analyse de fréquences, particulièrement le test du \\(\\chi^2\\) d’ajustement et d’indépendance. Les conditions sous-jacentes à cette analyse ont été énoncées, ainsi que quelques diagnostics et mesures préventives lors de l’élaboration de la stratégie d’échantillonnage. Des approches alternatives ont aussi été brièvement présentées lorsque les conditions du test du \\(\\chi^2\\) ne peuvent être respectées. Finalement, le paradoxe de Simpson a été illustré avec un exemple afin de montrer que l’analyse de fréquences sans tenir compte d’une variable additionnelle peut influencer les conclusions. Rappel: le \\(z\\) est l’écart normal de la distribution normale centrée réduite.↩︎ Rappel: les fréquences théoriques sont les fréquences que l’on aurait dû observer si \\(H_0\\) est vraie. En contraste, les fréquences observées sont les données brutes utilisées dans l’analyse.↩︎ Nous verrons avec l’analyse de variance à deux critères (voir la leçon 7) que cette interprétation ressemble beaucoup au concept d’interaction.↩︎ "],["exercices-4.html", "5.2 Exercices", " 5.2 Exercices 5.2.1 Question 1 a. Lequel des éléments suivants s’avère un problème pour le test du khi carré (\\(\\chi^2\\))? Fréquences observées &lt; 3. Fréquences théoriques &lt; 5. Hétérogénéité de la variance. Réponse (b) Fréquences théoriques &lt; 5. La restriction concerne les fréquences théoriques et non observées. L’hétérogénéité de la variance est une supposition de modèles qui dépendent de la distribution normale, tels que le test-\\(t\\) de Student. b. Un tableau de contingence résume les fréquences pour chaque combinaison des niveaux de deux variables catégoriques différentes. Vrai ou faux? Réponse Vrai c. Nommez un test équivalent au \\(\\chi^2\\) qui s’applique dans les mêmes conditions que ce dernier. Réponse Le test-\\(G\\) est un test équivalent au \\(\\chi^2\\). 5.2.2 Question 2 Un concessionnaire automobile désire mieux cibler sa clientèle dans une ville. Il demande une étude d’observation afin de déterminer si les résidants de quatre arrondissements (A, B, C, D) d’une même ville utilisent plutôt des voitures produites par des manufacturiers domestiques (GM, Chrysler, Ford) ou produites par des compagnies étrangères (p.ex., Honda, Toyota, Subaru, Mitsubishi, Mazda, Nissan, Suzuki, Kia, Hyundai, BMW, VolksWagen, Mercedes, Mini, Volvo, SAAB, SMART). On échantillonne aléatoirement 300 voitures dans les quartiers de chacun des quatre arrondissements. On note la marque de chaque voiture. Le tableau 5.8 présente les données. Table 5.8: Tableau de données des fréquences pour chaque type de véhicule (importé ou domestique) dans les quatre arrondissements (A, B, C, D). A B C D Domestique 125 223 62 180 Importé 175 77 238 120 a. Créez un objet approprié pour stocker les données de ce tableau de contingence. Réponse On peut stocker les données dans une matrice: ##matrice autos &lt;- matrix(data = c(125, 223, 62, 180, 175, 77, 238, 120), nrow = 2, ncol = 4, byrow = TRUE) ##on peut ajouter les étiquettes colnames(autos) &lt;- c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;, &quot;D&quot;) rownames(autos) &lt;- c(&quot;Domestique&quot;, &quot;Importe&quot;) ##visualisons le tout autos ## A B C D ## Domestique 125 223 62 180 ## Importe 175 77 238 120 b. Énoncez les hypothèses statistiques pour l’analyse de ces données, ainsi que le seuil de signification. Réponse On peut tester les hypothèses statistiques suivantes, soit formulées en terme d’indépendance ou encore en terme de proportions. Les deux formulations sont équivalentes. Formulation en termes d’indépendance : \\(H_0\\) (indépendance): L’origine des véhicules est indépendante de l’arrondissement de la ville. \\(H_a\\) (non-indépendance): L’origine des véhicules n’est pas indépendante de l’arrondissement de la ville. \\(\\alpha = 0.05\\) OU Formulation en termes de proportions : \\(H_0\\): La proportion des véhicules d’origine domestique ne diffère pas selon l’arrondissement de la ville. \\(H_a\\): La proportion des véhicules d’origine domestique diffère selon l’arrondissement de la ville. \\(\\alpha = 0.05\\) c. Choisissez l’analyse appropriée et justifiez votre choix. Réponse On peut analyser les données du tableau de contingence avec un test du \\(\\chi^2\\) puisque les données ont été obtenues à l’aide d’un dispositif complètement aléatoire et parce que nous avons 5 fois plus d’observations qu’il y a de cellules dans le tableau (300 fois plus d’observations). Par conséquent, aucune fréquence théorique n’est inférieure à 5. d. À l’aide de R, effectuez l’analyse que vous avez choisie précédemment et interprétez les résultats. Réponse Si on a choisi le \\(\\chi^2\\), on procédera comme suit : out.autos &lt;- chisq.test(x=autos) On observe un \\(\\chi^2\\) de 194.36 avec trois degrés de liberté et \\(P(\\chi^2_{0.05, 3} &gt; 194.36) &lt; 0.0001\\). On rejette \\(H_0\\) et on conclut que l’origine des véhicules n’est pas indépendante de l’arrondissement. L’interprétation alternative, en termes de proportion, indique que la proportion de véhicules d’origine domestique varie avec le quartier. En d’autres mots, la compagnie peut viser certains quartiers afin d’optimiser sa campagne de publicité. e. Est-il approprié d’identifier les différences significatives entre les fréquences observées ? Dans l’affirmative, veuillez effectuer l’analyse prévue à cet effet. Que pouvez-vous conclure ? Réponse Il est pertinent d’identifier les différences entre les fréquences observées lorsqu’on rejette l’hypothèse nulle. Les résidus de Pearson sont les suivants : ##on extrait les résidus de Pearson des résultats du test out.autos$residuals ## A B C D ## Domestique -1.852621 6.216572 -7.039959 2.676008 ## Importe 1.821997 -6.113811 6.923588 -2.631773 L’étude de ces résidus indique qu’un concessionnaire de voitures fabriquées par des compagnies étrangères aura tout intérêt à cibler l’arrondissement C, puisque la proportion de leurs véhicules y est significativement plus élevée. Si, à l’inverse, la concession représente des manufacturiers domestiques, les arrondissements B et D devraient être considérés. Des valeurs de résidus près des limites de signification pour l’arrondissement A mènent à s’interroger sur la pertinence de s’intéresser à cette partie du territoire. La commande ci-dessous permet de connaître les différences entre les fréquences observées et théoriques. On remarque que ces différences sont les plus faibles. Il n’y a donc pas lieu de s’intéresser formellement à cet arrondissement pour accroitre les ventes de voitures importées. out.autos$observed-out.autos$expected ## A B C D ## Domestique -22.5 75.5 -85.5 32.5 ## Importe 22.5 -75.5 85.5 -32.5 "],["analyse-de-variance-à-un-critère.html", "Module 6 Analyse de variance à un critère", " Module 6 Analyse de variance à un critère Dans la leçon précédente, nous avons vu l’analyse de fréquences, particulièrement le test du \\(\\chi^2\\) d’ajustement et d’indépendance. Nous continuons ici avec l’analyse de données numériques continues, du même type que pour le test \\(t\\). Dans la présente leçon, nous verrons un outil pour comparer plus de deux groupes, l’analyse de la variance. "],["théories-5.html", "6.1 Théories", " 6.1 Théories 6.1.1 Comparaison de plusieurs groupes Nous avons vu que le test \\(t\\) permet de comparer deux groupes afin de déterminer s’ils proviennent de la même population. Dans certains cas, on s’intéresse à plus de deux groupes. Par exemple, on pourrait vouloir comparer la distance quotidienne parcourue par les chauffeurs de trois compagnies de transport différentes. Rappelons qu’à chaque fois que nous réalisons un test d’hypothèse statistique, nous avons une probabilité \\(\\alpha\\) de commettre une erreur de type I (c.-à-d., déclarer une différence lorsqu’il n’y en a pas, un faux positif). Avec trois groupes (A, B, C), il existe trois comparaisons possibles: \\[ A \\: vs \\: B \\qquad A \\: vs \\: C \\qquad B \\: vs \\: C\\] À noter que la comparaison \\(A \\: vs \\: B\\) est identique à \\(B \\: vs \\: A\\). La probabilité de trouver une différence entre les trois moyennes est supérieure à \\(\\alpha\\). Plus on fait de comparaisons avec un test \\(t\\) (e.g., \\(A \\: vs \\: B\\), \\(A \\: vs \\: C\\)), plus on risque de conclure à tort qu’il y a une différence entre les moyennes des groupes (c.-à-d. l’erreur de type I augmente avec le nombre de comparaisons). Si les tests sont indépendants, on peut calculer la probabilité de commettre au moins une erreur de type I à l’aide de l’équation: \\[P = 1 - (1 - \\alpha)^k\\] où \\(\\alpha\\) est le seuil de signification pour chaque comparaison et \\(k\\) est le nombre de comparaisons effectuées. Pour un seuil fixé à \\(\\alpha = 0.05\\) : avec 3 comparaisons, la probabilité de commettre une erreur de type I est de 0.14; avec 10 comparaisons, la probabilité de commettre une erreur de type I est de 0.40; avec 20 comparaisons, la probabilité de commettre une erreur de type I est de 0.64. Toutefois, si les comparaisons ne sont pas indépendantes, la probabilité réelle de commettre une erreur de type I est inférieure ou égale à celle obtenue avec l’équation ci-haut. On comprend que l’augmentation du nombre de comparaisons risque d’entraîner des conclusions erronées. Une alternative à ce problème consiste à corriger le seuil \\(\\alpha\\) par le nombre de comparaisons que l’on veut effectuer. C’est ce qu’on appelle l’ajustement de Bonferroni (Bonferroni adjustment). Dans une expérience où on étudie 5 traitements différents, nous avons cinq moyennes arithmétiques (5 groupes: A, B, C, D, E) et les comparaisons suivantes : \\[A \\: vs \\: B \\] \\[A \\: vs \\: C \\qquad B \\: vs \\: C \\] \\[A \\: vs \\: D \\qquad B \\: vs \\: D \\qquad C \\: vs \\: D \\] \\[A \\: vs \\: E \\qquad B \\: vs \\: E \\qquad C \\: vs \\: E \\qquad D \\: vs \\: E \\: .\\] Puisqu’il y a 10 comparaisons, on devrait diviser le seuil \\(\\alpha\\) par 10 pour exécuter l’ajustement de Bonferroni. Si on utilise un seuil de 0.05, il faudra observer \\(P \\leq 0.005\\) pour déclarer une différence significative entre deux groupes (\\(\\alpha/10 = 0.05/10 = 0.005\\)). On constate que l’ajustement de Bonferroni est une stratégie souvent trop conservatrice, qui augmente la probabilité de commettre une erreur de type II (un faux négatif). Une meilleure stratégie consiste à comparer tous les groupes simultanément. C’est d’ailleurs ce que permet de faire l’analyse de variance – on compare tous les groupes en une seule étape. 6.1.2 Analyse de variance (ANOVA) L’analyse de variance (analysis of variance, ANOVA) est une approche statistique qui compare les moyennes de plusieurs groupes entre eux pour déterminer si au moins un des groupes diffère des autres. Le scénario typique de l’ANOVA est celui d’une expérience visant à comparer l’effet d’une variable catégorique sur une variable réponse d’intérêt. Une variable catégorique est une variable à partir de laquelle la variable réponse peut être classée dans une catégorie particulière. Par variable réponse, on entend une variable dépendante que l’on mesure et qui peut être influencée par la variable catégorique. On utilise aussi le terme facteur pour désigner une variable catégorique. En fait, les termes “variable catégorique”, “facteur” et “critère” sont souvent interchangeables dans la littérature. Nous utiliserons le terme “facteur” pour les besoins du cours. Des exemples de variables catégoriques incluent les classes d’âge (jeune, mature, vieux), les traitements pharmaceutiques (médicament 1, médicament 2, placebo), et la concentration d’engrais (très faible, modérée, élevée, très élevée). Dans le contexte de l’ANOVA, la variable réponse est une variable numérique continue qui varie selon les niveaux du facteur. Comme nous le verrons plus loin, l’ANOVA est l’extension du test \\(t\\) pour des cas avec des variables catégoriques qui ont plus de 2 niveaux. Étant donné que l’on s’intéresse aux moyennes, on pourrait se demander pourquoi appeler cette approche ANOVA. Lorsqu’on réalise une ANOVA, on détermine si la variabilité des données expliquée par le traitement est plus grande que la variabilité inexpliquée. Pour ce faire, nous calculons la somme des carrés (sum of squares). Le concept de la somme des carrés a été présenté lors de la leçon 1 Statistiques descriptives. 6.1.2.1 Décomposition de la variance 6.1.2.1.1 Sommes des carrés Pendant l’ANOVA, on décortique la variance totale des données en différentes composantes: une partie expliquée et une autre inexpliquée. Puisque les sommes des carrés sont additives, on peut écrire : \\[SST = SSA + SSE \\: \\] où \\(SST\\) correspond à la somme des carrés totale, \\(SSA\\) correspond à la somme des carrés du facteur \\(A\\) (aussi appelée somme des carrés du traitement) et \\(SSE\\) est la somme des carrés des erreurs. On peut diviser les sommes des carrés par les degrés de liberté appropriés pour obtenir le carré moyen (la variance) attribuable aux effets qui nous intéressent. Nous présentons dans les prochaines lignes le calcul de chacune des sommes des carrés discutées précédemment. On calcule la somme des carrés totale (\\(SST\\)): \\[ SST = \\sum_{i=1}^n (y_i - \\bar{y})^2\\] où \\(y_i\\) est la \\(i^{\\mathrm{i\\grave{e}me}}\\) observation et \\(\\bar{y}\\) représente la moyenne globale de toutes les observations (tous les groupes confondus). Cette somme des carrés compare chaque observation à la moyenne globale. Les degrés de liberté associés à \\(SST\\) sont obtenus avec \\(df = N - 1\\), où \\(N\\) correspond au nombre total d’observations. On calcule la somme des carrés des erreurs (\\(SSE\\)): \\[ SSE = \\sum_{j=1}^k \\sum_{i=1}^n (y_{ij} - \\bar{y_j})^2\\] où \\(y_{ij}\\) correspond à la \\(i^{\\mathrm{i\\grave{e}me}}\\) observation du groupe \\(j\\) et \\(\\bar{y_j}\\) est la moyenne du groupe \\(j\\). Cette somme des carrés compare chaque observation à la moyenne de son groupe respectif. On obtient les degrés de liberté avec \\(df = k(n - 1)\\), où \\(k\\) correspond au nombre de groupes et \\(n\\) correspond au nombre d’observations dans chacun des groupes. On calcule la somme des carrés du facteur \\(A\\) (le traitement qui définit les différents groupes) (\\(SSA\\)): \\[SSA = \\sum_{j=1}^k \\sum_{i=1}^n (\\bar{y}_{j} - \\bar{y})^2 = n \\sum_{j=1}^k (\\bar{y}_{j} - \\bar{y})^2\\] où \\(\\bar{y}_{j}\\) correspond à la moyenne du groupe \\(j\\), \\(\\bar{y}\\) est la moyenne globale et \\(n\\) correspond au nombre d’observations dans chacun des groupes. On peut aussi la calculer par soustraction à l’aide de \\(SSA = SST - SSE\\). La somme des carrés du traitement compare la moyenne de chaque groupe à la moyenne globale. Les degrés de liberté s’obtiennent avec \\(df = k - 1\\) où \\(k\\) est le nombre de groupes dans le facteur \\(A\\). On veut déterminer si la concentration d’ozone en parties par million (ppm) diffère entre trois jardins d’une ville industrielle. Le jeu de données jardins.txt contient les observations suivantes : ##importation du jeu de données ozone &lt;- read.table(&quot;Module_6/data/jardins.txt&quot;, header = TRUE) ozone ## Ozone Jardin ## 1 14.82 A ## 2 20.59 A ## 3 6.18 A ## 4 7.61 A ## 5 31.35 A ## 6 11.62 A ## 7 32.70 A ## 8 26.18 A ## 9 19.91 A ## 10 10.96 A ## 11 12.60 A ## 12 17.09 A ## 13 5.97 A ## 14 17.95 A ## 15 9.60 A ## 16 20.46 B ## 17 18.26 B ## 18 28.63 B ## 19 14.81 B ## 20 14.22 B ## 21 13.97 B ## 22 20.19 B ## 23 16.21 B ## 24 23.63 B ## 25 21.02 B ## 26 20.41 B ## 27 18.59 B ## 28 13.19 B ## 29 18.28 B ## 30 11.15 B ## 31 10.03 C ## 32 11.58 C ## 33 26.68 C ## 34 5.61 C ## 35 12.21 C ## 36 24.87 C ## 37 18.60 C ## 38 34.78 C ## 39 14.63 C ## 40 17.09 C ## 41 5.05 C ## 42 20.53 C ## 43 28.67 C ## 44 12.19 C ## 45 28.65 C On peut visualiser les données à l’aide d’un diagramme de boîtes et moustaches (Figure 6.1). ##diagramme de boîtes et moustaches boxplot(Ozone ~ Jardin, data = ozone, ylab = &quot;Concentration d&#39;ozone (ppm)&quot;, xlab = &quot;Jardin&quot;, cex.lab = 1.2) Figure 6.1: Diagramme de boîtes et moustaches de la concentration d’ozone dans les trois jardins. La somme des carrés des erreurs s’obtient ainsi : \\[ SSE = \\hspace{1em} \\sum_{j=1}^k \\sum_{i=1}^n (y_{ij} - \\bar{y_j})^2 \\hspace{2em} \\bar{y_A} = 16.34 \\hspace{2em} \\bar{y_B} = 18.20 \\hspace{2em} \\bar{y_C} = 18.08 \\] \\[ SSE = \\hspace{1em}\\sum_{i=1}^n((y_{iA} - \\bar{y_A})^2 + (y_{iB} - \\bar{y_B})^2 + (y_{iC} - \\bar{y_C})^2) \\] \\[ SSE = (14.82 - 16.34)^2 + \\ldots + (9.60 - 16.34)^2 + \\] \\[\\hspace{1em} (20.46 - 18.20)^2 + \\ldots + (11.15 - 18.20)^2 + \\] \\[\\hspace{1em} (10.03 - 18.08)^2 + \\ldots + (28.65 - 18.08)^2 \\] \\[ SSE = 2451.5 \\] On calcule la somme des carrés des traitements avec l’équation complète ou par soustraction: \\[ SSA = \\sum_{j=1}^k \\sum_{i=1}^n (\\bar{y}_{j} - \\bar{y})^2 = n \\sum_{j=1}^k (\\bar{y}_{j} - \\bar{y})^2 \\hspace{4em} n_A = n_B = n_C = 15 \\] \\[SSA = 15 \\cdot ((16.34 - 17.54)^2 + (18.20 - 17.54)^2 + (18.08 - 17.54)^2) \\] \\[SSA = 32.43\\] On effectue ces calculs dans R : ##SST SST &lt;- sum((ozone$Ozone - mean(ozone$Ozone))^2) SST ## [1] 2483.881 ##degrés de liberté de SST df.tot &lt;- nrow(ozone) - 1 df.tot ## [1] 44 ##SSE ##sous-jeu de données ozoneA &lt;- ozone[ozone$Jardin == &quot;A&quot;, ] ozoneB &lt;- ozone[ozone$Jardin == &quot;B&quot;, ] ozoneC &lt;- ozone[ozone$Jardin == &quot;C&quot;, ] ##SSE de A SSE.A &lt;- sum((ozoneA$Ozone-mean(ozoneA$Ozone))^2) ##SSE de B SSE.B &lt;- sum((ozoneB$Ozone-mean(ozoneB$Ozone))^2) ##SSE de C SSE.C &lt;- sum((ozoneC$Ozone-mean(ozoneC$Ozone))^2) SSE &lt;- SSE.A + SSE.B + SSE.C SSE ## [1] 2451.451 ##degrés de liberté de SSE df.erreur &lt;- 3 * (15 - 1) df.erreur ## [1] 42 ##SSA SSA &lt;- SST - SSE SSA ## [1] 32.43014 ##autre façon de la calculer SSA.alt &lt;- (15 * (mean(ozoneA$Ozone) - mean(ozone$Ozone))^2) + (15 * (mean(ozoneB$Ozone) - mean(ozone$Ozone))^2) + (15 * (mean(ozoneC$Ozone) - mean(ozone$Ozone))^2) SSA.alt ## [1] 32.43014 ##degrés de liberté du traitement df.trait &lt;- 3 - 1 df.trait ## [1] 2 6.1.2.1.2 Tableau de l’ANOVA Après avoir calculé les sommes des carrés des différents termes ainsi que leurs degrés de liberté respectifs, on peut les assembler dans un tableau d’ANOVA (Table 6.1). Ce tableau constitue une partie importante des résultats de l’analyse et doit figurer dans les rapports ou au moins y être résumé par écrit. Table 6.1: Calcul des différents éléments du tableau d’ANOVA. Source Somme des carrés (\\(SS\\)) Degrés de liberté (\\(df\\)) Carré moyen (\\(CM\\)) ratio \\(F\\) Traitement \\(SSA\\) \\(k-1\\) \\(SSA/df_A\\) \\(MSA/MSE\\) Erreur \\(SSE\\) \\(k(n-1)\\) \\(SSE/df_erreur\\) Total \\(SST\\) \\(N-1\\) Le carré moyen du traitement (\\(SSA/df_A\\)) estime la variance due au traitement, alors que le carré moyen des erreurs (\\(SSE/df_{erreur}\\)) estime la variance inexpliquée aussi appelée variance résiduelle (residual variance). La variance résiduelle est la meilleure estimation de la variance (\\(\\sigma^2\\)) commune à tous les \\(k\\) groupes comparés. C’est une des raisons pour lesquelles la méthode est appelée “analyse de variance” : on estime en même temps la variance commune à tous les groupes. Le ratio \\(F\\) consiste à comparer la variabilité expliquée et la variabilité inexpliquée. Si le ratio est nettement supérieur à 1, c’est parce que la variance expliquée par le traitement excède de beaucoup la variance résiduelle: il y a probablement un effet du traitement sur la variable réponse. Un ratio inférieur ou s’approchant de 1 suggère qu’il n’y a pas d’effet du traitement. L’interprétation du ratio \\(F\\) dépend aussi de la taille de l’échantillon. Ce ratio de variances est comparé à une distribution théorique, celle du \\(F\\). 6.1.2.2 La distribution du \\(F\\) La distribution du \\(F\\) est une distribution continue qui est définie par la fonction de densité : \\[ f(x \\vert \\nu_1, \\nu_2) = \\frac{\\sqrt{\\frac{(\\nu_1 x)^{\\nu_1}\\nu_{2}^{\\nu_2}}{(\\nu_1 x + \\nu_2)^{\\nu_1 + \\nu_2}}}}{x \\mathrm{B} \\left (\\frac{\\nu_1}{2}, \\frac{\\nu_2}{2} \\right )} \\: \\] où \\(\\nu_1\\) et \\(\\nu_2\\) sont les degrés de liberté et \\(B\\) est la fonction beta qui implique des intégrales (voir ?beta dans R). En ce qui concerne l’application de cette distribution à l’ANOVA, les degrés de liberté \\(\\nu_1\\) et \\(\\nu_2\\) sont ceux associés au numérateur et au dénominateur du ratio \\(F\\), respectivement, c.-à-d., \\(df_A\\) et \\(df_{erreur}\\) tels que présentés dans le Tableau 6.1. La Figure 6.2 présente la forme de la distribution du \\(F\\) selon différents degrés de liberté. Comme d’habitude, nous comparons la statistique obtenue à partir des données de l’échantillon à celle de la distribution du \\(F\\) selon les degrés de liberté et le seuil de signification spécifiés lorsque l’hypothèse nulle est vraie. Si la valeur du \\(F\\) observée est plus grande que celle déterminée par la distribution du \\(F\\), nous rejetterons l’hypothèse nulle. Figure 6.2: Distribution du \\(F\\) selon différentes valeurs de degrés de liberté. 6.1.2.3 Hypothèses statistiques Comme nous l’avons vu dans les leçons précédentes, l’hypothèse nulle (bilatérale) avec le test \\(t\\) de Student sur deux groupes indépendants implique l’égalité des moyennes des deux groupes : \\(H_0\\): \\(\\mu_{\\mathrm{A}} = \\mu_{\\mathrm{B}}\\) (non-différence) \\(H_a\\): \\(\\mu_{\\mathrm{A}} \\neq \\mu_{\\mathrm{B}}\\) (différence) En ce qui a trait à l’ANOVA, l’hypothèse nulle a la même forme. Par exemple, pour un facteur qui compte quatre niveaux (p. ex., faible, modéré, élevé, très élevé), nous aurions l’hypothèse nulle suivante : \\(H_0\\): \\(\\mu_{\\mathrm{A}} = \\mu_{\\mathrm{B}}\\) = \\(\\mu_{\\mathrm{C}} = \\mu_{\\mathrm{D}}\\) (non-différence) \\(H_a\\): au moins une moyenne diffère des autres moyennes Reprenons notre exemple sur la concentration d’ozone dans les trois jardins d’une ville industrielle. Nous pouvons émettre les hypothèses statistiques suivantes : \\(H_0\\): \\(\\mu_{\\mathrm{jardin \\: A}} = \\mu_{\\mathrm{jardin \\: B}}\\) = \\(\\mu_{\\mathrm{jardin \\: C}}\\) (non-différence) \\(H_a\\): au moins une moyenne diffère des autres moyennes \\(\\alpha = 0.05\\) Puisque nous avons déja calculé les sommes des carrés dans l’exemple 6.2, nous pouvons construire le tableau d’ANOVA qui nous permettra de tester l’hypothèse nulle (Table 6.2). Table 6.2: Calcul des différents éléments du tableau d’ANOVA. Source Somme des carrés (\\(SS\\)) Degrés de liberté (\\(df\\)) Carré moyen (\\(CM\\)) ratio \\(F\\) Jardin 32.43 2 16.22 0.28 Residuals 2451.45 42 58.37 On constate que le ratio \\(F\\) est de 0.28 (\\(16.22/58.37\\)). Étant donné que \\(P(F_{2, 42} \\geq 0.28) = 0.7588\\)18, on ne rejette pas \\(H_0\\) et on conclut que les moyennes des groupes ne diffèrent pas les unes des autres. La fonction aov( ) permet de réaliser une ANOVA dans R lorsque chaque groupe contient le même nombre d’observations19. On peut constater que la variable Jardin est une variable caractère (chr). Celle-ci doit être un facteur afin de réaliser l’ANOVA. La transformation est réalisée à l’aide de la commande suivante : ozone$Jardin &lt;- as.factor(ozone$Jardin) La commande ci-dessous permet de valider que la variable Jardin est maintenant un facteur (factor). str(ozone) ## &#39;data.frame&#39;: 45 obs. of 2 variables: ## $ Ozone : num 14.82 20.59 6.18 7.61 31.35 ... ## $ Jardin: Factor w/ 3 levels &quot;A&quot;,&quot;B&quot;,&quot;C&quot;: 1 1 1 1 1 1 1 1 1 1 ... On peut maintenant réaliser l’ANOVA et l’extraire à l’aide de la fonction summary( ). Cette dernière fonction est souvent utilisée pour obtenir un résumé des résultats d’une analyse statistique. ##exécuter l&#39;ANOVA aov1 &lt;- aov(Ozone ~ Jardin, data = ozone) ##on extrait les résultats summary(aov1) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Jardin 2 32.4 16.22 0.278 0.759 ## Residuals 42 2451.5 58.37 6.1.2.4 Suppositions L’analyse de variance est une extension du test \\(t\\) de Student lorsqu’on souhaite comparer les moyennes entre plus de deux groupes. Il n’est donc pas surprenant que l’analyse de variance doive répondre aux mêmes conditions d’utilisation que le test \\(t\\). Pour effectuer l’ANOVA, les erreurs doivent être indépendantes. L’échantillonnage aléatoire et l’utilisation d’un bon dispositif expérimental aident à respecter cette condition. L’ANOVA requiert également que les variances des groupes soient égales (homoscédasticité): \\(\\sigma_1^2 = \\sigma_2^2 = \\sigma_3^2 \\ldots \\sigma_k^2\\). Finalement, on suppose que chacun des groupes est issu d’une population normale (c.-à-d., résidus ou erreurs sont distribués normalement). Autrement dit, afin de pouvoir comparer des groupes dont la moyenne peut potentiellement différer d’un groupe à l’autre, il faut que ces groupes aient la même variance. Cette variance commune est estimée par le carré moyen de l’erreur (residual mean square, error mean square). 6.1.2.4.1 Résidus À l’instar du test \\(t\\), nous utilisons les résidus pour diagnostiquer des problèmes liés à la normalité et à l’homogénéité des variances. Les résidus sont obtenus en calculant la différence entre les valeurs observées \\(y_i\\) et les valeurs qui sont prédites \\(\\hat{y}_i\\) par notre modèle d’ANOVA. Dans le cas de l’ANOVA à un critère, les valeurs prédites (\\(\\hat{y}_i\\)) correspondent à la moyenne arithmétique de chaque groupe. Nous obtenons les résidus à l’aide de \\(y_i - \\hat{y}_i\\). Les fonctions residuals( ) et fitted( ) dans R permettent d’extraire les résidus et les valeurs prédites de plusieurs types d’analyses statistiques. Le prochain exemple illustre l’application directe de ces fonctions à l’objet qui contient le résultat de l’ANOVA. On peut vérifier les suppositions de l’ANOVA que nous avons exécutée dans l’exemple précédent. On obtient les résidus en calculant la différence entre les valeurs observées et les valeurs prédites, \\(y_i - \\hat{y}_i\\). On commence par extraire les valeurs observées et les valeurs prédites : ##valeurs observées obs &lt;- ozone$Ozone obs[1:10] ## [1] 14.82 20.59 6.18 7.61 31.35 11.62 32.70 26.18 19.91 10.96 ##extraire les valeurs prédites pred &lt;- fitted(aov1) pred[1:10] ## 1 2 3 4 5 6 7 8 9 10 ## 16.342 16.342 16.342 16.342 16.342 16.342 16.342 16.342 16.342 16.342 ##moyenne arithmétique de chaque groupe tapply(X = ozone$Ozone, INDEX = ozone$Jardin, FUN = mean) ## A B C ## 16.34200 18.20133 18.07800 On remarque que les valeurs prédites par l’ANOVA à un critère correspondent à la moyenne arithmétique de chaque groupe, comme l’indique par la fonction tapply( )20 applique une fonction donnée (ici, mean) aux valeurs d’une variable X (ozone\\$Ozone) regroupées selon les différentes catégories de INDEX (ozone\\$Jardin).]. Les résidus s’obtiennent facilement à l’aide du calcul \\(y_i - \\hat{y}_i\\) ou avec la fonction residuals( ) : ##résidus obs[1:10] - pred[1:10] ## 1 2 3 4 5 6 7 8 9 10 ## -1.522 4.248 -10.162 -8.732 15.008 -4.722 16.358 9.838 3.568 -5.382 ##extraire résidus du modèle res &lt;- residuals(aov1) res[1:10] #valeurs identiques au calcul obs - pred ## 1 2 3 4 5 6 7 8 9 10 ## -1.522 4.248 -10.162 -8.732 15.008 -4.722 16.358 9.838 3.568 -5.382 6.1.2.4.2 Normalité des résidus et homogénéité de la variance De la même façon qu’on utilise le graphique quantile-quantile lorsqu’on effectue un test \\(t\\), on utilise ce même graphique pour diagnostiquer des déviations par rapport à la supposition de normalité. La fonction qqnorm( ) permet d’obtenir ce graphique et la fonction qqline( ) ajoute une droite théorique représentant une distribution normale. Pour évaluer l’homogénéité des variances des différents groupes, on peut utiliser le diagramme de boîtes et moustaches (boxplot( )) des résidus en fonction de chaque groupe. Un autre graphique utile pour diagnostiquer des problèmes de variances hétérogènes est le graphique des résidus en fonction des valeurs prédites. Ce dernier devrait montrer un patron nul, c’est-à-dire des points distribués uniformément de part et d’autre de 0 sur l’axe des \\(y\\) sans patron apparent (Figure 6.3a). L’hétérogénéité des variances se traduit parfois par l’apparition d’un patron en forme d’entonnoir, indiquant que les variances augmentent avec les valeurs prédites (Figure 6.3b). S’il y a des doutes par rapport au respect de ces conditions, plusieurs options sont possibles. Les transformations vues dans la leçon 4 de ce cours peuvent être utiles, ou encore des tests plus complexes qui ne nécessitent pas ces conditions. Figure 6.3: Graphique de résidus en fonction des valeurs prédites illustrant l’homogénéité des variances (a) et l’hétérogénéité des variances (b). Notez le patron en forme d’entonnoir en b qui est indiqué par une variance qui augmente avec les valeurs prédites. Nous poursuivons l’exemple 6.4 en vérifiant si la condition de normalité est respectée. Pour vérifier la normalité, on extrait les résidus de l’ANOVA et on examine le graphique quantile-quantile. On constate que la condition de normalité est assez bien respectée, quoiqu’il y ait quelques valeurs aux extrémités des queues qui dévient de la normalité (Figure 6.4a). Le graphique des résidus en fonction des valeurs prédites suggère que les variances sont plutôt homogènes, bien que le groupe avec la plus grande moyenne ait une variabilité légèrement plus faible que les autres (Figure 6.4b). par(mfrow = c(1, 2)) ##graphique quantile-quantile qqnorm(res, ylab = &quot;Quantiles observés&quot;, xlab = &quot;Quantiles théoriques&quot;, main = &quot;Graphique quantile-quantile&quot;, cex.lab = 1.2) ##cex.lab indique que les étiquettes seront 1.2 fois plus grande ##ajout de droite théorique qqline(res) text(y = 15, x = -2, labels = &quot;a&quot;) ##résidus vs valeurs prédites plot(res ~ pred, ylab = &quot;Résidus&quot;, xlab = &quot;Valeurs prédites&quot;, main = &quot;Résidus vs valeurs prédites&quot;, cex.lab = 1.2) text(y = 15, x = 16.5, labels = &quot;b&quot;) Figure 6.4: Graphique quantile-quantile des résidus de l’ANOVA sur les concentrations d’ozone (a) et résidus en fonction des valeurs prédites (b). Nous avons maintenant toutes les notions en main afin de réaliser l’ANOVA et de vérifier le respect des conditions. C’est ce que nous ferons dans le prochain exemple. On fait une étude de santé publique sur la consommation de boissons sucrées par les jeunes de 19 à 25 ans dans cinq régions de l’Est du Québec. Pour ce faire, on fait un sondage détaillé auprès de six jeunes par région et on estime pour chacun sa consommation annuelle en litres. Les cinq régions sont: Chaudière-Appalaches (); Côte-Nord (); Saguenay Lac-Saint-Jean (). Bas-Saint-Laurent (); Gaspésie-Iles–de-la-Madeleine (); Les données sont incluses dans le jeu de données consommation.txt. cons &lt;- read.table(&quot;Module_6/data/consommation.txt&quot;, header = TRUE) head(cons) ## Volume Region ## 1 55.1 CA ## 2 45.7 CA ## 3 45.0 CA ## 4 73.1 CA ## 5 49.9 CA ## 6 63.2 CA Nous avons les hypothèses statistiques suivantes : \\(H_0\\): \\(\\mu_{\\mathrm{SLSJ}} = \\mu_{\\mathrm{GIM}} = \\mu_{\\mathrm{CA}} = \\mu_{\\mathrm{CN}} = \\mu_{\\mathrm{BSL}}\\) (non-différence) \\(H_a\\): au moins une moyenne diffère parmi toutes les moyennes \\(\\alpha = 0.05\\) Avant de faire l’analyse, on peut visualiser rapidement les données et la variabilité de chaque groupe avec boxplot( ) (Figure 6.5). ##boxplot de données brutes boxplot(Volume ~ Region, data = cons, ylab = &quot;Volume&quot;, xlab = &quot;Régions&quot;) Figure 6.5: Diagramme de boîtes et moustaches de la consommation annuelle de volume de boissées sucrées par jeune dans 5 régions de l’Est du Québec. On exécute l’ANOVA à un critère : ## Conversion en facteur de la variable Region cons$Region &lt;- as.factor(cons$Region) ##ANOVA m1 &lt;- aov(Volume ~ Region, data = cons) Avant de se lancer dans l’interprétation, il faut vérifier les suppositions de l’ANOVA, notamment celles concernant l’homoscédasticité et la normalité des résidus. On constate que la supposition d’homogénéité des variances est assez bien respectée et une seule observation se démarque des autres avec une valeur &gt; 15 (Figure 6.6a}). Toutefois, il est important de noter que six observations par groupe est une taille d’échantillon faible pour vérifier cette supposition d’homoscédasticité. Le graphique quantile-quantile avec les résidus indique que la supposition de normalité est respectée pour ce jeu de données (Figure 6.6b). ##on organise la fenêtre graphique ##pour avoir 1 rangée et deux colonnes ##de graphiques par(mfrow = c(1, 2)) ##homoscédasticité plot(residuals(m1) ~ fitted(m1), ylab = &quot;Résidus&quot;, xlab = &quot;Valeurs prédites&quot;, main = &quot;Résidus vs valeurs prédites&quot;, cex.lab = 1.2) ##normalité qqnorm(residuals(m1), ylab = &quot;Quantiles observés&quot;, xlab = &quot;Quantiles théoriques&quot;, main = &quot;Graphique quantile-quantile&quot;, cex.lab = 1.2) qqline(residuals(m1)) Figure 6.6: Diagnostics de l’ANOVA. On peut procéder à l’interprétation des résultats. ##on extrait les résultats summary(m1) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Region 4 853.6 213.39 4.302 0.00875 ** ## Residuals 25 1240.2 49.61 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 On note dans summary(m1) que la dernière ligne du tableau Signif. codes donne les symboles qui décrivent le seuil de signification statistique du facteur Region. Ces caractères ne sont affichés qu’en présence de termes significatifs dans l’analyse. Ici, Region a les caractères \\(\\mathtt{\\ast\\ast}\\) sur sa ligne, indiquant que \\(P \\leq 0.01\\). Ces symboles n’ont d’autre fonction que d’attirer l’attention de l’analyste sur les termes significatifs dans l’analyse. Il est peu probable d’observer une valeur de \\(F\\) de 4.3 dans une expérience avec des échantillons (groupes) qui ont la même taille que le jeu de données original tirés d’une population où \\(H_0\\) est vraie. La valeur exacte du \\(P\\) est de 0.0088 (\\(P(F_{4, 25} \\geq 4.3015) = 0.0088\\)). On rejette l’hypothèse nulle et on conclut qu’il y a un effet de la région : au moins une moyenne d’une région diffère d’une autre. Toutefois, l’ANOVA ne nous permet pas d’identifier quelles régions diffèrent entre elles. Pour ce faire, on doit utiliser un autre type de test qui sera présenté dans la prochaine section. 6.1.3 Comparaison multiples L’ANOVA permet de déterminer si la variance expliquée par le facteur à l’étude est supérieure à la variance résiduelle (c.-à-d., la variance inexpliquée). Ainsi, le tableau d’ANOVA nous permet de rejeter ou non \\(H_0\\). Lorsqu’on rejette l’hypothèse nulle après avoir effectué un test \\(t\\) pour comparer deux groupes indépendants, on sait qu’il existe une différence entre ces deux groupes. Dans une ANOVA qui a plus de deux groupes, le rejet de \\(H_0\\) ne nous révèle pas où se trouvent les différences21. Les comparaisons multiples permettent d’identifier les différences entre les moyennes des groupes. Comme nous l’avons mentionné au début de ce texte, effectuer des comparaisons multiples augmente la probabilité de commettre une erreur de type I. L’ANOVA est conçue pour comparer tous les groupes simultanément afin d’évaluer l’effet global du traitement. Lorsque l’ANOVA entraîne le rejet de l’hypothèse nulle, il s’avère nécessaire d’exécuter des comparaisons multiples entre les groupes afin de trouver quelles sont les différences. Ces comparaisons effectuées après l’ANOVA augmentent aussi la probabilité de commettre une erreur de type I – la probabilité de commettre ce type d’erreur est supérieure au seuil \\(\\alpha\\) que nous avons fixé. Puisque les comparaisons multiples sont effectués après l’ANOVA, on parle souvent de tests a posteriori ou post hoc (post hoc tests, a posteriori tests). 6.1.3.1 Erreurs associées aux comparaisons multiples On peut commettre des erreurs à deux niveaux lorsqu’on fait des comparaisons multiples : une erreur au niveau de la comparaison (comparison-wise error) et une erreur au niveau de l’expérience (experiment-wise error). L’erreur au niveau de la comparaison se traduit par la déclaration d’un faux positif lors d’une comparaison spécifique entre deux groupes. La probabilité de commettre cette erreur pour une comparaison donnée est déterminée par \\(\\alpha\\). L’erreur au niveau de l’expérience, quant à elle, correspond à déclarer au moins un faux positif parmi toutes les comparaisons effectuées. Plus on a de groupes (et de comparaisons), plus notre erreur au niveau de l’expérience augmente. Le prochain exemple illustre les différences entre les deux niveaux d’erreur. Pour faciliter la compréhension de l’erreur liée à la comparaison entre les groupes et l’erreur liée à l’expérimentation, nous vous proposons l’exercice de simulation suivant. Nous simulons des données où l’hypothèse nulle est vraie, c’est-à-dire qu’il n’y a aucune différence entre les moyennes des groupes. On crée trois groupes de \\(n = 50\\) tirés d’une population suivant une distribution normale avec \\(\\mu = 5\\) et \\(\\sigma = 3\\) (c.-à-d., N(5, 3)). En d’autres mots, nous simulons les données de trois groupes et ces données sont toutes tirées de la même population pour satisfaire \\(H_0\\). Il y a trois comparaisons possibles: groupe 1 vs groupe 3, groupe 2 vs groupe 3 et groupe 1 vs groupe 2. Nous voulons savoir quelle est la probabilité de commettre une erreur liée à la comparaison et une erreur liée à l’expérience. On peut décrire l’algorithme de la simulation comme suit : on génère des données aléatoires pour chacun des groupes conformément à \\(H_0\\) (rnorm(n = 50, mean = 5, sd = 3)); on réalise les trois comparaisons (1 vs 3, 2 vs 3, 1 vs 2) à l’aide de trois tests \\(t\\) (t.test( )); on détermine pour chaque test \\(t\\) effectué, si \\(P \\leq \\alpha\\) (p. ex., \\(P \\leq 0.05\\)); on répète les étapes 1 à 3 un grand nombre de fois (p. ex., 1000 fois). On peut réaliser ce genre d’exercice avec des boucles dans R. Le document Les boucles avec R couvre les concepts de base et donne des exemples pour construire des boucles qui permettront de réaliser des tâches répétitives. À la fin de la simulation, nous pouvons constater le nombre de fois où nous avons rejeté \\(H_0\\) par erreur. Ici, nous savons que \\(H_0\\) est vraie puisque nous avons simulé des données qui respectent l’hypothèse nulle (aucune différence entre les moyennes des groupes). À chaque fois que nous avons rejeté \\(H_0\\) dans cette simulation, nous avons commis une erreur. Le tableau 6.3 montre le résultat des 10 premières itérations de la simulation (les étapes 1 à 3 ci-dessus). Les trois premières colonnes correspondent aux trois comparaisons possibles. La valeur NS correspond à un résultat non significatif (\\(H_0\\) non rejetée). Le symbole * indique le rejet de \\(H_0\\) et, par conséquent, une erreur liée à la comparaison. La quatrième colonne correspond à l’erreur liée à l’expérience. Cette colonne prend la valeur de 1 lorsqu’au moins une comparaison a rejeté \\(H_0\\) (une erreur liée à l’expérience) et la valeur de 0 lorsqu’aucune comparaison ne rejette \\(H_0\\). Table 6.3: Premières itérations de la simulation illustrant les erreurs liée à la comparaison et à l’expérience. 1 vs 2 1 vs 3 2 vs 3 Erreur liée à l’expérience NS NS NS 0 NS NS NS 0 NS NS NS 0 NS NS NS 0 NS NS NS 0 NS NS NS 0 NS NS NS 0 NS 1 NS NS NS 0 NS NS 1 La première rangée du tableau 6.3 correspond à la première itération, et on constate qu’aucune erreur de comparaison n’a été commise, et, par conséquent, aucune erreur liée à l’expérience. La huitième rangée du même tableau montre que les comparaisons 1 vs 3 et 2 vs 3 ont rejeté par erreur \\(H_0\\) (erreurs liées à la comparaison) et qu’il y a donc une erreur au niveau de l’expérience. On remarquera qu’à l’intérieur de chaque colonne, la proportion du nombre d’erreurs de comparaison par rapport au nombre total d’itérations oscille autour de \\(\\alpha = 0.05\\), puisque c’est le seuil que nous avons utilisé dans les comparaisons multiples. Toutefois, en compilant le nombre total d’erreurs au niveau de l’expérience (la quatrième colonne), on obtient la probabilité de commettre une erreur au niveau de l’expérience. Ici, nous avons : \\[\\frac{124 \\: \\mathrm{erreurs \\: li\\acute{e}es \\: \\grave{a} \\: l^\\prime exp\\acute{e}rience}}{1000 \\: \\mathrm{it\\acute{e}rations}} = 0.124\\] Bien que nous ayons fixé le seuil \\(\\alpha\\) à 0.05, la probabilité de commettre une erreur au niveau de l’expérience est supérieure à ce seuil (0.124). Ce problème a motivé le développement de plusieurs types de comparaisons multiples afin de contrôler l’erreur au niveau de l’expérience. 6.1.3.2 Suppositions Les tests de comparaisons multiples ont les mêmes suppositions de normalité et d’homoscédasticité que l’ANOVA. Les comparaisons multiples sont moins robustes face aux déviations de ces conditions, surtout à celles d’homoscédasticité, qui augmentent les erreurs de type I et II. 6.1.3.3 Structure générale Un grand nombre de tests de comparaisons multiples ont été développés pour différentes applications. En général, les tests de comparaisons multiples suivent le même principe: on effectue l’ANOVA; si on rejette \\(H_0\\), on peut faire des comparaisons multiples. Si on ne rejette pas \\(H_0\\), l’analyse est terminée. Ce choix de ne pas poursuivre avec des comparaisons multiples s’explique du fait que l’ANOVA est plus puissante que les comparaisons multiples; on classe les moyennes des groupes par ordre croissant; on calcule la différence entre la plus grande moyenne vs la plus petite; comme pour un test \\(t\\) (voir la leçon 4), on divise les différences par une erreur-type pour une statistique \\(q\\), c-à-d un quantile de la distribution associée à l’hypothèse nulle: \\(q = (\\bar{x}_{grande} - \\bar{x}_{petite})/SE\\) . Le calcul du quantile \\(q\\) implique donc le carré moyen des erreurs, \\(MSE\\) (\\(MSE\\) étant une fonction de \\(SE\\)), mais dépend du test et de sa distribution; on compare le \\(q_{obs}\\) à un \\(q_{th\\acute{e}orique}\\), qui dépend du seuil \\(\\alpha\\), des \\(df\\) du \\(MSE\\) et du nombre de groupes comparés. Dans la plupart des cas, \\(\\alpha\\) représente l’erreur au niveau de l’expérience; un \\(q_{obs} \\geq q_{th\\acute{e}orique}\\) indique qu’il y a une différence entre la paire de moyennes comparées. Habituellement, on commence en comparant la plus grande moyenne \\(\\bar{x}_{k}\\) des \\(k\\) groupes (ordonnés en ordre croissant de 1 à \\(k\\)) aux autres (vs \\(\\bar{x}_{1}\\), vs \\(\\bar{x}_{2}\\), , vs \\(\\bar{x}_{k - 1}\\)). Si, par exemple, on ne rejette plus \\(H_0\\) à partir de la comparaison de \\(\\bar{x}_{k}\\) vs \\(\\bar{x}_{2}\\), il n’est pas nécessaire de vérifier les comparaisons pour les moyennes de 3 à \\(k-1\\) parce que l’on sait déjà qu’on ne rejettera pas \\(H_0\\). On passe ensuite à la comparaison de \\(\\bar{x}_{k - 1}\\) avec les moyennes de 1 à \\(k-2\\). Encore une fois, nous pouvons arrêter les comparaisons avec les moyennes subséquentes, dès qu’on ne rejettera plus \\(H_0\\) pour l’une des moyennes. On procède ainsi pour toutes les moyennes pour, enfin, terminer en comparant \\(\\bar{x}_{2}\\) vs \\(\\bar{x}_{1}\\). Par exemple, considérons les moyennes de cinq groupes organisées par ordre croissant: \\[ A \\qquad C \\qquad B \\qquad E \\qquad D \\] \\[ 3.1 \\qquad 9.3 \\qquad 10.5 \\qquad 10.9 \\qquad 11.8 \\] Si on compare la plus grande moyenne (groupe D) au groupe C et que l’on ne rejette pas \\(H_0\\), on ne testera ni le groupe D vs le groupe B, ni le groupe D vs le groupe E. De plus, on ne testera pas le groupe E vs le groupe C, car les groupes D et C ne diffèrent pas l’un de l’autre, et la valeur de E est comprise entre celles de C et D. 6.1.3.4 Test de Tukey Parmi les tests de comparaisons multiples, mentionnons le test de Dunnett (lorsqu’on veut comparer tous les groupes à un témoin), le test de Student-Newman-Keuls (SNK, Newman-Keuls) qui dépend du nombre de moyennes séparant les moyennes comparées, le test de Scheffé, et le test de Tukey (Tukey test, Tukey’s Honestly Significant Difference (HSD) test). Le test de Tukey est d’ailleurs l’un des tests de comparaisons multiples les plus recommandés. La statistique \\(q\\) s’obtient comme suit : \\[ q_{obs} = \\frac{\\bar{x}_{groupe \\: 1} - \\bar{x}_{groupe \\: 2}}{SE}\\] où \\(\\bar{x}_{groupe \\: 1}\\) correspond à la moyenne du groupe 1, \\(\\bar{x}_{groupe \\: 2}\\) à la moyenne du groupe 2 et \\(SE\\) est l’erreur-type du test de Tukey. Cette dernière est donnée par \\(SE = \\sqrt{\\frac{MSE}{n}}\\), où \\(MSE\\) est le carré moyen des erreurs et \\(n\\) est le nombre d’observations dans chaque groupe. Nous appliquons le test de Tukey dans le prochain exemple. Dans l’exemple 6.6, nous avons rejeté l’hypothèse nulle à l’aide de l’ANOVA réalisée sur des données de consommation annuelle de boissons sucrées par des jeunes de 5 régions différentes. On peut appliquer le test de Tukey pour trouver où se trouvent les différences entre les moyennes des groupes. On peut commencer par calculer les moyennes des groupes et les ordonner : ##moyennes des groupes moy &lt;- tapply(X = cons$Volume, INDEX = cons$Region, FUN = mean) ##en ordre croissant et en arrondissant à deux décimales round(sort(moy), digits = 2) ## BSL CA CN SLSJ GIM ## 46.52 55.33 56.93 61.05 61.07 Nous pouvons calculer l’erreur-type du dénominateur nécessaire au calcul de la statistique, \\(SE = \\sqrt{\\frac{MSE}{n}}\\) : ##sous-jeu de données SLSJ &lt;- cons[cons$Region == &quot;SLSJ&quot;, ] GIM &lt;- cons[cons$Region == &quot;GIM&quot;, ] CA &lt;- cons[cons$Region == &quot;CA&quot;, ] CN &lt;- cons[cons$Region == &quot;CN&quot;, ] BSL &lt;- cons[cons$Region == &quot;BSL&quot;, ] ##SSE de SLSJ SSE.SLSJ &lt;- sum((SLSJ$Volume-mean(SLSJ$Volume))^2) ##SSE de GIM SSE.GIM &lt;- sum((GIM$Volume-mean(GIM$Volume))^2) ##SSE de CA SSE.CA &lt;- sum((CA$Volume-mean(CA$Volume))^2) ##SSE de CN SSE.CN &lt;- sum((CN$Volume-mean(CN$Volume))^2) ##SSE de BSL SSE.BSL &lt;- sum((BSL$Volume-mean(BSL$Volume))^2) ##SSE SSE &lt;- SSE.SLSJ + SSE.GIM + SSE.CA + SSE.CN + SSE.BSL SSE ## [1] 1240.203 ##degrés de liberté de SSE df.erreur &lt;- 5 * (6 - 1) df.erreur ## [1] 25 ##carré moyen des erreurs MSE &lt;- SSE/df.erreur MSE ## [1] 49.60813 ##SE pour le Tukey SE &lt;- sqrt(MSE/6) SE ## [1] 2.875417 Par la suite, on calcule le \\(q\\) pour chaque comparaison multiple (tableau 6.4). Chaque valeur de \\(q\\) est comparée à ce qu’on devrait obtenir si \\(H_0\\) est vraie et la fonction ptukey( ) nous fournit la probabilité cumulative en faveur de \\(H_0\\) pour nmeans groupes et df degrés de liberté du terme d’erreur de l’ANOVA. À titre d’exemple, la probabilité de la comparison GIM vs BSL se calcule à l’aide de la commande suivante : ptukey(q=5.06, nmeans=5, df=25, lower.tail = FALSE) ## [1] 0.01152797 Où \\(q\\) est \\(q_{obs} = (\\bar{x}_{GIM} - \\bar{x}_{BSL})/{SE}\\) Comme d’habitude, on rejette \\(H_0\\) lorsqu’elle est peu probable (c-à-d, \\(P \\leq \\alpha\\)). Table 6.4: Comparaisons multiples de Tukey entre les groupe définis par les régions. Comparaison Difference SE q_obs P Conclusion GIM vs BSL 14.55 2.88 5.06 0.012 rejeter \\(H_0\\) GIM vs CA 5.73 2.88 1.994 0.627 ne pas rejeter \\(H_0\\) GIM vs CN 4.13 2.88 on ne teste pas GIM vs SLJS 0.02 2.88 on ne teste pas SLSJ vs BSL 14.53 2.88 5.054 0.012 rejeter \\(H_0\\) SLSJ vs CA 5.72 2.88 on ne teste pas SLSJ vs CN 4.12 2.88 on ne teste pas CN vs BSL 10.42 2.88 3.623 0.109 ne pas rejeter \\(H_0\\) CN vs CA 1.6 2.88 on ne teste pas CA vs BSL 8.82 2.88 on ne teste pas On peut créer ce tableau rapidement à l’aide de la fonction TukeyHSD( ) : ##test de Tukey TukeyHSD(m1, which = &quot;Region&quot;) ## Tukey multiple comparisons of means ## 95% family-wise confidence level ## ## Fit: aov(formula = Volume ~ Region, data = cons) ## ## $Region ## diff lwr upr p adj ## CA-BSL 8.81666667 -3.125984 20.75932 0.2243248 ## CN-BSL 10.41666667 -1.525984 22.35932 0.1088202 ## GIM-BSL 14.55000000 2.607350 26.49265 0.0115253 ## SLSJ-BSL 14.53333333 2.590683 26.47598 0.0116387 ## CN-CA 1.60000000 -10.342650 13.54265 0.9946026 ## GIM-CA 5.73333333 -6.209317 17.67598 0.6272414 ## SLSJ-CA 5.71666667 -6.225984 17.65932 0.6297485 ## GIM-CN 4.13333333 -7.809317 16.07598 0.8453941 ## SLSJ-CN 4.11666667 -7.825984 16.05932 0.8472695 ## SLSJ-GIM -0.01666667 -11.959317 11.92598 1.0000000 D’après la sortie R, on observe seulement deux différences significatives (\\(\\alpha= 0.05\\)) : la consommation moyenne pour la région BSL est plus petite que celles des régions SLSJ et GIM. En conséquence, on ne peut pas conclure que les moyennes pour les régions CA et CN diffèrent avec celles des autres régions, ni que la moyenne pour la région SLSJ diffère de celle de la région GIM. 6.1.3.5 Présentation des résultats Il est possible de présenter les résultats des comparaisons multiples de différentes manières. L’une d’elles consiste à ordonner les groupes selon la valeur des moyennes et d’unir à l’aide d’un trait les groupes qui ne diffèrent pas entre eux. On peut aussi présenter les résultats graphiquement, typiquement en présentant les moyennes ± des barres d’erreur. Comme barre d’erreur, on peut utiliser l’erreur-type des moyennes de chaque groupe, la racine-carrée du carré moyen des erreurs (\\(\\sqrt{MSE}\\)) ou des intervalles de confiance construits à partir de l’une ou l’autres des mesures de dispersion mentionnées. On emploie des lettres pour distinguer les groupes qui diffèrent entre eux sur le graphique. Le prochain exemple illustre ces deux méthodes avec les données de consommation annuelle de boissons sucrées par des jeunes selon différentes régions de l’Est du Québec. À noter que les comparaisons multiples indiquent parfois qu’un traitement appartient à deux groupes. Cette double appartenance indique que la comparaison multiple n’a pas permis de détecter une différence entre ces deux groupes. Les tests de comparaisons multiples ne peuvent parfois trouver de groupes qui diffèrent entre eux, alors que l’ANOVA avait rejeté \\(H_0\\). L’ANOVA est plus puissante que les tests de comparaisons multiples, ce qui explique, à l’occasion, l’absence d’une différence entre les paires de groupes. Puisque l’ANOVA est plus puissante que les comparaisons multiples, nous recommandons de ne pas procéder à des comparaisons multiples si l’ANOVA n’a pas réussi à rejeter \\(H_0\\). On peut représenter succinctement le résultat des comparaisons multiples sur les données de consommation de boissons sucrées selon la région : BSL CA CN SLSJ GIM 46.52 55.33 56.93 61.05 61.07 La présentation des résultats à l’aide de traits facilite l’interprétation. On voit rapidement que CA et CN ont une double appartenance: ils appartiennent au groupe constitué de BSL, CA et CN ainsi qu’au groupe composé de CA, CN, SLSJ et GIM. Les groupes SLSJ et GIM ne diffèrent pas entre eux, mais sont différents du groupe BSL. En effet, le même trait relie les groupes SLSJ et GIM et ce même trait n’inclut pas le groupe BSL. Le graphique montre la même information (présence de deux groupes), mais à l’aide de lettres (Figure 6.7). On remarque que les groupes CA et CN ont une double appartenance puisqu’ils sont identifiés avec deux lettres. On trouve le code complet pour obtenir le graphique ci-dessous : ##ordonner moyennes moys &lt;- sort(moy) ##calculer racine carrée de MSE sqrt.MSE &lt;- sqrt(MSE) ##calculer les limites des barres d&#39;erreur lim.sup &lt;- moys + sqrt.MSE lim.inf &lt;- moys - sqrt.MSE ##créer graphique vide sans axe des x&#39;s plot(x = 0, y = 0, type = &quot;n&quot;, ylim = c(min(lim.inf), max(lim.sup+10)), xlim = c(0, 6), xlab = &quot;Région&quot;, ylab = &quot;Consommation annuelle (L)&quot;, main = &quot;Moyennes ± racine-carrée de MSE&quot;, xaxt = &quot;n&quot;, cex.lab = 1.2) ##ajouter axe des x&#39;s axis(side = 1, at = c(1, 2, 3, 4, 5), labels = names(moys)) ##ajouter moyennes points(x = c(1, 2, 3, 4, 5), y = moys) ##ajouter barres d&#39;erreurs arrows(x0 = c(1, 2, 3, 4, 5), y0 = lim.inf, x1 = c(1, 2, 3, 4, 5), y1 = lim.sup, length = 0.05, angle = 90, code = 3) ##ajouter les lettres, lim.sup + 10 text(x = 1, y = 54.5, labels = &quot;a&quot;) text(x = 2, y = 63.3, labels = &quot;ab&quot;) text(x = 3, y = 64.9, labels = &quot;ab&quot;) text(x = 4, y = 69.0, labels = &quot;b&quot;) text(x = 5, y = 69.1, labels = &quot;b&quot;) Figure 6.7: Présentation graphique des résultats des comparaisons multiples avec le test de Tukey. 6.1.4 Conclusion Nous venons d’étudier le concept de comparaisons multiples ainsi que les problèmes qui y sont associés, notamment une augmentation de la probabilité de commettre une erreur de type I (déclarer un faux positif). Au lieu d’utiliser une série de tests \\(t\\), nous avons vu que l’analyse de variance (ANOVA) permet de comparer tous les groupes simultanément en estimant la variance expliquée par le facteur (variable catégorique) d’intérêt et en la comparant à la variance résiduelle (inexpliquée). Le tableau d’ANOVA véhicule beaucoup d’information et il est important d’inclure ses détails dans la rédaction des résultats. Bien que l’ANOVA permette de déterminer si le facteur a un effet important sur la variable réponse, elle ne permet pas de déterminer où se trouvent les différences. Pour ce faire, nous avons recours à des tests de comparaisons multiples exécutés a posteriori qui contrôlent l’erreur au niveau de l’expérience. Pour terminer, nous avons fait quelques propositions concernant la présentation des résultats afin de faciliter l’interprétation d’une ANOVA. Rappel: Ici, la valeur de \\(P\\) correspond à la probabilité d’obtenir une valeur de \\(F\\) supérieure ou égale à celle qu’on a observée (dans les données originales) lorsqu’on répète l’échantillonnage avec la même taille d’échantillon dans la même population et que l’hypothèse nulle est vraie. On peut écrire plus succinctement: \\(P = 0.7588\\).↩︎ Dans les cas où le nombre d’observations varie d’un groupe à l’autre, il est préférable d’utiliser la fonction plus générale, lm( ) qui permet de réaliser plusieurs types de modèles linéaires.↩︎ La fonction tapply( )↩︎ Le tableau d’ANOVA classique ne permet pas de décortiquer où se trouvent les différences entre les groupes. Néanmoins, il est possible d’utiliser des contrastes orthogonaux pour décomposer la somme des carrés du facteur à l’intérieur du tableau d’ANOVA. Ce concept est plus avancé et ne sera pas couvert dans le cours.↩︎ "],["les-boucles-avec-r.html", "6.2 Les boucles avec R", " 6.2 Les boucles avec R 6.2.1 Les boucles Les boucles dans R permettent de réaliser des tâches répétitives et ennuyeuses. Elles sont plus rapides que les tâches exécutées à la main et réduisent les occasions de commettre des erreurs. Par exemple, on peut utiliser une boucle pour effectuer une manipulation d’un jeu de données, telle que d’importer une série de fichiers stockés dans le même répertoire afin d’y extraire les données d’intérêt. 6.2.1.1 Structure Il existe plusieurs façons de créer une boucle. Celle que nous verrons ici utilise la fonction for( ) pour mettre en place la boucle. Le contenu de la boucle sera inclus entre des accolades { }. Comme premier exemple, nous allons générer 100 échantillons contenant chacun 30 observations qui proviennent d’une distribution normale N(\\(\\mu = 10\\), \\(\\sigma = 1\\)), puis nous calculerons la moyenne de chaque échantillon. Premièrement, on crée un objet pour stocker les moyennes calculées : ##objet pour stocker résultats output &lt;- rep(x = NA, times = 100) output ## [1] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA ## [35] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA ## [69] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA La boucle comme telle sera initiée par la commande : ##on crée la boucle for(i in 1:100) { ##on génère l&#39;échantillon simdata &lt;- rnorm(n = 30, mean = 10, sd = 1) ##on remplace le NA par la moyenne output[i] &lt;- mean(simdata) } Décortiquons le code présenté ci-dessus: La commande for( ) indique que la boucle sera effectuée de i = 1 jusqu’à i = 100. À noter que la boucle nécessite un nom de variable quelconque, par exemple, i, j, iter, sim, ou tout autre nom de notre choix. Toutefois, la boucle doit se référer à cette même variable plus tard. Immédiatement après la commande for( ), on trouvera une accolade { qui indique le début du corps de la boucle. À la deuxième ligne, on crée un objet temporaire simdata qui contiendra 30 observations provenant d’une distribution normale ayant une moyenne de 10 et un écart-type de 1. La prochaine ligne calcule la moyenne des observations de l’objet simdata et stocke cette moyenne dans l’objet output. Ainsi, le premier NA sera remplacé par la première moyenne. La dernière ligne contient une accolade } qui indique la fin de la première itération (i = 1). À l’itération i = 2, l’objet simdata stockera 30 nouvelles observations, et cette moyenne sera stockée dans output[i = 2]. La boucle continue jusqu’à l’itération i = 100}`. Dans notre cas, la boucle a donné le résultat, qui sera différent sur votre machine puisque nous n’avons pas spécifié set.seed( ) : output ## [1] 9.843143 9.843070 9.905685 10.194206 9.792114 9.897694 9.962324 9.898893 10.186835 9.955964 ## [11] 9.932570 10.047830 9.575321 9.712321 10.054829 10.092733 9.989643 9.982827 10.043200 10.228745 ## [21] 10.197045 10.086565 10.249376 10.177044 10.298926 9.943895 10.046831 9.395522 9.990081 9.933483 ## [31] 9.918902 10.033769 10.107036 10.085682 10.126150 10.112908 9.953102 9.725974 10.070861 10.270107 ## [41] 10.063887 10.098440 10.061172 10.437588 10.166183 10.194932 9.820259 10.192904 9.777376 10.090552 ## [51] 9.763093 9.824848 10.127299 10.249598 9.790525 10.253496 9.876486 9.753068 9.975168 9.916710 ## [61] 9.890595 9.959431 10.028949 10.053114 9.968057 10.011333 10.006457 9.764378 10.126387 10.181128 ## [71] 10.020601 9.862828 10.048828 9.955702 9.872555 10.035752 10.225581 9.989991 10.107747 10.055392 ## [81] 10.146808 9.739223 9.797985 9.995382 10.134123 10.402918 9.991563 9.951723 9.947842 10.018451 ## [91] 9.989671 9.820996 10.023143 10.155746 10.350405 10.155261 9.895186 9.974955 9.903979 9.901210 Nous avons 100 moyennes calculées à partir de 100 échantillons de 30 observations provenant d’une population N(\\(\\mu = 10\\), \\(\\sigma = 1\\)). On peut utiliser les boucles pour effectuer des manipulations plus complexes. D’ailleurs, les boucles sont utiles pour effectuer des tests de randomisation ou des simulations. Comme deuxième exemple, illustrons comment estimer la probabilité de commettre une erreur au niveau de l’expérience et au niveau des comparaisons telles que discutées dans le texte Analyse de variance à un critère. Dans l’exemple en question, nous avons considéré les probabilités de commettre des erreurs au niveau de l’expérience et au niveau des comparaisons avec trois groupes. ##illustration des erreurs au niveau de l&#39;expérience et ##au niveau des comparaisons ##3 groupes de n = 50 qui ne diffèrent pas les uns des autres ##tous provenant d&#39;une même population normale avec mu=5 et sd=3 ##alpha = 0.05 ############################# ##on crée une matrice pour stocker les résultats set.seed(seed = 221) nsims &lt;- 1000 #nombre de simulations out &lt;- matrix(NA, nrow = nsims, ncol = 4) colnames(out) &lt;- c(&quot;1 vs 2&quot;, &quot;1 vs 3&quot;, &quot;2 vs 3&quot;, &quot;Erreur liée à l&#39;expérience&quot;) ##débuter boucle for (i in 1:nsims) { ##on crée 1er groupe ##tous les groupes sont égaux (H0 est vraie) groupe1 &lt;- rnorm(n = 50, mean = 5, sd = 3) ##on crée 2ième groupe groupe2 &lt;- rnorm(n = 50, mean = 5, sd = 3) ##on crée 3ième groupe groupe3 &lt;- rnorm(n = 50, mean = 5, sd = 3) ##1vs2 comp1 &lt;- t.test(groupe1, groupe2) ##1vs3 comp2 &lt;- t.test(groupe1, groupe3) ##2vs3 comp3 &lt;- t.test(groupe2, groupe3) out[i, 1] &lt;- ifelse(comp1$p.value&lt;=0.05, &quot;*&quot;, &quot;NS&quot;) out[i, 2] &lt;- ifelse(comp2$p.value&lt;=0.05, &quot;*&quot;, &quot;NS&quot;) out[i, 3] &lt;- ifelse(comp3$p.value&lt;=0.05, &quot;*&quot;, &quot;NS&quot;) out[i, 4] &lt;- ifelse(any(c(comp1$p.value, comp2$p.value, comp3$p.value) &lt;= 0.05), 1, 0) } Le code ci-dessus commence en spécifiant une valeur initiale afin d’alimenter l’algorithme du générateur de nombres aléatoires, ce qui permet de reproduire les mêmes résultats sur tous les ordinateurs. La prochaine ligne crée l’objet nsims pour spécifier le nombre d’itérations désirées pour la boucle (ici, 1000 itérations). La création d’un objet pour spécifier les itérations permet de se référer à cet objet tout le long du code. Ainsi, si on désire modifier le nombre d’itérations, nous n’aurons qu’à modifier l’objet nsims et le reste du code demeurera inchangé. La ligne suivante crée la matrice out de 4 colonnes et nsims rangées. À noter qu’ici, les résultats des trois comparaisons (groupe 1 vs groupe 2, groupe 1 vs groupe 3 et groupe 2 vs groupe 3) seront sauvegardés dans les trois premières colonnes alors que la dernière colonne sera réservée pour déterminer l’erreur au niveau de l’expérience. La fonction colnames permet d’ajouter des étiquettes aux colonnes pour clairement identifier chaque colonne. La ligne débutant par for(i in 1:nsims) indique qu’on veut une boucle de i = 1 jusqu’à i = nsims. Ensuite, nous avons trois lignes de codes pour générer les données de chaque groupe conforme à \\(H_0\\), c’est-à-dire qu’il n’y a pas de différences entre les trois groupes (ils proviennent de la même population). Nous effectuons ensuite les comparaisons entre chaque groupe à l’aide d’un test \\(t\\). Pour chacun des tests, si la valeur de \\(P \\leq \\alpha\\), nous avons décidé d’ajouter un * dans la colonne, autrement nous avons inséré les caractères NS. À noter que si on rejette \\(H_0\\) pour un test \\(t\\) sur ces données, nous avons commis une erreur de type I puisque les données simulées sont réellement tirées de la même population (la boucle a été construite pour générer des données conformes à \\(H_0\\)). Pour une itération i donnée, à chaque fois que la colonne 1 contient *, c’est une erreur au niveau de la comparaison. Il en va de même avec les colonnes 2 et 3. La quatrième colonne de la matrice out prend la valeur de 1 si au moins une des colonnes à l’itération i a rejeté \\(H_0\\) ce qui correspond à l’erreur au niveau de l’expérience, autrement la colonne prend la valeur de 0. Ces étapes sont répétées i fois. Lorsque toutes les itérations de la boucle sont complétées, nous obtenons les résultats suivants : ##20 premières itérations out[1:20, ] ## 1 vs 2 1 vs 3 2 vs 3 Erreur liée à l&#39;expérience ## [1,] &quot;NS&quot; &quot;NS&quot; &quot;NS&quot; &quot;0&quot; ## [2,] &quot;NS&quot; &quot;NS&quot; &quot;NS&quot; &quot;0&quot; ## [3,] &quot;NS&quot; &quot;NS&quot; &quot;NS&quot; &quot;0&quot; ## [4,] &quot;NS&quot; &quot;NS&quot; &quot;NS&quot; &quot;0&quot; ## [5,] &quot;NS&quot; &quot;NS&quot; &quot;NS&quot; &quot;0&quot; ## [6,] &quot;NS&quot; &quot;NS&quot; &quot;NS&quot; &quot;0&quot; ## [7,] &quot;NS&quot; &quot;NS&quot; &quot;NS&quot; &quot;0&quot; ## [8,] &quot;NS&quot; &quot;*&quot; &quot;*&quot; &quot;1&quot; ## [9,] &quot;NS&quot; &quot;NS&quot; &quot;NS&quot; &quot;0&quot; ## [10,] &quot;NS&quot; &quot;*&quot; &quot;NS&quot; &quot;1&quot; ## [11,] &quot;NS&quot; &quot;NS&quot; &quot;NS&quot; &quot;0&quot; ## [12,] &quot;NS&quot; &quot;NS&quot; &quot;NS&quot; &quot;0&quot; ## [13,] &quot;NS&quot; &quot;NS&quot; &quot;NS&quot; &quot;0&quot; ## [14,] &quot;NS&quot; &quot;NS&quot; &quot;NS&quot; &quot;0&quot; ## [15,] &quot;NS&quot; &quot;*&quot; &quot;NS&quot; &quot;1&quot; ## [16,] &quot;NS&quot; &quot;NS&quot; &quot;NS&quot; &quot;0&quot; ## [17,] &quot;NS&quot; &quot;NS&quot; &quot;NS&quot; &quot;0&quot; ## [18,] &quot;NS&quot; &quot;NS&quot; &quot;NS&quot; &quot;0&quot; ## [19,] &quot;NS&quot; &quot;NS&quot; &quot;NS&quot; &quot;0&quot; ## [20,] &quot;NS&quot; &quot;NS&quot; &quot;NS&quot; &quot;0&quot; ##erreurs au niveau de la comparaison 1 vs 2 err.comp1 &lt;- sum(out[, 1] == &quot;*&quot;)/nsims err.comp2 &lt;- sum(out[, 2] == &quot;*&quot;)/nsims err.comp3 &lt;- sum(out[, 3] == &quot;*&quot;)/nsims err.comp1 ## [1] 0.045 err.comp2 ## [1] 0.05 err.comp3 ## [1] 0.057 ##erreur au niveau de l&#39;expérience err.exp &lt;- sum(out[, 4] == &quot;1&quot;)/nsims Afin de déterminer la probabilité de commettre une erreur au niveau des comparaisons, on peut calculer la proportion du nombre de résultats significatifs (\\(P &lt; \\alpha\\)) par rapport au nombre total de simulations. On obtient les valeurs de 0.045, 0.05 et 0.057, pour les comparaions 1 vs 2, 1 vs 3 et 2 vs 3, respectivement. Ces valeurs oscillent très près du seuil de signification car elles dépendent directement de cette valeur (ici, 0.05). L’erreur au niveau de l’expérience est de 0.124. Si on avait utilisé un seuil de 0.10, nous aurions obtenu les valeurs 0.109, 0.104 et 0.101 pour les comparaisons 1 vs 2, 1 vs 3 et 2 vs 3, respectivement. L’erreur au niveau de l’expérience dans ce cas serait de 0.245, obtenu en divisant le nombre de lignes avec au moins un résultat significatif par le nombre total d’itérations. Les calculs pour les erreurs au niveau de l’expérience et au niveau des comparaisons ne sont pas montrés ici pour un seuil \\(\\alpha = 0.10\\). Nous vous suggérons de les faire comme exercice. 6.2.2 Alternatives aux boucles L’utilisation de boucles dans R permet beaucoup de flexibilité pour réaliser des tâches très variées. Les boucles peuvent être imbriquées les unes dans les autres. Nous avons illustré l’utilisation de for( ), bien qu’il est aussi possible de réaliser des boucles avec des fonctions comme while( ) ou break( ). Pour de très gros jeux de données ou certains types de calculs, les boucles peuvent être lentes. Certaines fonctions peuvent aussi agir un peu comme des boucles. C’est le cas de la fonction apply( ) que nous verrons dans la section qui suit. 6.2.2.1 apply( ) et fonctions apparentées La fonction apply( ) et ses fonctions apparentées telles que lapply( ) et tapply( ) (pour plus d’options, faites help.search(apropos = \"apply\") permettent d’appliquer des calculs ou des manipulations à une matrice, à une liste ou à un jeu de données, entre autres. Les fonctions de la famille apply( ) utilisent un processus qu’on appelle la vectorisation, c’est-à-dire que les calculs sont effectués simultanément sur des vecteurs au lieu d’une valeur à la fois comme avec la boucle. Les fonctions apply( ) sont souvent beaucoup plus rapides que les boucles. On peut utiliser tapply( ) pour calculer une statistique (FUN) d’une variable numérique X = Ozone pour différents groupes définis par une variable catégorique INDEX = Jardin dans le jeu de données jardins.txt. ##importer le jeu de données jardins.txt ozone &lt;- read.table(&quot;Module_6/data/jardins.txt&quot;, header = TRUE) str(ozone) ## &#39;data.frame&#39;: 45 obs. of 2 variables: ## $ Ozone : num 14.82 20.59 6.18 7.61 31.35 ... ## $ Jardin: chr &quot;A&quot; &quot;A&quot; &quot;A&quot; &quot;A&quot; ... ##on calcule la moyenne de Ozone pour chacun des groupes tapply(X = ozone$Ozone, INDEX = ozone$Jardin, FUN = mean) ## A B C ## 16.34200 18.20133 18.07800 ##on calcule la SD de Ozone pour chacun des groupes tapply(X = ozone$Ozone, INDEX = ozone$Jardin, FUN = sd) ## A B C ## 8.550944 4.506536 9.037485 ##on calcule le n pour chacun des groupes tapply(X = ozone$Ozone, INDEX = ozone$Jardin, FUN = length) ## A B C ## 15 15 15 ##on détermine si chaque groupe contient des valeurs numériques tapply(X = ozone$Ozone, INDEX = ozone$Jardin, FUN = is.matrix) ## A B C ## FALSE FALSE FALSE La fonction tapply( ) est destinée aux jeux de données. De façon plus générale, on peut appliquer une fonction (FUN) sur les rangées ou colonnes (MARGIN) d’une matrice (X) à l’aide de apply( ) : ##on crée une matrice mat.sim &lt;- matrix(data = 1:24, nrow = 6, ncol = 4) mat.sim ## [,1] [,2] [,3] [,4] ## [1,] 1 7 13 19 ## [2,] 2 8 14 20 ## [3,] 3 9 15 21 ## [4,] 4 10 16 22 ## [5,] 5 11 17 23 ## [6,] 6 12 18 24 ##on calcule la somme de chaque rangée apply(X = mat.sim, MARGIN = 1, FUN = sum) ## [1] 40 44 48 52 56 60 ##on compare à rowSums - identique rowSums(mat.sim) ## [1] 40 44 48 52 56 60 ##on calcule le produit de chaque rangée apply(X = mat.sim, MARGIN = 1, FUN = prod) ## [1] 1729 4480 8505 14080 21505 31104 ##on calcule la somme de chaque colonne apply(X = mat.sim, MARGIN = 2, FUN = sum) ## [1] 21 57 93 129 ##on compare à colSums - identique colSums(mat.sim) ## [1] 21 57 93 129 ##on calcule le produit de chaque colonne apply(X = mat.sim, MARGIN = 2, FUN = prod) ## [1] 720 665280 13366080 96909120 On comprend que lorsque l’argument MARGIN prend la valeur de 1, les calculs sont effectués sur chacune des rangées. Si MARGIN a la valeur de 2, les calculs se réalisent sur chaque colonne. "],["exercices-5.html", "6.3 Exercices", " 6.3 Exercices 6.3.1 Question 1 a. Donnez un avantage d’effectuer une ANOVA au lieu de plusieurs tests \\(t\\) pour comparer les moyennes des groupes entre elles? Réponse L’avantage principal est de réduire l’erreur de type I, puisqu’on teste tous les groupes simultanément. b. Distinguez entre “erreur au niveau de la comparaison” et “erreur au niveau de l’expérience”. Réponse L’erreur au niveau de la comparaison se produit lorsqu’on rejette faussement \\(H_0\\) alors qu’elle est vraie pendant une comparaison entre deux groupes. L’erreur au niveau de l’expérience se produit lorsqu’au moins une comparaison a rejeté faussement \\(H_0\\) alors qu’elle était vraie. Le test de Tukey maintient l’erreur liée à l’expérience au seuil \\(\\alpha\\) que nous avons fixé. 6.3.2 Question 2 a. Importez le fichier Survie.txt qui présente le temps de survie en heures d’animaux exposés à trois doses de poison (faible, moyenne, elevee). On désire savoir s’il existe des différences entre les moyennes des groupes définis par les doses de poison. Réponse ##on importe le jeu de données poison &lt;- read.table(&quot;Module_6/data/Survie.txt&quot;, header = TRUE) head(poison) ## Temps Dose ## 1 15.998507 faible ## 2 12.539667 faible ## 3 3.017424 faible ## 4 7.621695 faible ## 5 22.274759 faible ## 6 24.971949 faible b. Spécifiez l’hypothèse nulle que vous pourriez tester avec ces données. Réponse Les hypothèses statistiques testées sont : \\(H_0: \\mu_\\mathtt{faible} = \\mu_\\mathtt{moyenne} = \\mu_\\mathtt{elevee}\\) \\(H_a\\): au moins une moyenne diffère des autres \\(\\alpha = 0.05\\) c. À l’aide d’une ANOVA, déterminez si le temps de survie dépend de la dose de poison. Vérifiez toutes les suppositions de l’ANOVA et effectuez des transformations si nécessaire. Réponse Avant de faire l’ANOVA, observons la structure interne des données : str(poison) ## &#39;data.frame&#39;: 48 obs. of 2 variables: ## $ Temps: num 16 12.54 3.02 7.62 22.27 ... ## $ Dose : chr &quot;faible&quot; &quot;faible&quot; &quot;faible&quot; &quot;faible&quot; ... On observe que la variable Dose est une variable caractère (chr). Or elle doit être un facteur (factor) pour être utilisée dans l’analyse. Nous devons donc la transformer : poison$Dose &lt;- as.factor(poison$Dose) Remarquer que le changement a bel et bien été réalisé dans la structure des données : str(poison) ## &#39;data.frame&#39;: 48 obs. of 2 variables: ## $ Temps: num 16 12.54 3.02 7.62 22.27 ... ## $ Dose : Factor w/ 3 levels &quot;elevee&quot;,&quot;faible&quot;,..: 2 2 2 2 2 2 2 2 2 2 ... Nous pouvons maintenant réaliser l’ANOVA. m1 &lt;- aov(Temps ~ Dose, data = poison) ## Vérifions que les suppositions sont respectées. par(mfrow = c(2, 2)) plot(m1) On constate que les variances ne sont pas homogènes, puisque le graphique des résidus en fonction des valeurs prédites montre qu’un groupe varie moins que les autres. ##on essaie une transformation log poison$log.Temps &lt;- log(poison$Temps) m2 &lt;- aov(log.Temps ~ Dose, data = poison) ## Vérifions que les suppositions sont respectées suite à cette transformation: par(mfrow = c(2, 2)) plot(m2) Les variances sont maintenant homogènes après cette transformation. Les résidus respectent aussi la condition de normalité. De plus, nous assumons que le dispositif expérimental assure la supposition d’indépendance des erreurs. Nous pouvons donc procéder avec l’interprétation de l’ANOVA. d. Que pouvez-vous conclure de l’analyse? Incluez le tableau d’ANOVA dans votre présentation des résultats. Réponse summary(m2) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Dose 2 26.27 13.136 12.49 4.85e-05 *** ## Residuals 45 47.33 1.052 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 On conclut qu’il y a au moins une moyenne d’un groupe soumis à une dose de poison qui diffère des autres groupes. e. Si vous avez rejeté l’hypothèse nulle de l’ANOVA, effectuez des comparaisons multiples. Présentez les résultats des comparaisons multiples à l’aide d’un graphique. Réponse TukeyHSD(m2) ## Tukey multiple comparisons of means ## 95% family-wise confidence level ## ## Fit: aov(formula = log.Temps ~ Dose, data = poison) ## ## $Dose ## diff lwr upr p adj ## faible-elevee 1.7976010 0.9188352 2.6763667 0.0000311 ## moyenne-elevee 1.0975613 0.2187956 1.9763271 0.0111516 ## moyenne-faible -0.7000397 -1.5788054 0.1787261 0.1418379 Le test de Tukey indique que les groupes de dose moyenne et faible ne diffèrent pas l’un de l’autre, mais que le groupe à dose elevee diffère des deux autres groupes. On peut représenter le tout dans un graphique en suivant les étapes suivantes : ##moyennes des groupes moy.orig &lt;- tapply(X = poison$log.Temps, INDEX = poison$Dose, FUN = mean) ##on met les moyenne en ordre croissant moy &lt;- sort(moy.orig) ##extraire MSE MSE &lt;- 1.0527 ##aussi possible d&#39;extraire à partir du summary #MSE &lt;- summary(m2)[[1]][2, &quot;Mean Sq&quot;] ##calculer racine carrée de MSE sqrt.MSE &lt;- sqrt(MSE) ##calculer les limites des barres d&#39;erreur lim.sup &lt;- moy + sqrt.MSE lim.inf &lt;- moy - sqrt.MSE ##créer graphique vide sans axe des x&#39;s plot(x = 0, y = 0, type = &quot;n&quot;, ylim = c(min(lim.inf), max(lim.sup)+0.1), xlim = c(0, 4), xlab = &quot;Doses de poison&quot;, ylab = &quot;Log du temps de survie&quot;, main = &quot;Moyennes ± racine-carrée de MSE&quot;, xaxt = &quot;n&quot;) ##ajouter axe des x&#39;s axis(side = 1, at = c(1, 2, 3), labels = names(moy)) ##ajouter moyennes points(x = c(1, 2, 3), y = moy) ##ajouter barres d&#39;erreurs arrows(x0 = c(1, 2, 3), y0 = lim.inf, x1 = c(1, 2, 3), y1 = lim.sup, length = 0.05, angle = 90, code = 3) ##ajouter les lettres, lim.sup + 0.05 text(x = 1, y = lim.sup[1] + 0.05, labels = &quot;b&quot;) text(x = 2, y = lim.sup[2] + 0.05, labels = &quot;a&quot;) text(x = 3, y = lim.sup[3] + 0.05, labels = &quot;a&quot;) Il est aussi approprié de présenter les résultats sur l’échelle originale de la variable. Ici, puisque nous avons utilisé la transformation logarithmique à base \\(e\\) pour effectuer l’ANOVA, on peut faire la transformation inverse pour ramener les données à l’échelle originale (l’inverse de log( ) est exp( )). ##on transforme les moyennes orig.moy &lt;- exp(moy) ##on transforme les bornes des barres d&#39;erreurs orig.lim.inf &lt;- exp(lim.inf) orig.lim.sup &lt;- exp(lim.sup) ##créer graphique vide sans axe des x&#39;s plot(x = 0, y = 0, type = &quot;n&quot;, ylim = c(min(orig.lim.inf), max(orig.lim.sup)+0.1), xlim = c(0, 4), xlab = &quot;Doses de poison&quot;, ylab = &quot;Temps de survie (h)&quot;, main = &quot;Moyennes ± racine-carrée de MSE&quot;, xaxt = &quot;n&quot;) ##ajouter axe des x&#39;s axis(side = 1, at = c(1, 2, 3), labels = names(orig.moy)) ##ajouter moyennes points(x = c(1, 2, 3), y = orig.moy) ##ajouter barres d&#39;erreurs arrows(x0 = c(1, 2, 3), y0 = orig.lim.inf, x1 = c(1, 2, 3), y1 = orig.lim.sup, length = 0.05, angle = 90, code = 3) ##ajouter les lettres, lim.sup + 0.40 text(x = 1, y = orig.lim.sup[1] + 0.40, labels = &quot;b&quot;) text(x = 2, y = orig.lim.sup[2] + 0.40, labels = &quot;a&quot;) text(x = 3, y = orig.lim.sup[3] + 0.40, labels = &quot;a&quot;) "],["analyse-de-variance-à-deux-critères.html", "Module 7 Analyse de variance à deux critères", " Module 7 Analyse de variance à deux critères Dans la leçon précédente, nous avons vu l’application de l’analyse de la variance (ANOVA) dans des expériences où il n’y avait qu’un seul critère de classification (ou facteur). Les observations étaient classées selon un seul facteur en un nombre donné de groupes. Nous pouvions ainsi tester l’influence de ce facteur (p. ex., le sexe) sur la variable réponse. Dans d’autres cas, les données peuvent être réparties selon deux facteurs ou critères de classification. Par exemple, nous pourrions avoir une expérience qui étudie la concentration de calcium dans le plasma sanguin (la variable réponse) selon le traitement hormonal (avec traitement vs sans traitement) pour des individus selon leur sexe (mâle vs femelle). Nous pouvons généraliser l’ANOVA en présence de plusieurs facteurs. En présence de deux facteurs, nous réaliserons l’ANOVA à deux critères de classification (c.-à-d., facteurs). "],["théories-6.html", "7.1 Théories", " 7.1 Théories 7.1.1 ANOVA à deux critères On pourrait avoir le réflexe d’effectuer deux analyses, en effectuant une ANOVA à un critère sur chacun des facteurs pris séparément. Toutefois, cette approche n’est pas optimale, puisqu’effectuer deux tests séparément augmentera notre probabilité de commettre une erreur de type I (c.-à-d., un faux positif). En plus de tester l’effet de chacun des facteurs sur la variable réponse, l’ANOVA à deux critères permet de tester si l’effet d’un facteur dépend du niveau de l’autre. C’est ce qu’on entend par l’interaction entre les deux facteurs sur la variable réponse. Ainsi, nous dirons qu’il y a interaction entre le traitement hormonal et le sexe lorsque l’effet du traitement sur la concentration en calcium est soit plus grand ou plus petit selon le sexe des individus étudiés, et vice-versa, c’est-à-dire que la différence de la variable réponse entre mâle et femelle dépend du traitement hormonal. L’interaction est souvent d’intérêt puisqu’elle révèle un patron inattendu. À noter qu’on peut tester le terme d’interaction uniquement lorsqu’on a plus d’une observation pour chaque combinaison des groupes des deux facteurs (p. ex., mâle avec traitement, mâle sans traitement, femelle avec traitement, femelle sans traitement). On dit alors qu’il y a des répétitions (replication). Nous illustrerons le concept de l’interaction dans les exemples de cette leçon. L’ANOVA à un critère effectuée séparément sur chaque traitement ne permet pas de tester d’interaction, et c’est pourquoi il est préférable d’utiliser l’ANOVA à deux critères pour analyser les données en une seule étape. 7.1.2 Suppositions et dispositif expérimental L’ANOVA à deux critères requiert les mêmes suppositions que l’ANOVA à un critère, soit l’indépendance des observations, l’homoscédasticité et la normalité des résidus. Toutefois, le design expérimental de l’ANOVA à deux critères diffère légèrement de celui de l’ANOVA à un critère. À titre de rappel, l’ANOVA à un critère utilise un dispositif expérimental complètement aléatoire. On attribue aléatoirement un traitement (c.-à-d., un des niveaux du facteur testé) à chacune des unités expérimentales. Ici, l’unité expérimentale dépend de l’expérience, mais peut être constituée d’un patient, d’un client sondé au téléphone, d’un quadrat, d’un aquarium, ou d’un site, par exemple. Dans une expérience avec deux facteurs, nous attribuerons aléatoirement une combinaison des deux facteurs à chaque unité expérimentale. Reprenons notre exemple sur le traitement hormonal en fonction du sexe. Quatre combinaisons de groupes des deux facteurs sont possibles: mâle sans traitement, mâle avec traitement hormonal, femelle sans traitement et femelle avec traitement hormonal. Un dispositif équilibré (balanced design), c’est-à-dire, une expérience avec un nombre égal d’observations pour chaque combinaison des traitements, procure une puissance plus élevée qu’un dispositif avec un nombre inégal d’observations par combinaison des traitements. Ainsi, on devrait sélectionner un nombre égal de mâles et de femelles et attribuer le traitement à la moitié des mâles et des femelles. 7.1.3 Hypothèses statistiques Les hypothèses statistiques de l’ANOVA à deux critères sont formulées de la même façon qu’avec l’ANOVA à un critère. Poursuivons l’exemple en testant les différences entre les moyennes des groupes définis par le facteur d’intérêt : Traitement hormonal : \\(H_0\\): \\(\\mu_{\\mathrm{horm}} = \\mu_{\\mathrm{sans.horm}}\\) (non-différence) \\(H_a\\): \\(\\mu_{\\mathrm{horm}} \\neq \\mu_{\\mathrm{sans.horm}}\\) Sexe : \\(H_0\\): \\(\\mu_{\\mathrm{m\\hat{a}le}} = \\mu_{\\mathrm{femelle}}\\) (non-différence) \\(H_a\\): \\(\\mu_{\\mathrm{m\\hat{a}le}} \\neq \\mu_{\\mathrm{femelle}}\\) Puisqu’il y a plusieurs observations par combinaison des deux facteurs (mâle avec traitement, mâle sans traitement, femelle avec traitement, femelle sans traitement), il est possible de tester l’interaction entre le traitement hormonal et le sexe. Le terme d’interaction teste si l’effet du traitement hormonal sur la variable réponse dépend du sexe. Interaction traitement hormonal \\(\\times\\) sexe : \\(H_0\\): \\(\\mu_{\\mathrm{m\\hat{a}le -- horm}} = \\mu_{\\mathrm{m\\hat{a}le -- sans.horm}} = \\mu_{\\mathrm{femelle -- horm}} = \\mu_{\\mathrm{femelle -- sans.horm}}\\) (non-différence) \\(H_a\\): au moins une moyenne diffère des autres En mots, si la différence de moyenne entre traitement et sans traitement est égale entre mâle et femelle, il n’y a pas d’interactions et on ne rejette pas \\(H_0\\). À noter que cette dernière hypothèse ne peut pas être testée en présence d’une seule observation pour chaque combinaison des deux facteurs – il faut plus d’une observation (répétition) pour tester le terme d’interaction. Le seuil de signification (\\(\\alpha\\)) pour chacune de ces hypothèses est de 0.05. 7.1.4 Sommes des carrés On construit les sommes des carrés de l’ANOVA à deux critères de classification de façon similaire à l’ANOVA à un critère de classification. Ainsi, la somme des carrés totale (\\(SST\\)) s’obtient avec : \\[SST = \\sum_{i=1}^a \\sum_{j=1}^b \\sum_{k=1}^n (y_{ijk} - \\bar{y})^2\\] où on compare l’observation \\(k\\) du groupe \\(i\\) du facteur \\(A\\) et du groupe \\(j\\) du facteur \\(B\\) (\\(y_{ijk}\\)) à la moyenne globale (\\(\\bar{y}\\)). Les degrés de liberté associés à la somme des carrés totale est obtenue à l’aide de \\(df = N - 1\\), où \\(N\\) correspond au nombre total d’observations dans l’expérience. On calcule la somme des carrés de chaque facteur de la même manière que pour une ANOVA à un critère. Nous obtenons ainsi : \\[ SSA = \\sum_{i=1}^a \\sum_{k=1}^n (\\bar{y}_{i} - \\bar{y})^2 = n \\sum_{i=1}^a (\\bar{y}_{i} - \\bar{y})^2\\] où on compare la moyenne de chaque groupe \\(i\\) du facteur \\(A\\) (\\(\\bar{y}_{i}\\)) à la moyenne globale (\\(\\bar{y}\\)) et où ce calcul se répète \\(n\\) fois pour chaque moyenne d’un groupe \\(i\\) donné. Les degrés de liberté correspondant à \\(SSA\\) sont donnés par \\(df = a - 1\\), où \\(a\\) est le nombre de groupes défini par le facteur A. Le calcul de la somme des carrés du deuxième facteur (\\(B\\)) est identique à celui du facteur \\(A\\) : \\[SSB = \\sum_{j=1}^b \\sum_{k=1}^n (\\bar{y}_{j} - \\bar{y})^2 = n \\sum_{j=1}^b (\\bar{y}_{j} - \\bar{y})^2\\] Les degrés de liberté s’obtiennent avec \\(df = b - 1\\), où \\(b\\) correspond au nombre de groupes du facteur \\(B\\). La somme des carrés résiduelle (\\(SSE\\)), aussi appelée somme des carrés des erreurs, se calcule de la même façon que pour l’ANOVA à un critère: \\[SSE = \\sum_{i=1}^a \\sum_{j=1}^b \\sum_{k=1}^n (y_{ijk} - \\bar{y}_{ij})^2\\] où on compare une observation \\(k\\) appartenant à la fois à un groupe donné \\(i\\) du facteur \\(A\\) et d’un groupe \\(j\\) du facteur \\(B\\) à la moyenne de ces groupes combinés (\\(\\bar{y}_{ij}\\)). Les degrés de liberté associés à \\(SSE\\) sont \\(df = ab(n - 1)\\), où \\(a\\) et \\(b\\) sont le nombre de groupes du facteur \\(A\\) et \\(B\\), respectivement, et \\(n\\) est le nombre d’observations pour chaque combinaison des facteurs \\(A\\) et \\(B\\). L’ANOVA à deux critères qui contient plus d’une observation des combinaisons de groupes des deux facteurs permet de tester l’interaction entre les deux facteurs de l’expérience. La somme des carrés de l’interaction (\\(SSInter\\)) s’obtient avec: \\[SSinter = \\sum_{i=1}^a \\sum_{j=1}^b \\sum_{k=1}^n ( (\\bar{y}_{ij} -\\bar{y}) - (\\bar{y}_i - \\bar{y} ) - (\\bar{y}_j - \\bar{y}) )^2\\] où le premier terme, \\((\\bar{y}_{ij} -\\bar{y})\\), est la différence observée de la moyenne des groupes combinés \\(i\\) et \\(j\\) avec la moyenne globale, et les deux autres termes représentent la différence que devrait avoir cette moyenne des groupes combinés avec la moyenne globale s’il n’y avait pas d’interaction, c-à-d, la somme des différences entre la moyenne du groupe \\(i\\) et la moyenne globale, \\((\\bar{y}_{i} -\\bar{y})\\), et entre la moyenne du groupe \\(j\\) et la moyenne globale, \\((\\bar{y}_{j} -\\bar{y})\\). Nous pouvons simplifier l’équation pour \\(SSinter\\): \\[ SSinter = \\sum_{i=1}^a \\sum_{j=1}^b \\sum_{k=1}^n (\\bar{y}_{ij} - \\bar{y}_i - \\bar{y}_j + \\bar{y})^2 = n \\sum_{i=1}^a \\sum_{j=1}^b (\\bar{y}_{ij} - \\bar{y}_i - \\bar{y}_j + \\bar{y})^2\\] L’exemple qui suit illustre le calcul de ces différentes valeurs nécessaires à l’ANOVA à deux critères. Les degrés de liberté associés au terme \\(SSInter\\) sont donnés par \\(df = (a - 1)(b - 1)\\), où \\(a\\) et \\(b\\) correspondent aux nombre de groupes du facteur \\(A\\) et \\(B\\), respectivement. On s’intéresse à l’effet d’un traitement hormonal sur la concentration de calcium dans le plasma sanguin chez des oiseaux mâles et femelles. La variable réponse est la concentration du calcium dans le plasma sanguin. Nos deux facteurs sont le traitement hormonal (avec traitement, sans traitement) et le sexe (mâle, femelle). Les données sont contenues dans le fichier calcium.txt. ## On importe le jeu de données calcium &lt;- read.table(&quot;Module_7/data/calcium.txt&quot;, header = TRUE) ## On jette un coup d&#39;oeil aux premières observations head(calcium) ## Concentration Trait Sexe ## 1 16.5 sans_horm f ## 2 18.4 sans_horm f ## 3 12.7 sans_horm f ## 4 14.0 sans_horm f ## 5 12.8 sans_horm f ## 6 14.5 sans_horm m ## On transforme en facteurs les colonnes ## de caractères `Trait` et `Sexe` calcium$Trait &lt;- as.factor(calcium$Trait) calcium$Sexe &lt;- as.factor(calcium$Sexe) La Figure @ref(fig:boxplotcalcium illustre les données selon les différentes combinaisons des deux facteurs. ## On crée un boxplot pour chaque ## combinaison des deux facteurs boxplot(Concentration ~ Trait + Sexe, data = calcium, ylab = &quot;Concentration&quot;, xlab = &quot;Combinaison de traitement et sexe&quot;, cex.lab = 1.2) Figure 7.1: Diagramme de boîtes et moustaches avec les données de concentration en calcium en fonction du traitement hormonal et du sexe. On peut déterminer le nombre d’observations pour chaque combinaison des deux facteurs à l’aide de la fonction table( ) : ## On détermine le nombre de répétitions par groupe table(calcium$Trait, calcium$Sexe) ## ## f m ## horm 5 5 ## sans_horm 5 5 On constate qu’il y a cinq observations pour chaque combinaison des niveaux des deux facteurs. Nous dirons donc qu’il y a cinq répétitions (replicates) par combinaison de traitements. Calculons la somme des carrés totale : ##moyenne globale y.bar &lt;- mean(calcium$Concentration) ##SST SST &lt;- sum((calcium$Concentration - y.bar)^2) SST ## [1] 1827.697 ##df df.SST &lt;- length(calcium$Concentration) - 1 On peut ensuite calculer la somme des carrés des facteurs \\(A\\) et \\(B\\) : ##sous-jeu de données fem &lt;- calcium[calcium$Sexe == &quot;f&quot;, ] mal &lt;- calcium[calcium$Sexe == &quot;m&quot;, ] horm &lt;- calcium[calcium$Trait == &quot;horm&quot;, ] nohorm &lt;- calcium[calcium$Trait == &quot;sans_horm&quot;, ] ##moyenne des groupes y.bar.f &lt;- mean(fem$Concentration) y.bar.m &lt;- mean(mal$Concentration) y.bar.horm &lt;- mean(horm$Concentration) y.bar.nohorm &lt;- mean(nohorm$Concentration) ## Calcul de SSA ## où le nombre d&#39;observations femelle et mâle est nrow(fem) ## [1] 10 nrow(mal) ## [1] 10 SSA &lt;- 10*(y.bar.f - y.bar)^2 + 10*(y.bar.m - y.bar)^2 SSA ## [1] 70.3125 ## Calculer df.SSA. On a df.SSA&lt;- a - 1 ## où a est le nombre de groupes definis par le facteur A. ## Ici, il y a homme et femme, donc a = 2. df.SSA &lt;- 2 - 1 df.SSA ## [1] 1 ##SSB ## où le nombre d&#39;observations horm et nohorm est nrow(horm) ## [1] 10 nrow(nohorm) ## [1] 10 SSB &lt;- 10*(y.bar.horm - y.bar)^2 + 10*(y.bar.nohorm - y.bar)^2 SSB ## [1] 1386.112 ## Calculer df.SSB. On a df.SSB&lt;- b - 1 ## où b est le nombre de groupes définis par le facteur B. ## Ici, il y a traitement hormonal et pas ## de traitement, donc b = 2 df.SSB &lt;- 2 - 1 df.SSB ## [1] 1 La somme des carrés des erreurs se calcule comme suit : ##sous-groupes en combinant les deux facteurs f.horm &lt;- calcium[calcium$Sexe == &quot;f&quot; &amp; calcium$Trait == &quot;horm&quot;, ] m.horm &lt;- calcium[calcium$Sexe == &quot;m&quot; &amp; calcium$Trait == &quot;horm&quot;, ] f.nohorm &lt;- calcium[calcium$Sexe == &quot;f&quot; &amp; calcium$Trait == &quot;sans_horm&quot;, ] m.nohorm &lt;- calcium[calcium$Sexe == &quot;m&quot; &amp; calcium$Trait == &quot;sans_horm&quot;, ] ## ##calculs des moyennes des combinaisons de groupes y.bar.f.horm &lt;- mean(f.horm$Concentration) y.bar.m.horm &lt;- mean(m.horm$Concentration) y.bar.f.nohorm &lt;- mean(f.nohorm$Concentration) y.bar.m.nohorm &lt;- mean(m.nohorm$Concentration) ## ## Calcul de SSE SSE.f.horm &lt;- sum((f.horm$Concentration - y.bar.f.horm)^2) SSE.m.horm &lt;- sum((m.horm$Concentration - y.bar.m.horm)^2) SSE.f.nohorm &lt;- sum((f.nohorm$Concentration - y.bar.f.nohorm)^2) SSE.m.nohorm &lt;- sum((m.nohorm$Concentration - y.bar.m.nohorm)^2) SSE &lt;- SSE.f.horm + SSE.m.horm + SSE.f.nohorm + SSE.m.nohorm SSE ## [1] 366.372 ## ## Cacul des degrés de liberté ## df.SSE = ab(n-1), où a et b sont le nombre de groupes ## du facteur A et B, respectivement, ## et n est le nombre d&#39;observations pour chaque combinaison ## des facteurs A et B. df.SSE &lt;- length(levels(calcium$Sexe)) * length(levels(calcium$Trait)) * (5 - 1) df.SSE ## [1] 16 Finalement, on obtient la somme des carrés du terme d’interaction entre les deux facteurs. Notez que l’équation pour \\(SSinter\\) peut se décomposer de la façon suivante: \\[SSinter = n \\sum_{i=1}^a \\sum_{j=1}^b (\\bar{y}_{ij} - \\bar{y}_i - \\bar{y}_j + \\bar{y})^2 \\] \\[= n \\sum_{i=1}^a ( (\\bar{y}_{i.horm} - \\bar{y}_i - \\bar{y}_{horm} + \\bar{y})^2 + (\\bar{y}_{i.nohorm} - \\bar{y}_i - \\bar{y}_{nohorm} + \\bar{y})^2)\\] \\[= n ( (\\bar{y}_{f.horm} - \\bar{y}_f - \\bar{y}_{horm} + \\bar{y})^2\\] \\[+ (\\bar{y}_{f.nohorm} - \\bar{y}_f - \\bar{y}_{nohorm} + \\bar{y})^2 \\] \\[+ (\\bar{y}_{m.horm} - \\bar{y}_m - \\bar{y}_{horm} + \\bar{y})^2 \\] \\[+ (\\bar{y}_{m.nohorm} - \\bar{y}_m - \\bar{y}_{nohorm} + \\bar{y})^2)) \\] Ansi, nous calculons \\(SSinter\\) de la façon suivante : ## SS de l&#39;interaction SSInter.f.horm &lt;- 5*sum((y.bar.f.horm - y.bar.f - y.bar.horm + y.bar)^2) SSInter.m.horm &lt;- 5*sum((y.bar.m.horm - y.bar.m - y.bar.horm + y.bar)^2) SSInter.f.nohorm &lt;- 5*sum((y.bar.f.nohorm - y.bar.f - y.bar.nohorm + y.bar)^2) SSInter.m.nohorm &lt;- 5*sum((y.bar.m.nohorm - y.bar.m - y.bar.nohorm + y.bar)^2) SSInter &lt;- SSInter.f.horm + SSInter.m.horm + SSInter.f.nohorm + SSInter.m.nohorm SSInter ## [1] 4.9005 ## degrés de liberté de SSInter ## df.SSInter = (a-1)(b-1), où a et b correspondent aux ## nombre de groupes du facteur A et B, respectivement. df.SSInter &lt;- df.SSA * df.SSB df.SSInter ## [1] 1 On peut réaliser tous ces calculs directement dans R à l’aide de la fonction aov( ) : aov2 &lt;- aov(Concentration ~ Sexe + Trait + Sexe:Trait, data = calcium) summary(aov2) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Sexe 1 70.3 70.3 3.071 0.0989 . ## Trait 1 1386.1 1386.1 60.534 7.94e-07 *** ## Sexe:Trait 1 4.9 4.9 0.214 0.6499 ## Residuals 16 366.4 22.9 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 On peut reconstituer ce même tableau à partir des calculs que nous avons effectués plus haut : result &lt;- data.frame(Df = c(df.SSA, df.SSB, df.SSInter, df.SSE), Sum.Sq = c(SSA, SSB, SSInter, SSE)) result$Mean.Sq &lt;- result$Sum.Sq/result$Df result$F.value &lt;- c(result$Mean.Sq[1]/result$Mean.Sq[4], result$Mean.Sq[2]/result$Mean.Sq[4], result$Mean.Sq[3]/result$Mean.Sq[4], NA) result$P.value &lt;- c(1 - pf(result$F.value[1], df1 = result$Df[1], df2 = result$Df[4]), 1 - pf(result$F.value[2], df1 = result$Df[2], df2 = result$Df[4]), 1 - pf(result$F.value[3], df1 = result$Df[3], df2 = result$Df[4]), NA) ##tableau identique à aov( ) result ## Df Sum.Sq Mean.Sq F.value P.value ## 1 1 70.3125 70.31250 3.070650 9.885618e-02 ## 2 1 1386.1125 1386.11250 60.533556 7.943078e-07 ## 3 1 4.9005 4.90050 0.214012 6.498700e-01 ## 4 16 366.3720 22.89825 NA NA Avant d’interpréter les résultats, on doit vérifier les suppositions du modèle. On vérifie les suppositions de l’ANOVA à deux critères avec les mêmes outils que pour l’ANOVA à un critère. On constate que les résidus suivent une distribution normale (Figure 7.2a): ##vérification de la normalité des résidus par(mfrow = c(1, 2)) qqnorm(residuals(aov2), main = &quot;Normalité des résidus&quot;, ylab = &quot;Quantiles observés&quot;, xlab = &quot;Quantiles théoriques&quot;, cex.lab = 1.2) qqline(residuals(aov2)) text(x = -1.9, y = 7.5, labels = &quot;a&quot;, cex = 1.2) plot(residuals(aov2) ~ fitted(aov2), xlab = &quot;Valeurs prédites&quot;, ylab = &quot;Résidus&quot;, cex.lab = 1.2) text(x = 12.5, y = 7.5, labels = &quot;b&quot;, cex = 1.2) Figure 7.2: Graphique quantile-quantile évaluant la normalité des résidus (a) et le graphique des résidus en fonction des valeurs prédites pour diagnostiquer l’hétérogénéité de la variance (b) à partir de l’ANOVA à deux critères sur les données de concentration en calcium dans le plasma sanguin. Le graphique des résidus en fonction des valeurs prédites présente des signes d’hétérogénéité de la variance (Figure 7.2b). En effet, le patron en forme d’entonnoir montre que la variance augmente avec les valeurs prédites. Par conséquent, on ne peut pas interpréter le tableau de l’ANOVA obtenu plus haut. Afin de corriger l’hétérogénéité de la variance, la transformation logarithmique s’avère souvent efficace. Essayons-la sur la concentration en calcium. ## Transformation log calcium$log.concentration &lt;- log(calcium$Concentration) aov.log &lt;- aov(log.concentration ~ Trait + Sexe + Trait:Sexe, data = calcium) Produisons le nouveau graphique pour évaluer la normalité des résidus et l’hétérogénéité de la variace. par(mfrow = c(1, 2)) qqnorm(residuals(aov.log), ylab = &quot;Quantiles observés&quot;, xlab = &quot;Quantiles prédits&quot;, main = &quot; &quot;, cex.lab = 1.2) qqline(residuals(aov.log)) text(x = 1.5, y = -0.35, labels = &quot;a&quot;, cex = 1.2) plot(residuals(aov.log) ~ fitted(aov.log), ylab = &quot;Résidus&quot;, xlab = &quot;Valeurs prédites&quot;, cex.lab = 1.2) text(x = 3.3, y = -0.35, labels = &quot;b&quot;, cex = 1.2) Figure 7.3: Graphique quantile-quantile (a) et résidus en fonction des valeurs prédites (b) de l’ANOVA à deux critères réalisée à partir des données de concentration de calcium log-transformées. Les suppositions de normalité et d’homogénéité de la variance sont maintenant respectées (Figure 7.3). Le tableau d’ANOVA réalisé à partir du logarithme de la concentration en calcium donne : ## Résultats summary(aov.log) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Trait 1 3.196 3.196 86.121 7.69e-08 *** ## Sexe 1 0.145 0.145 3.901 0.0658 . ## Trait:Sexe 1 0.007 0.007 0.176 0.6805 ## Residuals 16 0.594 0.037 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Lorsqu’on effectue une analyse de variance à deux critères avec répétitions, on doit toujours vérifier en premier la signification du terme d’interaction. Ici, on remarque qu’il n’y a pas d’interaction entre les deux facteurs (). Par ailleurs, on remarque un effet marginal du sexe ()22 et un effet important du traitement hormonal sur le log de la concentration en calcium (\\(F_{1, 16} = 86.12, P &lt; 0.0001\\)). L’effet de chaque facteur est indépendant de l’effet de l’autre, puisque l’ANOVA n’a pu démontrer que le terme d’interaction était statistiquement significatif. Tout comme l’ANOVA à un critère, on peut poursuivre l’ANOVA à deux critères avec des comparaisons multiples afin de déterminer où se trouvent les différences lorsque nous avons rejeté \\(H_0\\)23. ##comparaisons multiples - Sexe sexe.mult &lt;- TukeyHSD(aov.log, which = &quot;Sexe&quot;) sexe.mult$Sexe ## diff lwr upr p adj ## m-f -0.1701524 -0.3527725 0.01246759 0.06576033 ##comparaisons multiples - Trait trait.mult &lt;- TukeyHSD(aov.log, which = &quot;Trait&quot;) trait.mult$Trait ## diff lwr upr p adj ## sans_horm-horm -0.7994414 -0.9820614 -0.6168213 7.686578e-08 Les comparaisons multiples basées sur le test de Tukey indiquent la même conclusion que le tableau d’ANOVA, puisque chaque facteur ne comporte ici que deux niveaux. Le log de la concentration en calcium du plasma sanguin des femelles est marginalement supérieur à celui des mâles (\\(\\bar{x}_{\\mathrm{m\\hat{a}les}} - \\bar{x}_{\\mathrm{femelles}} = -0.17, P = 0.0658\\)). En ce qui concerne le facteur du traitement hormonal, on constate que le groupe avec traitement hormonal est supérieur à celui du groupe sans traitement hormonal (\\(\\bar{x}_{\\mathrm{sans\\_horm}} - \\bar{x}_{\\mathrm{horm}} = -0.799, P &lt; 0.0001\\)). On peut aussi illustrer les résultats à l’aide d’un graphique en représentant les moyennes de chaque groupe \\(\\pm\\) un intervalle de confiance à 95 %. Tout d’abord produisons un graphique sur une échelle logarithmique. ## Sexe - moyenne des groupes ## valeurs pour lesquelles faire des prédictions ## On commence par créer un tableau dans lequel ## chaque ligne correspond à une combinaison des facteurs A et B. pred.set &lt;- expand.grid(Sexe = c(&quot;m&quot;, &quot;f&quot;), Trait = c(&quot;sans_horm&quot;, &quot;horm&quot;)) ## La fonction predict nous donne les valeurs prédites par l’ANOVA sex.means &lt;- predict(aov.log, newdata = pred.set, se.fit = TRUE) ## Ajout des prédictions dans le jeu de données pred.set$fit &lt;- sex.means$fit pred.set$se.fit &lt;- sex.means$se.fit ##IC à 95% ## On détermine l’intervalle de confiance pour ## construire nos barres d’erreur ## Rappelons que l’intervalle de confiance est donné par : ## (IC.inf, IC.sup) (voir leçon 2) ## IC.inf = x.bar + qt(p=0.025, df)*SE ## IC.sup = x.bar - qt(p=0.025, df)*SE pred.set$low95 &lt;- pred.set$fit + qt(p = 0.025, df = aov.log$df.residual) * pred.set$se.fit pred.set$upp95 &lt;- pred.set$fit - qt(p = 0.025, df = aov.log$df.residual) * pred.set$se.fit pred.set ##créer le graphique plot(y = 0, x = 0, ylab = &quot;Log de la concentration en calcium&quot;, xlab = &quot;Traitement&quot;, ylim = c(min(pred.set$low95), max(pred.set$upp95)), cex.lab = 2, xlim = c(0, 3), type = &quot;n&quot;, cex.axis = 2, xaxt = &quot;n&quot;, main = &quot;Moyennes ± IC 95 % (échelle log)&quot;, cex.main = 2) ##ajout de l&#39;axe des x&#39;s axis(side = 1, at = c(1, 2), labels = c(&quot;sans horm&quot;, &quot;horm&quot;), cex.axis = 2) ##ajout des points pour mâles points(y = pred.set$fit[c(1,3)], x = c(0.9, 1.9), pch = 1, cex = 2) ##ajout des points pour femelles points(y = pred.set$fit[c(2,4)], x = c(1.1, 2.1), pch = 2, cex = 2) ##barres d&#39;erreurs pour mâles arrows(x0 = c(0.9, 1.9), y0 = pred.set$low95[c(1, 3)], y1 = pred.set$upp95[c(1, 3)], x1 = c(0.9, 1.9), angle = 90, code = 3, length = 0.05) ##barres d&#39;erreurs pour femelles arrows(x0 = c(1.1, 2.1), y0 = pred.set$low95[c(2, 4)], y1 = pred.set$upp95[c(2, 4)], x1 = c(1.1, 2.1), angle = 90, code = 3, length = 0.05) ##ajouter légende legend(x = &quot;topleft&quot;, legend = c(&quot;mâles&quot;, &quot;femelles&quot;), pch = c(1, 2)) text(x = 2.8, y = 2.3, labels = &quot;a&quot;, cex = 2) ## Sexe Trait fit se.fit low95 upp95 ## 1 m sans_horm 2.482887 0.08614538 2.300267 2.665507 ## 2 f sans_horm 2.689163 0.08614538 2.506543 2.871783 ## 3 m horm 3.318452 0.08614538 3.135832 3.501072 ## 4 f horm 3.452481 0.08614538 3.269861 3.635101 Figure 7.4: Résultats de l’ANOVA à deux critères présentés sur l’échelle logarithmique (a) et sur l’échelle originale de la variable réponse (b). Le graphique sur l’échelle logarithmique (Figure 7.4a) montre clairement que les effets sont additifs (pas d’effet d’interaction), puisque la distance entre le cercle et le triangle du traitement hormonal est la même que celle entre les deux symboles du traitement sans hormone. Il est aussi approprié de présenter les résultats sur l’échelle originale de la variable réponse en effectuant l’opération inverse du logarithme naturel (c.-à-d., exp( )) sur la moyenne et sur les bornes inférieures et supérieures des intervalles de confiance. ## On prend l’exponentiel pour convertir les données ## transformées par le logarithme. pred.set$orig.fit &lt;- exp(pred.set$fit) pred.set$orig.low95 &lt;- exp(pred.set$low95) pred.set$orig.upp95 &lt;- exp(pred.set$upp95) ##créer graphique plot(y = 0, x = 0, ylab = &quot;Concentration en calcium&quot;, xlab = &quot;Traitement&quot;, cex.lab = 2, ylim = c(min(pred.set$orig.low95), max(pred.set$orig.upp95)), xlim = c(0, 3), type = &quot;n&quot;, cex.axis = 2, xaxt = &quot;n&quot;, main = &quot;Moyennes ± IC 95 % (échelle originale)&quot;, cex.main = 2) ##ajout de l&#39;axe des x&#39;s axis(side = 1, at = c(1, 2), labels = c(&quot;sans horm&quot;, &quot;horm&quot;), cex.axis = 2) ##ajout des points pour mâles points(y = pred.set$orig.fit[c(1,3)], x = c(0.9, 1.9), pch = 1, cex = 2) ##ajout des points pour femelles points(y = pred.set$orig.fit[c(2,4)], x = c(1.1, 2.1), pch = 2, cex = 2) ##barres d&#39;erreurs pour mâles arrows(x0 = c(0.9, 1.9), y0 = pred.set$orig.low95[c(1, 3)], y1 = pred.set$orig.upp95[c(1, 3)], x1 = c(0.9, 1.9), angle = 90, code = 3, length = 0.05) ##barres d&#39;erreurs pour femelles arrows(x0 = c(1.1, 2.1), y0 = pred.set$orig.low95[c(2, 4)], y1 = pred.set$orig.upp95[c(2, 4)], x1 = c(1.1, 2.1), angle = 90, code = 3, length = 0.05) text(x = 2.8, y = 10, labels = &quot;b&quot;, cex = 2) ##ajouter légende legend(x = &quot;topleft&quot;, legend = c(&quot;mâles&quot;, &quot;femelles&quot;), pch = c(1, 2), cex = 2) En l’absence d’une interaction statistiquement significative (comme dans cet exemple), on présente les résultats séparément pour chaque facteur (Figure 7.5). On voit à la Figure 7.5 que la concentration de calcium du groupe sans traitement hormonal est bien inférieure à celle du groupe avec traitement hormonal, alors que les différences entre mâles et femelles sont beaucoup moins marquées. Figure 7.5: Résultats présentés séparément pour chaque facteur de l’ANOVA à deux critères présentés sur l’échelle d’origine. Les groupes qui ont les mêmes lettres ne diffèrent pas entre eux pour un facteur donné et avec un seuil de d’effet significatif \\(\\alpha\\) &lt; 0.05. 7.1.5 Effets additifs Dans l’exemple précédent, on remarque que l’effet du premier facteur est indépendant de l’effet du deuxième facteur. En d’autres mots, la différence entre les mâles et les femelles est constante, peu importe le traitement hormonal. En l’absence d’une interaction statistiquement significative entre les deux facteurs, nous dirons que les effets des facteurs sont additifs. C’est ce qu’on entend par l’additivité des facteurs. En contrepartie, la présence d’une interaction significative entre les facteurs complique l’interprétation des résultats de l’expérimentation. Effectivement, la présence d’une interaction permet d’identifier des phénomènes souvent plus intéressants scientifiquement que s’il n’y avait pas d’interaction entre les facteurs. C’est ce que nous constaterons dans certains des exemples suivants. Nous désirons déterminer l’effet du type de moulée (avoine, blé ou orge) et du type de supplément alimentaire (agrimore, control, supergain et supersupp) sur le gain en masse en kg de bétail après 6 semaines. Les données sont stockées dans le fichier croissance.csv24. ## importation avec séparateur virgule car ici les champs sont séparés par des virgules croissance &lt;- read.table(&quot;Module_7/data/croissance.csv&quot;, header = TRUE, sep = &quot;,&quot;) head(croissance) ## Supplement Diete Gain ## 1 supergain ble 17.37125 ## 2 supergain ble 16.81489 ## 3 supergain ble 18.08184 ## 4 supergain ble 15.78175 ## 5 control ble 17.70656 ## 6 control ble 18.22717 ## la fonction str() affiche la structure interne du tableau C’est une fonction alternative à summary() str(croissance) ## &#39;data.frame&#39;: 48 obs. of 3 variables: ## $ Supplement: chr &quot;supergain&quot; &quot;supergain&quot; &quot;supergain&quot; &quot;supergain&quot; ... ## $ Diete : chr &quot;ble&quot; &quot;ble&quot; &quot;ble&quot; &quot;ble&quot; ... ## $ Gain : num 17.4 16.8 18.1 15.8 17.7 ... ## comparer avec importation sans sep = &#39;,&#39; crois2 &lt;- read.table(&quot;Module_7/data/croissance.csv&quot;, header = TRUE) head(crois2) ## Supplement X..Diete...Gain. ## 1 supergain ,&quot;ble&quot;,17.37125111 ## 2 supergain ,&quot;ble&quot;,16.81488903 ## 3 supergain ,&quot;ble&quot;,18.0818374 ## 4 supergain ,&quot;ble&quot;,15.78174829 ## 5 control ,&quot;ble&quot;,17.70656456 ## 6 control ,&quot;ble&quot;,18.22716932 ## on remarque un problème à l&#39;importation les étiquettes des variables et les valeurs sont erronées str(crois2) ## &#39;data.frame&#39;: 48 obs. of 2 variables: ## $ Supplement : chr &quot;supergain&quot; &quot;supergain&quot; &quot;supergain&quot; &quot;supergain&quot; ... ## $ X..Diete...Gain.: chr &quot;,\\&quot;ble\\&quot;,17.37125111&quot; &quot;,\\&quot;ble\\&quot;,16.81488903&quot; &quot;,\\&quot;ble\\&quot;,18.0818374&quot; &quot;,\\&quot;ble\\&quot;,15.78174829&quot; ... ## transformer en facteurs les variables en caracteres croissance$Supplement &lt;- as.factor(croissance$Supplement) croissance$Diete &lt;- as.factor(croissance$Diete) On vérifie le nombre de répétitions pour chaque combinaison des deux facteurs : table(croissance$Supplement, croissance$Diete) ## ## avoine ble orge ## agrimore 4 4 4 ## control 4 4 4 ## supergain 4 4 4 ## supersupp 4 4 4 Nous disposons de quatre répétitions pour chaque combinaison de facteurs. Ainsi, nous pourrons inclure un terme d’interaction dans l’ANOVA à deux facteurs. Avant de lancer l’analyse, on peut jeter un coup d’oeil aux données brutes à l’aide d’un diagramme de boîtes et moustaches (Figure 7.6). ##on crée un boxplot boxplot(Gain ~ Supplement + Diete, data = croissance) Figure 7.6: Diagramme de boîtes et moustaches présentant les données de gain de masse en fonction du type de moulée (Diete) et de supplément (Supplement). Calculons maintenant l’ANOVA : aov.crois &lt;- aov(Gain ~ Supplement + Diete + Supplement:Diete, data = croissance) summary(aov.crois) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Supplement 3 91.88 30.63 17.82 2.95e-07 *** ## Diete 2 287.17 143.59 83.52 3.00e-14 *** ## Supplement:Diete 6 3.41 0.57 0.33 0.917 ## Residuals 36 61.89 1.72 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Avant d’interpréter les résultats, on vérifie les suppositions du modèle. par(mfrow = c(1, 2)) ##Vérification de la normalité des résidus qqnorm(residuals(aov.crois), ylab = &quot;Quantiles observés&quot;, xlab = &quot;Quantiles théoriques&quot;, main = &quot;Graphique quantile-quantile&quot;, cex.lab = 1.2) qqline(residuals(aov.crois)) ##Vérification de l&#39;homogénéité de la variance plot(residuals(aov.crois) ~ fitted(aov.crois), ylab = &quot;Résidus&quot;, xlab = &quot;Valeurs prédites&quot;, main = expression(paste(bold(Résidus), &quot; &quot;, bold(italic(vs)), &quot; &quot;, bold(valeurs), &quot; &quot;, bold(prédites))), cex.lab = 1.2) Figure 7.7: Vérification des suppositions de l’ANOVA à deux critères. Nous remarquons que les résidus sur le graphique suivent généralement une distribution normale et qu’à l’exception de trois observations, l’homogénéité des variances est respectée (Figure 7.7). Nous pouvons donc procéder à l’interprétation des résultats. Le type de supplément et le type de moulée ont tous deux un effet important sur le gain de masse (supplément: \\(F_{3, 36} = 17.81, P &lt; 0.0001\\); type de moulée: \\(F_{2, 36} = 83.52, P &lt; 0.0001\\)). Toutefois, il n’y aucune preuve d’une interaction entre les deux facteurs (\\(F_{6, 36} = 0.33, P = 0.92\\)). On peut donc conclure que les effets sont additifs. Les comparaisons multiples suggèrent des différences entre certains groupes de chaque facteur : ##On test d’abord les interactions entre les suppléments tapply(X = croissance$Gain, INDEX = croissance$Supplement, FUN = mean) ## agrimore control supergain supersupp ## 23.09531 20.39861 19.71385 22.36796 ## ##Tukey mult.supp &lt;- TukeyHSD(aov.crois, which = &quot;Supplement&quot;) mult.supp$Supplement ## diff lwr upr p adj ## control-agrimore -2.6967005 -4.138342 -1.2550592 7.641492e-05 ## supergain-agrimore -3.3814586 -4.823100 -1.9398173 1.534370e-06 ## supersupp-agrimore -0.7273521 -2.168993 0.7142892 5.326710e-01 ## supergain-control -0.6847581 -2.126399 0.7568832 5.817637e-01 ## supersupp-control 1.9693484 0.527707 3.4109897 4.053368e-03 ## supersupp-supergain 2.6541065 1.212465 4.0957478 9.722472e-05 ##On test les interactions entre les diètes tapply(X = croissance$Gain, INDEX = croissance$Diete, FUN = mean) ## avoine ble orge ## 21.32882 18.43134 24.42164 ##Tukey mult.diete &lt;- TukeyHSD(aov.crois, which = &quot;Diete&quot;) mult.diete$Diete ## diff lwr upr p adj ## ble-avoine -2.897481 -4.030582 -1.764379 9.530072e-07 ## orge-avoine 3.092817 1.959715 4.225918 2.634600e-07 ## orge-ble 5.990298 4.857196 7.123399 0.000000e+00 On peut représenter les résultats des comparaisons multiples comme suit pour le type de supplément : supergain control supersupp agrimore 19.714 20.399 22.368 23.005 On peut représenter les résultats des comparaisons multiples comme suit pour le type de moulée (Diete) : blé avoine orge 18.431 21.329 24.422 La figure 7.8 illustre les résultats. par(mfrow = c(1, 2)) ##supplement supp.means &lt;- tapply(X = croissance$Gain, INDEX = croissance$Supplement, FUN = mean) supp &lt;- data.frame(supp.means, MSE = 1.719) supp$RMSE &lt;- sqrt(supp$MSE) supp$low &lt;- supp$supp.means - supp$RMSE supp$upp &lt;- supp$supp.means + supp$RMSE ##diete diet.means &lt;- tapply(X = croissance$Gain, INDEX = croissance$Diet, FUN = mean) diet &lt;- data.frame(diet.means, MSE = 1.719) diet$RMSE &lt;- sqrt(diet$MSE) diet$low &lt;- diet$diet.means - diet$RMSE diet$upp &lt;- diet$diet.means + diet$RMSE ##créer graphique plot(y = 0, x = 0, ylab = &quot;Gain de masse (kg)&quot;, xlab = &quot;Supplément&quot;, ylim = c(18, max(supp$upp)), xlim = c(0, 5), type = &quot;n&quot;, cex.lab = 1.2, xaxt = &quot;n&quot;, main = &quot;Moyennes ± racine carrée de MSE&quot;) ##ajout de l&#39;axe des x&#39;s axis(side = 1, at = c(1, 2, 3, 4), labels = c(&quot;agri&quot;, &quot;cont&quot;, &quot;sgain&quot;, &quot;ssupp&quot;), cex.axis = 0.85) ##ajout des points pour l&#39;ensemble des cereales points(y = supp$supp.means, x = 1:4, pch = 1) ##barres d&#39;erreurs arrows(x0 = 1:4, y0 = supp$low, y1 = supp$upp, x1 = 1:4, angle = 90, code = 3, length = 0.05) ##groups text(x = 2, y = 18.93, labels = &quot;a&quot;, cex = 1.2) text(x = 3, y = 18.25, labels = &quot;a&quot;, cex = 1.2) text(x = 1, y = 21.61, labels = &quot;b&quot;, cex = 1.2) text(x = 4, y = 20.89, labels = &quot;b&quot;, cex = 1.2) ##créer graphique plot(y = 0, x = 0, ylab = &quot;Gain de masse (kg)&quot;, xlab = &quot;Diète&quot;, ylim = c(16, max(diet$upp)), xlim = c(0, 4), type = &quot;n&quot;, cex.lab = 1.2, xaxt = &quot;n&quot;, main = &quot;Moyennes ± racine carrée de MSE&quot;) ##ajout de l&#39;axe des x&#39;s axis(side = 1, at = c(1, 2, 3), labels = c(&quot;avoine&quot;, &quot;blé&quot;, &quot;orge&quot;), cex = 1.2) ##ajout des points pour blé points(y = diet$diet.means, x = 1:3, pch = 1) ##barres d&#39;erreurs arrows(x0 = 1:3, y0 = diet$low, y1 = diet$upp, x1 = 1:3, angle = 90, code = 3, length = 0.05) ##groups text(x = 1, y = 19.7, labels = &quot;a&quot;, cex = 1.2) text(x = 2, y = 16.8, labels = &quot;b&quot;, cex = 1.2) text(x = 3, y = 22.8, labels = &quot;c&quot;, cex = 1.2) Figure 7.8: Graphique des effets du type de supplément alimentaire ( exttt{Supplement}) et du type de moulée ( exttt{Diete}) sur le gain de masse en kilogrammes. Comme dernier exemple, nous analyserons l’effet de la concentration d’un antibiotique et de l’humidité sur la croissance d’une espèce de bactéries que l’on peut cultiver en laboratoire dans des plats de Pétri contenant un substrat de sucres appelé agar-agar. La variable réponse ici est la surface du substrat couverte par les colonies de bactérie mesurée en mm\\(^2\\). Le fichier antibio.txt contient les données. antibio &lt;- read.table(&quot;Module_7/data/antibio.txt&quot;, header = TRUE) head(antibio) ## Surface Humidite Concentration ## 1 2.103529 sec faible ## 2 2.732448 sec faible ## 3 1.864153 sec faible ## 4 2.358879 sec faible ## 5 2.195207 sec faible ## 6 1.897846 sec moderee ##on détermine le nombre de répétitions table(antibio$Humidite, antibio$Concentration) ## ## elevee faible moderee ## humide 5 5 5 ## sec 5 5 5 On présente les données par un diagramme de boîtes et moustaches (Figure 7.9). boxplot(Surface ~ Humidite + Concentration, data = antibio) Figure 7.9: Diagramme de boîtes et moustaches de la surface couverte en mm\\(^2\\) en fonction de la concentration d’un antibiotique et de l’humidité. On vérifie ensuite les suppositions d’homogénéité des variances et de normalités des résidus. On remarque que celles sont respectées (fig. 7.10). On réalise maintenant l’ANOVA à deux critères : aov.antibio &lt;- aov(Surface ~ Humidite + Concentration + Humidite:Concentration, data = antibio) summary(aov.antibio) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Humidite 1 20.23 20.228 181.6 1.09e-12 *** ## Concentration 2 15.93 7.965 71.5 7.76e-11 *** ## Humidite:Concentration 2 36.40 18.199 163.4 1.05e-14 *** ## Residuals 24 2.67 0.111 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Figure 7.10: Homogénéité des variances et normalité des résidus de l’ANOVA à deux critères réalisées sur les données d’activité bactérienne (surface couverte par les colonies de bactéries). Le tableau d’ANOVA montre une interaction importante entre les facteurs Humidite et Concentration (\\(F_{2, 24} = 163.4, P &lt; 0.0001\\)). Les effets de la concentration d’antibiotique et de l’humidité ne sont pas additifs. Ceci implique que nous ne pouvons pas interpréter ces effets séparément, car l’effet d’un facteur dépend du niveau de l’autre facteur. Nous pouvons utiliser les comparaisons multiples pour trouver les différences entre les groupes définis par les différentes combinaisons des deux facteurs. tapply(X = antibio$Surface, INDEX = list(antibio$Humidite, antibio$Concentration), FUN = mean) ## elevee faible moderee ## humide 1.637384 5.865661 5.036441 ## sec 3.069767 2.250843 2.292084 mult.inter &lt;- TukeyHSD(aov.antibio, which = &quot;Humidite:Concentration&quot;) mult.inter$&#39;Humidite:Concentration&#39; ## diff lwr upr p adj ## sec:elevee-humide:elevee 1.43238312 0.779715574 2.0850507 6.988460e-06 ## humide:faible-humide:elevee 4.22827694 3.575609401 4.8809445 2.331468e-14 ## sec:faible-humide:elevee 0.61345927 -0.039208277 1.2661268 7.400734e-02 ## humide:moderee-humide:elevee 3.39905706 2.746389512 4.0517246 3.608225e-13 ## sec:moderee-humide:elevee 0.65470026 0.002032716 1.3073678 4.897318e-02 ## humide:faible-sec:elevee 2.79589383 2.143226283 3.4485614 2.259004e-11 ## sec:faible-sec:elevee -0.81892385 -1.471591395 -0.1662563 8.269047e-03 ## humide:moderee-sec:elevee 1.96667394 1.314006394 2.6193415 2.716546e-08 ## sec:moderee-sec:elevee -0.77768286 -1.430350402 -0.1250153 1.312775e-02 ## sec:faible-humide:faible -3.61481768 -4.267485222 -2.9621501 1.112443e-13 ## humide:moderee-humide:faible -0.82921989 -1.481887432 -0.1765523 7.359216e-03 ## sec:moderee-humide:faible -3.57357668 -4.226244229 -2.9209091 1.366685e-13 ## humide:moderee-sec:faible 2.78559779 2.132930246 3.4382653 2.443656e-11 ## sec:moderee-sec:faible 0.04124099 -0.611426550 0.6939085 9.999549e-01 ## sec:moderee-humide:moderee -2.74435680 -3.397024340 -2.0916893 3.354361e-11 Les comparaisons multiples suggèrent les groupes suivants : humide-élevée sec-faible sec-modérée sec-élevée humide-modérée humide-faible 1.64 mm2 2.25 mm2 2.29 mm2 3.07 mm2 5.04 mm2 5.87 mm2           Représentons maintenant les résultats graphiquement. ##valeurs prédites pred.out &lt;- expand.grid(Humidite = c(&quot;sec&quot;, &quot;humide&quot;), Concentration = c(&quot;faible&quot;, &quot;moderee&quot;, &quot;elevee&quot;)) preds &lt;- predict(aov.antibio, newdata = pred.out, se.fit = TRUE) pred.out$mean &lt;- preds$fit pred.out$se &lt;- preds$se.fit ##IC à 95% pred.out$low95 &lt;- pred.out$mean + qt(p = 0.025, df = aov.antibio$df.residual) * pred.out$se pred.out$upp95 &lt;- pred.out$mean - qt(p = 0.025, df = aov.antibio$df.residual) * pred.out$se ##graphique plot(y = 0, x = 0, ylab = &quot;&quot;, xlab = &quot;Concentration d&#39;antibiotique&quot;, ylim = c(1, max(pred.out$upp95)), xlim = c(0, 4), type = &quot;n&quot;, cex.lab = 1.2, xaxt = &quot;n&quot;, main = &quot;Moyennes ± IC à 95 %&quot;) ##ajout d&#39;étiquette en marge ##expression permet d&#39;ajouter des lettres grecques, ##indices ou exposants mtext(text = expression(paste(&quot;Surface (&quot;, mm^2, &quot;)&quot;)), side = 2, line = 2, cex = 1.2) ##ajout de l&#39;axe des x&#39;s axis(side = 1, at = c(1, 2, 3), labels = c(&quot;faible&quot;, &quot;modérée&quot;, &quot;élevée&quot;), cex = 1.2) ##points pour sec points(y = pred.out$mean[c(1, 3, 5)], x = 1:3, pch = 1) ##barres d&#39;erreurs arrows(x0 = 1:3, y0 = pred.out$low95[c(1, 3, 5)], y1 = pred.out$upp95[c(1, 3, 5)], x1 = 1:3, angle = 90, code = 3, length = 0.05) ##points pour humide points(y = pred.out$mean[c(2, 4, 6)], x = 1:3, pch = 2) ##barres d&#39;erreurs arrows(x0 = 1:3, y0 = pred.out$low95[c(2, 4, 6)], y1 = pred.out$upp95[c(2, 4, 6)], x1 = 1:3, angle = 90, code = 3, length = 0.05) ##légende legend(x = &quot;topright&quot;, pch = c(1, 2), legend = c(&quot;sec&quot;, &quot;humide&quot;), title = &quot;Humidité&quot;) ##groupes text(x = 1, y = 5.4, labels = &quot;a&quot;, cex = 1.2) text(x = 2, y = 4.6, labels = &quot;b&quot;, cex = 1.2) text(x = 3, y = 1.2, labels = &quot;c&quot;, cex = 1.2) text(x = 1, y = 1.82, labels = &quot;cd&quot;, cex = 1.2) text(x = 2, y = 1.87, labels = &quot;d&quot;, cex = 1.2) text(x = 3, y = 2.6, labels = &quot;e&quot;, cex = 1.2) Figure 7.11: Interaction significative entre l’humidité et la concentration d’antibiotique sur l’activité bactérienne. La Figure 7.11 montre qu’il y a bien une interaction entre les deux facteurs. Nous voyons clairement que la différence entre la moyenne du groupe “sec” et celle du groupe “humide” varie énormément avec la concentration d’antibiotique, et que cette différence est la plus faible quand la concentration d’antibiotique est élevée. On remarque qu’il y a une inversion des différences entre les groupes “sec” et les groupes “humide” : la surface couverte moyenne du groupe “humide” est supérieure à celle du groupe “sec” aux concentrations faible et modérée d’antibitioque. Toutefois, le groupe “sec” a une plus grande surface couverte moyenne que le groupe “humide” à la concentration élevée d’antibiotique. On conclut que l’augmentation de la concentration de l’antibiotique réduit l’activité bactérienne sous des conditions humides, mais augmente l’activité bactérienne en conditions plus sèches. 7.1.6 Conclusion Dans ce texte, vous vous êtes familiarisés avec l’ANOVA à deux critères avec plusieurs répétitions par groupe aux traitements combinés, et particulièrement avec les concepts d’interaction et d’additivité. Trois exemples ont été présentés afin d’illustrer les différentes étapes de l’analyse, de la vérification des suppositions, de l’interprétation et de la présentation graphique des résultats. Nous avons vu qu’en présence d’une interaction entre deux facteurs, on ne peut pas interpréter les effets principaux, car l’effet d’un facteur sur la variable réponse dépend du niveau de l’autre. On dit dans ce cas que les effets ne sont pas additifs. C’est justement ce qu’illustre le dernier exemple sur l’activité bactérienne: l’augmentation de la concentration d’antibiotique n’avait pas le même effet selon le niveau d’humidité du milieu. Le concept d’interaction est important et il est souvent associé à des hypothèses intéressantes en sciences. Nous vous encourageons à revoir les exemples afin de bien saisir la notion d’interaction, car nous l’étudierons à nouveau dans les leçons à venir. Les résultats de l’ANOVA montrent que la probabilité associée au facteur sexe est suffisamment près du seuil de 0.05 pour que l’effet soit considéré biologiquement significatif. Pour cette raison, on peut retenir ce facteur dans les comparaisons multiples.↩︎ À noter que dans notre exemple, il n’y a que deux niveaux (groupes) à chaque facteur: sexe a deux niveaux (mâle, femelle), et traitement hormonal n’a que deux niveaux (avec traitement, sans traitement). Ainsi, il ne serait pas absolument nécessaire de poursuivre avec des comparaisons multiples, car si on rejette \\(H_0\\), on sait que les deux groupes diffèrent entre eux.↩︎ Le fichier csv est un type de fichier de texte standard où chaque élément est séparé le plus souvent par une virgule, un espace ou une point-virgule. Ce format s’importe dans R à l’aide de read.table( ) puisque c’est un fichier de texte, en modifiant la valeur de l’argument delim.↩︎ "],["laboratoire-1.html", "7.2 Laboratoire", " 7.2 Laboratoire 7.2.1 Question 1 On effectue une expérience pour déterminer si la longueur en cm chez une espèce de poisson varie selon le sexe ou le pH de l’eau. Importez le fichier Longueur_poisson.xls pour voir les données25. a. Déterminez le nombre de répétitions (replicates) pour chaque combinaison des deux facteurs. Veuillez interpréter les résultats obtenus avec RStudio ou R Commander. b. Réalisez une ANOVA à deux critères (fournir le tableau de l’ANOVA) et effectuez tous les diagnostics permettant d’évaluer la pertinence de votre analyse. Que pouvez-vous conclure? c. Lorsqu’il est approprié de le faire, utilisez un test de Tukey (fournir l’output) afin d’identifier les différences entre les moyennes. Que pouvez-vous conclure? d. Illustrez vos conclusions de l’étude à l’aide d’un graphique. e. Les effets sont-ils additifs? Si MS Excel n’est pas installé sur votre ordinateur, il est possible d’importer le fichier à l’aide des suites bureautiques libres OpenOffice http://www.openoffice.org/fr/Produits/ ou LibreOffice http://fr.libreoffice.org/.↩︎ "],["blocs-complets-aléatoires.html", "Module 8 Blocs complets aléatoires", " Module 8 Blocs complets aléatoires Dans la leçon précédente, nous avons présenté l’ANOVA à deux critères avec répétitions. Nous avons vue en détail les concepts d’interaction et d’additivité. Rappelons qu’en présence d’une interaction entre deux facteurs, on ne peut pas interpréter les effets principaux séparément. En effet, une interaction implique que l’effet d’un facteur sur la variable réponse dépend du niveau de l’autre. On dit dans ce cas que les effets ne sont pas additifs. "],["théories-7.html", "8.1 Théories", " 8.1 Théories 8.1.1 Effets fixes et effets aléatoires Dans les leçons précédentes, nous avons vu l’ANOVA à un critère et l’ANOVA à deux critères. Dans tous les exemples que nous avons présentés, les effets étudiés étaient fixés par l’expérimentateur. Par exemple, on s’intéressait à l’effet du sexe ou d’un traitement hormonal sur la concentration en calcium dans le plasma sanguin . Lorsque les niveaux du facteur sont spécifiquement choisis par l’expérimentateur, nous dirons que c’est un effet fixe. Ce genre d’effet est le plus commun. Dans une expérience sur l’effet de la zoothérapie sur des personnes atteintes d’un stress chronique, on sélectionne aléatoirement 30 personnes atteintes. L’expérimentateur s’intéresse à la différence entre les traitements suivants : thérapie accompagnée d’un chat, thérapie accompagnée d’un chien et thérapie conventionnelle. On attribue aléatoirement l’un des traitements à chaque personne atteinte, de façon à avoir 10 répétitions pour chaque traitement. Puisque l’expérimentateur s’intéresse spécifiquement à ces trois traitements, l’effet de la zoothérapie est un effet fixe. En contrepartie, lorsque les niveaux d’un facteur sont déterminés aléatoirement, nous dirons que c’est un effet aléatoire. Autrement dit, l’expérimentateur choisit aléatoirement les niveaux qui seront inclus dans l’expérience parmi tous les niveaux disponibles. L’effet aléatoire permet d’étudier l’effet global d’un facteur sur la variable réponse en considérant tous les traitements possibles plutôt que les différences entre des traitements spécifiques. On veut connaître s’il y a un effet du fournisseur de batteries de téléphone portable sur leur durée de vie. Dans un catalogue de fournisseurs de batteries, on sélectionne aléatoirement 3 fournisseurs de batteries. On commande 10 batteries de chacun des 3 fournisseurs. Puisque les trois niveaux du facteur fournisseur de batteries ont été sélectionnés aléatoirement, nous dirons que c’est un effet aléatoire. La différence entre l’effet fixe et l’effet aléatoire est au niveau de l’interprétation des résultats. Avec un effet fixe, on s’intéresse à la différence entre les niveaux spécifiques du facteur et notre conclusion se rapporte directement à ces niveaux. Pour l’effet aléatoire, la conclusion est pertinente à l’effet global du facteur et à tous les niveaux possibles du facteur. Dans le dernier cas, il peut exister une multitude de niveaux qui n’ont pas nécessairement été inclus dans l’analyse. L’exemple suivant distingue les deux types d’effets. On veut déterminer la variabilité géographique de la concentration en cadmium dans le sol au Québec. Si l’expérimentateur identifie différentes régions du Québec et qu’il sélectionne aléatoirement trois régions, on considérera l’effet comme aléatoire. À l’opposé, si l’expérimentateur s’intéresse spécifiquement aux régions de l’Abitibi, du Lac St-Jean et de la Côte-Nord, on considérera l’effet comme fixe. 8.1.1.1 ANOVA à un critère Lorsqu’on effectue une ANOVA à un critère, le calcul de la somme des carrés, des carrés moyens et du \\(F\\) demeure le même, peu importe que l’effet soit fixe ou aléatoire. Seule l’interprétation va changer. 8.1.1.2 ANOVA à deux critères En ce qui concerne l’ANOVA à deux critères (A et B) avec répétition, c’est-à-dire avec plus d’une observation, on distingue trois scénarios d’effets fixes et aléatoires possibles : Modèle I: A fixe et B fixe Modèle II: A aléatoire et B aléatoire Modèle III: A aléatoire et B fixe26 Le type d’effet (fixe vs aléatoire) détermine le terme d’erreur à utiliser dans le tableau d’ANOVA. Le tableau 8.1 présente la formulation des différents ratios des carrés moyens pour tester les effets de A, de B et de leur interaction selon la nature de ces effets. Table 8.1: Termes d’erreurs utilisés pour tester les effets de A, de B et de leur interaction selon la nature des effets (fixes ou aléatoires) en présence de répétitions. Facteur Modèle I (A fixe, B fixe) Modèle II (A aléatoire, B aléatoire) Modèle III (A aléatoire, B fixe) A \\(\\frac{MS \\: \\mathrm{A}}{MS \\: \\mathrm{erreur}}\\) \\(\\frac{MS \\: \\mathrm{A}}{MS \\: \\mathrm{A \\: \\times \\: B}}\\) \\(\\frac{MS \\: \\mathrm{A}}{MS \\: \\mathrm{erreur}}\\) B \\(\\frac{MS \\: \\mathrm{B}}{MS \\: \\mathrm{erreur}}\\) \\(\\frac{MS \\: \\mathrm{B}}{MS \\: \\mathrm{A \\: \\times \\: B}}\\) \\(\\frac{MS \\: \\mathrm{B}}{MS \\: \\mathrm{A \\: \\times \\: B}}\\) A x B \\(\\frac{MS \\: \\mathrm{A \\: \\times \\: B}}{MS \\: \\mathrm{erreur}}\\) \\(\\frac{MS \\: \\mathrm{A \\: \\times \\: B}}{MS \\: \\mathrm{erreur}}\\) \\(\\frac{MS \\: \\mathrm{A \\: \\times \\: B}}{MS \\: \\mathrm{erreur}}\\) Le modèle III est aussi appelé modèle mixte puisqu’il contient des effets fixes et des effets aléatoires. Les modèles mixtes englobent une variété d’approches qui tiennent compte de la structure de données complexes, telles que celles provenant d’un dispositif dit niché ou emboité (p.ex., sondages auprès d’individus emboîtés dans des quartiers, ces derniers emboîtés eux-mêmes dans différentes villes) ou encore avec des mesures répétées dans le temps. Le dispositif expérimental le plus simple du modèle III est le dispositif en blocs complets aléatoires et c’est celui que nous verrons en détail dans la prochaine section. 8.1.2 Blocs complets aléatoires Le dispositif en blocs complets aléatoires consiste en deux facteurs, l’un fixe, l’autre aléatoire, mais sans répétition. Avant d’aller plus loin, revisitons les dispositifs expérimentaux menant à certaines des analyses que nous avons vues dans les leçons précédentes. 8.1.2.1 Révision de l’échantillonnage Certains dispositifs expérimentaux permettent d’échantillonner les données de façon complètement aléatoire. Par exemple, lorsqu’on applique le test \\(t\\) à un échantillon, on sélectionne les unités expérimentales aléatoirement parmi la population et on compare la moyenne à celle de l’hypothèse nulle (Figure 8.1). Figure 8.1: Sélection aléatoire des unités expérimentales parmi la population pour le test \\(t\\) à un échantillon (\\(n = 12\\)). Pour ce qui est du test \\(t\\) sur deux échantillons indépendants, on sélectionne les unités expérimentales et on attribue aléatoirement l’un des deux traitements aux unités (Figure 8.2). Dans l’ANOVA à un critère, les unités expérimentales sont sélectionnées aléatoirement et on attribue les traitements de façon complètement aléatoire (Figure 8.3). Figure 8.2: Sélection aléatoire des unités expérimentales et attribution aléatoire des unités parmi la population pour le test \\(t\\) à deux groupes. À noter que les deux couleurs distinguent les deux traitements appliqués aux unités expérimentales (\\(n = 8\\) répétitions par traitement).} Figure 8.3: Sélection aléatoire des unités expérimentales et assignation aléatoire de trois traitements dans un dispositif complètement aléatoire. À noter que chaque couleur correspond à un traitement appliqué à l’unité expérimentale (\\(n = 8\\) répétitions par traitement). D’autres dispositifs expérimentaux imposent une structure aux données: certaines unités expérimentales ne sont pas indépendantes. Par exemple, le test \\(t\\) pour données appariées traite les données par paire. On sélectionne aléatoirement les paires et on attribue aléatoirement l’un des traitements à chaque élément de la paire (Figure 8.4). Ce dispositif minimise la variabilité intrapaire, mais permet une grande variabilité entre paires. Cette stratégie d’échantillonnage est avantageuse puisque, dans certains cas, il est logistiquement plus simple d’échantillonner des paires d’unités expérimentales que des unités expérimentales indépendantes. Par exemple, le dispositif apparié présenté à la figure (8.4) nécessite huit paires d’unités expérimentales. En contrepartie, si on utilise un dispositif complètement aléatoire avec deux groupes comme pour le test \\(t\\) à deux groupes indépendants, on aura recours à 16 unités expérimentales indépendantes (Figure 8.2). Figure 8.4: Sélection aléatoire des paires d’unités expérimentales et assignation aléatoire de l’un des deux traitements. À noter que chaque couleur correspond à un des deux traitements appliqués à chaque unité expérimentale d’une paire (\\(n = 8\\) paires). L’ANOVA en blocs complets aléatoires est l’extension du test \\(t\\) pour données appariées lorsqu’on a plus de deux traitements. Le bloc est l’analogue à la paire dans le test \\(t\\): alors que la paire était constituée de deux observations liées entre elles, le bloc est constitué de plus de deux observations reliées entre elles (Figure 8.5). Tout comme le test \\(t\\) pour données appariées, le bloc minimise la variabilité à l’intérieur d’un bloc, mais la variabilité entre blocs est grande. En fait, l’ANOVA en blocs complets aléatoires effectuée sur des paires est identique au test \\(t\\) pour données appariées. Figure 8.5: Sélection aléatoire des blocs et assignation aléatoire de l’un des trois traitements aux unités expérimentales. À noter que chaque couleur correspond à l’un des trois traitements appliqués à chaque unité expérimentale d’un bloc (\\(n = 8\\) blocs). Dans la figure 8.5, l’ANOVA en blocs complets aléatoires utilise huit blocs pour tester l’effet de trois traitements, alors que l’ANOVA à un critère classique aurait nécessité 24 unités expérimentales pour tester ces mêmes effets (Figure 8.3). Étant donné ses avantages logistiques (p.ex., coût en temps et en argent), le dispositif en blocs complets aléatoires est beaucoup utilisé en sciences. 8.1.2.2 Origine du dispositif en blocs complets aléatoires Le dispositif en blocs complets aléatoires a été développé en agronomie pour les cas où un gradient est présent sur les parcelles étudiées. On entend par gradient, un ensemble de conditions telles que l’humidité, la température, le pH ou l’exposition au soleil, qui varient sur un site d’étude ou dans un laboratoire. Les blocs sont disposés perpendiculairement au gradient (Figure 8.6), de façon à ce que les conditions à l’intérieur d’un bloc soient homogènes. Figure 8.6: Les blocs sont perpendiculaires au gradient (ici, le gradient va de gauche à droite). Cinq blocs ont été utilisés pour comparer trois traitements. 8.1.2.3 Particularité du dispositif en blocs complets aléatoires Comme il est illustré aux figures 8.5 et 8.6, chaque bloc reçoit une seule répétition de chaque traitement, ce qui explique le terme “bloc complet” (chaque bloc reçoit tous les traitements). Les traitements sont assignés aléatoirement à chaque unité dans le bloc. Cette approche s’applique également en laboratoire lorsque l’espace est limité. Par exemple, on pourrait considérer des essais dans une chambre de croissance27 comme des blocs, où chaque essai consisterait à placer un plat de Petri pour chacun des traitements d’intérêt dans la chambre de croissance pendant une période donnée. Nous pourrions répéter la procédure plusieurs fois pour obtenir plusieurs essais. Comme autre exemple, on pourrait penser à des aquariums divisés en trois sections avec les mêmes densités de têtards de grenouilles, mais où chaque section recevrait différents prédateurs, afin d’estimer la survie des têtards. Ici, les aquariums représenteraient les blocs, puisque chaque aquarium donnerait trois observations (le nombre de survivants), soit une observation dans chaque section. Un dernier exemple serait une compagnie de raffinerie de pétrole qui s’intéresse à la performance en termes d’économie d’essence de 4 types d’essence qu’ils ont développés. Elle pourrait tester les 4 types d’essence sur 8 modèles de voitures sélectionnées aléatoirement. Dans tous ces exemples, il y donc toujours un facteur dont l’effet nous intéresse (effet des prédateurs sur les têtards, effet des types d’essence sur l’économie) et un facteur qui ne nous intéresse pas ou peu (effet des différents aquariums, effet des différents modèles de voitures). Le premier facteur représente l’effet fixe et le second représente l’effet aléatoire. L’ANOVA en blocs complets aléatoires est en fait une ANOVA à deux critères sans répétitions: il n’y a qu’une observation pour chaque combinaison de traitement et de bloc. Le facteur d’intérêt est un facteur fixe, alors que le bloc peut être considéré comme un facteur aléatoire. On suppose que les blocs sont différents entre eux. Afin d’optimiser ce dispositif, il faut s’assurer que les conditions soient homogènes à l’intérieur d’un bloc donné, et que les blocs diffèrent entre eux. Ceci a pour effet de minimiser la variabilité à l’intérieur d’un bloc et de maximiser la variabilité entre les blocs (\\(\\sigma^2_{\\mathrm{intra-bloc}} &lt; \\sigma^2_{\\mathrm{inter-bloc}}\\)). Puiqu’il n’y aucune répétition à l’intérieur d’un même bloc (p.ex., une observation par traitement), il est impossible de tester le terme d’interaction bloc \\(\\times\\) traitement. À l’instar de l’ANOVA à deux critères, le rejet de l’hypothèse nulle pour le facteur d’intérêt peut être suivi de comparaisons multiples pour déterminer où se trouvent les différences. Toutefois, si le facteur bloc explique une bonne partie de la variance et l’hypothèse nulle est rejetée pour le bloc (c.-à-d., \\(P &lt; \\alpha\\)), nous ne testons pas les différences entre blocs. Le facteur bloc n’est pas directement d’intérêt et il reflète simplement le dispositif qui a été utilisé. Le prochain exemple illustre l’application de l’ANOVA à ce dispositif expérimental. On veut déterminer l’effet de quatre diètes différentes sur le gain de masse en grammes de cochons d’Inde. Chaque individu est placé dans une cage individuelle située dans un grand entrepôt. Étant donné qu’il n’y a qu’une source de chaleur à une extrémité de l’entrepôt, on pourrait s’attendre à avoir un gradient de température. On dispose les cages en blocs complets aléatoires de façon à avoir des groupes de quatre cages à différentes distances de la source de chaleur. On trouve ces données dans le fichier cochons.txt. ##importation cochons &lt;- read.table(&quot;Module_8/data/cochons.txt&quot;, header = TRUE) ##premières observations head(cochons) ## Bloc Diete Masse ## 1 1 1 1.5 ## 2 1 2 2.7 ## 3 1 3 2.1 ## 4 1 4 1.3 ## 5 2 1 1.4 ## 6 2 2 2.9 ##structure du jeu de données str(cochons) ## &#39;data.frame&#39;: 20 obs. of 3 variables: ## $ Bloc : int 1 1 1 1 2 2 2 2 3 3 ... ## $ Diete: int 1 2 3 4 1 2 3 4 1 2 ... ## $ Masse: num 1.5 2.7 2.1 1.3 1.4 2.9 2.2 1 1.4 2.1 ... Dans notre expérience, les deux facteurs sont Bloc et Diete. Toutefois, R considère ces deux variables comme numérique (des entiers). À chaque fois qu’un facteur est codé avec des valeurs numériques, comme ici 1, 2, 3, 4, il faut recoder soi-même la variable en facteur. ##convertir en facteur cochons$Bloc &lt;- as.factor(cochons$Bloc) cochons$Diete &lt;- as.factor(cochons$Diete) ##reconnus maintenant comme facteurs str(cochons) ## &#39;data.frame&#39;: 20 obs. of 3 variables: ## $ Bloc : Factor w/ 5 levels &quot;1&quot;,&quot;2&quot;,&quot;3&quot;,&quot;4&quot;,..: 1 1 1 1 2 2 2 2 3 3 ... ## $ Diete: Factor w/ 4 levels &quot;1&quot;,&quot;2&quot;,&quot;3&quot;,&quot;4&quot;: 1 2 3 4 1 2 3 4 1 2 ... ## $ Masse: num 1.5 2.7 2.1 1.3 1.4 2.9 2.2 1 1.4 2.1 ... On peut visualiser les données à l’aide d’un diagramme de boîtes et moustaches (Figure 8.7). boxplot(Masse ~ Diete, data = cochons, xlab = &quot;Diète&quot;, ylab = &quot;Gain de massse (g)&quot;) Figure 8.7: Diagramme de boîtes et moustaches à partir des données de gain de masse de cochons d’Inde. Pour réaliser l’ANOVA à deux critères sans répétition, on utilise encore une fois aov( ) : ##ANOVA à deux critères sans répétition bloc.aov &lt;- aov(Masse ~ Diete + Bloc, data = cochons) Avant d’interpréter les résultats de l’ANOVA, nous devons vérifier les suppositions habituelles. ##vérification des suppositions par(mfrow = c(2, 2)) plot(bloc.aov) Figure 8.8: Vérifications des suppositions d’homogénéité des variances et de la normalité des résidus. On remarque que les variances sont légèrement hétérogènes (Figure 8.8). Essayons une transformation logarithmique afin d’homogénéiser les variances : ##ANOVA à deux critères sans répétition ##en utilisant le logarithme de la Masse bloc.aov.log &lt;- aov(log(Masse) ~ Diete + Bloc, data = cochons) ##vérification des suppositions par(mfrow = c(2, 2)) plot(bloc.aov.log) Figure 8.9: Vérifications des suppositions d’homogénéité des variances et de la normalité des résidus avec les données log-transformées. On observe que la transformation logarithmique a permis de mieux respecter les suppositions (Figure 8.9). Comparons maintenant l’ANOVA en blocs complets aléatoires et l’ANOVA à un critère avec ces mêmes données. ##ANOVA à 1 critère aov1.log &lt;- aov(log(Masse) ~ Diete, data = cochons) ##comparaisons ANOVA à 1 critère vs blocs summary(aov1.log) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Diete 3 2.2515 0.7505 43.12 6.83e-08 *** ## Residuals 16 0.2784 0.0174 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 summary(bloc.aov.log) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Diete 3 2.2515 0.7505 49.363 5.03e-07 *** ## Bloc 4 0.0960 0.0240 1.579 0.243 ## Residuals 12 0.1824 0.0152 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 On remarque que la somme des carrés du facteur Diete est le même pour les deux analyses (2.251). La différence entre l’ANOVA à un critère et l’ANOVA en blocs complets aléatoires est au niveau du terme d’erreur. Dans l’ANOVA en blocs complets aléatoires, la somme des carrés des erreurs (\\(SSE\\)) est de 0.182, alors que celle de l’ANOVA à un critère est de 0.278. On constate que la somme des carrés du terme Bloc est de 0.096 et que cette valeur ajoutée à la somme de carrés des erreurs de l’ANOVA en blocs complets aléatoires donne un total de 0.278 (\\(0.096 + 0.182 = 0.278\\)). Le terme Bloc explique une partie de la variabilité de la variable réponse (Masse) et réduit le terme d’erreur. Ceci a pour conséquence de réduire le carré moyen de l’erreur (\\(MSE\\)). Puisque le \\(F\\) est le ratio entre le carré moyen de Diete et le carré moyen de l’erreur, la valeur du \\(F\\) est plus grande en présence du bloc. On constate que la diète influence fortement le gain de masse (\\(F_{3, 12} = 49.36\\), ) et que les blocs ne diffèrent pas entre eux (\\(F_{4, 12} = 1.58, P = 0.24\\)). On peut procéder à des comparaisons multiples afin d’identifier les groupes définis par la diète. ##moyennes des groupes tapply(X = cochons$Masse, INDEX = cochons$Diete, FUN = mean) ## 1 2 3 4 ## 1.38 2.80 2.24 1.24 ##comparaisons multiples TukeyHSD(x = bloc.aov.log, which = &quot;Diete&quot;) ## Tukey multiple comparisons of means ## 95% family-wise confidence level ## ## Fit: aov(formula = log(Masse) ~ Diete + Bloc, data = cochons) ## ## $Diete ## diff lwr upr p adj ## 2-1 0.6990462 0.4675197 0.93057281 0.0000060 ## 3-1 0.4836196 0.2520930 0.71514616 0.0002317 ## 4-1 -0.1143399 -0.3458665 0.11718666 0.4856391 ## 3-2 -0.2154267 -0.4469532 0.01609992 0.0712144 ## 4-2 -0.8133862 -1.0449127 -0.58185959 0.0000012 ## 4-3 -0.5979595 -0.8294861 -0.36643294 0.0000299 Les comparaisons multiples identifient deux groupes, le premier est constitué des diètes 4 et 1, et le deuxième est composé des diètes 2 et 3 : 4 1 2 3 1.242 1.382 2.242 2.802             On peut présenter les résultats graphiquement. ##Définissons l&#39;axe des x de la figure, c&#39;est à dire les diètes diete &lt;- 1:4 ##Puis, calculons les moyennes sur les donnees transformees means.log &lt;- tapply(X = log(cochons$Masse), INDEX = cochons$Diete, FUN = mean) ##Il nous faut maintenant trouver le MSE. ##Pour se faire, nous devons nous référer au résumé de l&#39;ANOVA summary(bloc.aov.log) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Diete 3 2.2515 0.7505 49.363 5.03e-07 *** ## Bloc 4 0.0960 0.0240 1.579 0.243 ## Residuals 12 0.1824 0.0152 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ##Le MSE est la valeur données à la ligne Residuals de la colonne Mean Sq MSE &lt;- 0.0152 ##On peut aussi le trouver de cette façon: S&lt;-summary(bloc.aov.log) MSE&lt;-S[[1]][[3]][[3]] ##Calculons maintenant les barres d&#39;erreur low.log &lt;- means.log - sqrt(MSE) upp.log &lt;- means.log + sqrt(MSE) ##On affiche les moyennes plot(means.log ~ diete, ylab = &quot;Log du gain de masse (g)&quot;, xlab = &quot;Diète&quot;, type = &quot;p&quot;, ylim = c(min(low.log), max(upp.log)), xlim = c(0, 5), cex.lab = 1.2, xaxt = &quot;n&quot;, main = &quot;Log des Moyennes ± racine carrée de MSE&quot;) ##Ajoutons l&#39;axe des x axis(side = 1, at = c(1, 2, 3, 4)) ##Ajoutons les barres d&#39;erreur arrows(x0 = diete, x1 = diete, y0 = low.log, y1 = upp.log, angle = 90, code = 3, length = 0.05) ##Ajoutons les lettres qui designent les groupes text(x = 1, y = 0.18, labels = &quot;a&quot;, cex = 1.2) text(x = 4, y = 0.07, labels = &quot;a&quot;, cex = 1.2) text(x = 2, y = 0.88, labels = &quot;b&quot;, cex = 1.2) text(x = 3, y = 0.66, labels = &quot;b&quot;, cex = 1.2) Figure 8.10: Effet de la diète sur le logarithme du gain de masse. Les lettres différentes indiquent les groupes. Pour faciliter l’interprétation de notre analyse représentons les données originales plutôt que les données transformées. ##Calculons les moyennes sur les donnees originales means &lt;- tapply(X = cochons$Masse, INDEX = cochons$Diete, FUN = mean) ##Les barres d&#39;erreurs sur les donnees originales se determinent en prenant la fonction exp() ## sur les barres d&#39;erreurs des donnees transformees low &lt;- exp(low.log) upp &lt;- exp(upp.log) ##On cree la figure plot(means ~ diete, ylab = &quot;Gain de masse (g)&quot;, xlab = &quot;Diète&quot;, type = &quot;p&quot;, ylim = c((min(low)-0.2), max(upp)), xlim = c(0, 5), cex.lab = 1.2, xaxt = &quot;n&quot;, main = &quot;Moyennes ± racine carrée de MSE&quot;) ##Ajoutons l&#39;axe des x axis(side = 1, at = c(1, 2, 3, 4)) ##Ajoutons les barres d&#39;erreur arrows(x0 = diete, x1 = diete, y0 = low, y1 = upp, angle = 90, code = 3, length = 0.05) ##Ajoutons les lettres qui designent les groupes text(x = 1, y = 1.19, labels = &quot;a&quot;, cex = 1.2) text(x = 4, y = 1.0, labels = &quot;a&quot;, cex = 1.2) text(x = 2, y = 2.4, labels = &quot;b&quot;, cex = 1.2) text(x = 3, y = 1.9, labels = &quot;b&quot;, cex = 1.2) Figure 8.11: Effet de la diète sur le gain de masse. Les lettres différentes indiquent les groupes.} Sur la figure 8.11, on constate la présence de deux groupes de diètes. Le premier groupe est composé des diètes 4 et 1, et le deuxième est constitué des groupes 3 et 2. De plus, on peut conclure que les diètes 4 et 1 augmentent moins le gain de masse que les diètes 3 et 2. 8.1.2.4 Applications moins conventionnelles Typiquement en agronomie ou en écologie forestière, les blocs sont disposés dans l’espace et perpendiculaires à un gradient. Toutefois, on peut aussi disposer les blocs dans le temps. Par exemple, considérons une expérience de germination de graines en serre (croissance en une semaine), où l’on ne peut pas répéter tous les traitements d’intérêt dû à un manque d’espace. Pour pallier le problème, on peut créer des blocs temporels de façon à avoir une répétition de chaque traitement et à effectuer un bloc par semaine. Dans d’autres cas, on peut avoir plusieurs mesures sur un même individu. L’individu constituera le bloc. Pour illustrer ce concept, imaginons une expérience sur la production de lait par des vaches avec différents types de fourrage (p.ex., luzerne, trèfle, blé, maïs). Si on soupçonne que la production de lait varie beaucoup entre les vaches, on pourrait traiter chaque vache comme un bloc. Chaque vache recevrait chacun des traitements, dans une séquence aléatoire et avec suffisamment de temps entre les traitements pour éviter de contaminer les réponses des vaches par l’effet du fourrage précédent. De cette façon, on peut mieux contrôler les différences individuelles en comparant la réponse d’un même individu à différents traitements. Un autre exemple serait de suivre les résultats d’élèves à différentes périodes lors desquelles ils auraient utilisé différentes méthodes d’apprentissage. Chaque élève représenterait un bloc complet et utiliserait toutes les méthodes à travers le temps dans un ordre aléatoire. De cette façon, on peut contrôler pour la grande variabilité inter-élève et se concentrer sur l’effet des méthodes d’apprentissage. 8.1.2.5 Conditions d’application En plus des conditions nécessaires à l’ANOVA classique (homogénéité des variances, normalité des résidus), il faut respecter les conditions suivantes : chaque bloc doit être suffisamment petit pour assurer des conditions homogènes à l’intérieur du bloc; chaque bloc doit être suffisamment grand pour accommoder une répétition de chaque traitement: il doit y avoir assez de place pour que les répétitions de chaque traitement soient espacées afin d’être indépendantes. En d’autres mots, l’observation provenant d’un traitement dans un bloc ne doit pas être influencée par un traitement adjacent du même bloc (p.ex., l’application de différentes concentrations d’engrais à des parcelles d’un bloc et le ruissellement contaminant le traitement d’une parcelle adjacent du même bloc); les blocs doivent être suffisamment espacés afin d’être indépendants les uns des autres; il ne doit y avoir aucune interaction entre les blocs et les traitements (effets additifs seulement): l’effet d’un traitement doit être constant pour tous les blocs. 8.1.2.6 Avantages du dispositif en blocs complets aléatoires Le dispositif en blocs complets aléatoires comporte plusieurs avantages, notamment : il contrôle l’hétérogénéité environnementale (tient compte des gradients) qui autrement ferait augmenter le carré moyen des erreurs (\\(MSE\\)) et rendrait plus difficile de rejeter l’hypothèse nulle concernant le facteur d’intérêt; en présence de gradients, l’ANOVA en blocs complets aléatoires est plus puissante que l’ANOVA complètement aléatoire; il est logistiquement intéressant lorsque la répétition est contrainte dans l’espace ou le temps; il peut être utilisé avec un design apparié (p.ex., &gt; 2 observations par bloc) pour tenir compte de facteurs additionnels tels que la variabilité entre les individus d’une même famille. 8.1.2.7 Désavantages du dispositif en blocs complets aléatoires Le dispositif en blocs complets aléatoires comporte certains désavantages, notamment : il y a un coût statistique à utiliser les blocs. Si l’effectif (\\(n\\)) est petit et si l’effet du bloc est faible, l’ANOVA à un critère est plus puissante dans ce cas. avec des blocs trop petits, on risque d’introduire une dépendance des observations soumis aux différents traitements d’un même bloc, si les traitements sont physiquement près les uns des autres; il faut un dispositif complètement balancé: si on a au moins une donnée manquante dans un bloc, on doit se débarrasser du bloc ou estimer les valeurs des données manquantes; s’il y a une interaction entre les blocs et les traitements (effets non-additifs), on a un problème puisqu’on ne peut tester l’interaction en l’absence de répétitions. Si on soupçonne un potentiel effet interactif, il faut prévoir récolter plus d’une observation d’un même traitement dans chaque bloc pour le tester formellement. Le cas échéant, on utilisera une analyse plus complexe, telle que l’ANOVA en tiroirs (split-plot)28. Une autre option est de mesurer directement la variable qui est responsable du gradient (p.ex., humidité, ensoleillement, position dans la pente). 8.1.3 Conclusion Dans cette leçon, nous avons revisité différents types de dispositifs associés aux analyses vues jusqu’à présent dans le cours. Nous avons présenté les concepts d’effets fixes et d’effets aléatoires ainsi que de modèle mixte. Nous avons présenté l’ANOVA en blocs complets aléatoires comme étant le cas le plus simple de modèle mixte et comme l’extension du test \\(t\\) pour les données appariées. Le dispositif en blocs complets aléatoires se caractérise par la disposition de blocs perpendiculaires à un gradient. Ce gradient n’est pas mesuré directement, mais les blocs sont une mesure indirecte de la variabilité associée à ce gradient. Ce dispositif comporte des avantages importants, notamment sur le plan logistique comparé aux dispositifs complètement aléatoires (p.ex., ANOVA à un critère avec répétitions, ANOVA à deux critères avec répétitions). Bien qu’à l’origine le dispositif en blocs complets aléatoires ait été développé en agronomie, il est maintenant utilisé dans des disciplines variées incluant les sciences environnementales, la médecine, la chimie et le marketing. La prochaine leçon traitera de façon plus générale de l’élaboration de dispositifs expérimentaux et de recommandations afin d’arriver à des conclusions solides. Le cas opposé, avec A fixe et B aléatoire, est bien sûr équivalent.↩︎ Une chambre de croissance est un contenant isotherme dans lequel les conditions sont contrôlées afin d’optimiser la croissance de plantes ou de cultures.↩︎ Ce dispositif plus complexe ne sera pas vu dans le cours.↩︎ "],["exercices-6.html", "8.2 Exercices", " 8.2 Exercices 8.2.1 Question 1 Donnez un avantage et un inconvénient associés à l’utilisation du dispositif en blocs complets aléatoires. Réponse Le dispositif en blocs complets aléatoires comporte des avantages tels que : il contrôle l’hétérogénéité environnementale (différences entre blocs); l’ANOVA en blocs complets aléatoires est plus puissante que l’ANOVA complètement aléatoire en présence de gradients; il est logistiquement intéressant lorsque la répétition est contrainte dans l’espace ou le temps; le bloc peut être un individu soumis à différents traitements ou des individus provenant d’une même portée (famille). Les désavantages sont les suivants : l’ANOVA en blocs complets aléatoires est moins puissante que l’ANOVA à un critère lorsque l’effectif (\\(n\\)) est petit et que l’effet du bloc est faible; la dépendance potentielle des observations soumises aux différents traitements d’un même bloc dans des blocs trop petits; il ne permet pas de données manquantes; il ne permet pas de tester l’interaction entre le bloc et le facteur d’intérêt. 8.2.2 Question 2 Voici le jeu de données lavage.txt d’un design en blocs complets aléatoires destiné à évaluer la blancheur des vêtements (indice de 0 à 10) après avoir laver ces vêtements avec trois détergents différents (le conventionnel pour le contrôle et deux nouveaux types) ; et après qu’on ait appliqué sur ces vêtements des produits plus ou moins tachants (pouvoir tachant non-mesuré). Chaque bloc sera donc un produit tachant choisi au hasard (p. ex., boue, vin, ketchup) sur lequel on utilisera les trois détergents. a. Effectuez une ANOVA en blocs complets aléatoires. Si vous rejetez \\(H_o\\) pour le facteur détergent, déterminez où se trouvent les différences. Interprétez les résultats et présentez les résultats sous forme graphique. Réponse On importe le jeu de données : ##importation lavage &lt;- read.table(&quot;Module_8/data/lavage.txt&quot;, header = TRUE) ##premières observations head(lavage) ## Blancheur Detergent Bloc ## 1 4.59 Controle 1 ## 2 8.15 Type1 1 ## 3 8.06 Type2 1 ## 4 7.38 Type1 2 ## 5 4.88 Controle 2 ## 6 5.85 Type2 2 ##structure du jeu de données str(lavage) ## &#39;data.frame&#39;: 15 obs. of 3 variables: ## $ Blancheur: num 4.59 8.15 8.06 7.38 4.88 5.85 5.89 3.29 2.74 5.46 ... ## $ Detergent: chr &quot;Controle&quot; &quot;Type1&quot; &quot;Type2&quot; &quot;Type1&quot; ... ## $ Bloc : int 1 1 1 2 2 2 3 3 3 4 ... ##niveau du facteur bloc levels(lavage$Bloc) ## NULL On remarque que la variable n’est pas reconnue comme facteur et on doit la convertir en facteur avant de continuer : lavage$Bloc &lt;- as.factor(lavage$Bloc) ##maintenant reconnu comme facteur levels(lavage$Bloc) ## [1] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; &quot;5&quot; On note que le dispositif est complètement balancé (aucune donnée manquante) : ##nombre d&#39;observations par combinaison de facteurs tapply(X = lavage$Blancheur, INDEX = list(lavage$Detergent, lavage$Bloc), FUN = length) ## 1 2 3 4 5 ## Controle 1 1 1 1 1 ## Type1 1 1 1 1 1 ## Type2 1 1 1 1 1 Le diagramme de boîtes et moustaches montre les données brutes (Figure 8.12). boxplot(Blancheur ~ Detergent, data = lavage, ylab = &quot;Indice de blancheur (0 à 10)&quot;, xlab = &quot;Détergent&quot;) Figure 8.12: Diagramme de boîtes et moustaches présentant les données de la blancheur de vêtements lavés à l’aide d’un détergent conventionnel (Controle) et de deux nouveaux détergent (Type1 et Type2). On exécute l’ANOVA en blocs complets aléatoires : ##ANOVA à deux critères sans répétitions aov.bloc &lt;- aov(Blancheur ~ Detergent + Bloc, data = lavage) On fait la vérification des suppositions d’homogénéité de la variance et de la normalité des résidus (Figure 8.13): ##préparation de la fenêtre graphique ##pour accomoder 4 graphiques sur la même page par(mfrow = c(2, 2)) ##présentation de quatre graphiques diagnostiques plot(aov.bloc) Figure 8.13: Diagnostics des suppositions d’homogénéité de la variance et de normalité des résidus à la suite de l’ANOVA en blocs complets aléatoires. Puisque les suppositions de l’ANOVA en blocs complets aléatoires sont respectées (fig. 8.13), on peut procéder à l’interprétation des résultats. Le tableau d’ANOVA nous indique : summary(aov.bloc) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Detergent 2 25.29 12.646 11.421 0.00453 ** ## Bloc 4 44.27 11.069 9.997 0.00335 ** ## Residuals 8 8.86 1.107 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 On conclut que les blocs et le type de détergent ont un effet sur la blancheur des vêtements après lavage. On peut effectuer des comparaisons multiples pour déterminer où se trouve les différences entre les groupes de détergent. On ne fait aucune comparaisons multiples pour les blocs, qui représentent différents produits tachants, puisque ce facteur ne nous intéresse pas vraiment. On inclut les blocs dans l’analyse pour tenir compte du dispositif expérimental que nous avons utilisé. ##moyenne des groupes Moys &lt;- tapply(X = lavage$Blancheur, INDEX = lavage$Detergent, FUN = mean) Moys ## Controle Type1 Type2 ## 3.644 6.730 5.854 TukeyHSD(aov.bloc, which = &quot;Detergent&quot;) ## Tukey multiple comparisons of means ## 95% family-wise confidence level ## ## Fit: aov(formula = Blancheur ~ Detergent + Bloc, data = lavage) ## ## $Detergent ## diff lwr upr p adj ## Type1-Controle 3.086 1.1843628 4.987637 0.0042126 ## Type2-Controle 2.210 0.3083628 4.111637 0.0254478 ## Type2-Type1 -0.876 -2.7776372 1.025637 0.4257772 On note deux groupes, l’un formé par les groupes Type1 et Type2, et l’autre formé par Controle. On peut représenter ces groupes à l’aide de traits : Contrôle Type2 Type1 3.642 5.852 6.732           Ou encore, on peut illustrer le résultat pour les différents niveaux du traitement Detergent directement dans un graphique (Figure 8.14). ##MSE de l&#39;ANOVA MSE &lt;- 1.11 ##vecteur pour créer l&#39;axe des x&#39;s Deter &lt;- 1:3 ##barres d&#39;erreur inf &lt;- Moys - sqrt(MSE) sup &lt;- Moys + sqrt(MSE) ##graphique ##À noter que l&#39;on crée le graphique avec suffisamment ##d&#39;espace pour permettre le placement des barres d&#39;erreurs ##ainsi que les lettres en bas. C&#39;est pourquoi on étend ##la limite inférieure à 0.2 unités au bas du graphique ##au delà de la valeur minimale de la barre d&#39;erreur. plot(Moys ~ Deter, ylab = &quot;Indice de blancheur (0 à 10)&quot;, xlab = &quot;Type de détergent&quot;, type = &quot;p&quot;, ylim = c(min(inf) - 0.2, max(sup)), xlim = c(0, 4), cex.lab = 1.2, xaxt = &quot;n&quot;, main = &quot;Moyennes ± racine carrée de MSE&quot;) ##ajout de l&#39;axe des x&#39;s axis(side = 1, at = c(1, 2, 3), labels = c(&quot;Contrôle&quot;, &quot;Type 1&quot;, &quot;Type 2&quot;)) ##ajout de barres d&#39;erreurs arrows(x0 = Deter, x1 = Deter, y0 = inf, y1 = sup, angle = 90, code = 3, length = 0.05) ##ajout des lettres text(x = 1, y = 2.46, labels = &quot;a&quot;, cex = 1.2) text(x = 2, y = 5.54, labels = &quot;b&quot;, cex = 1.2) text(x = 3, y = 4.67, labels = &quot;b&quot;, cex = 1.2) Figure 8.14: Effets de deux nouveaux type de détergents et d’un détergent conventionnel contrôle sur la blancheur de vêtements ayant été tachés. Finalement, on conclut que la blancheur des vêtements ne diffère pas entre les groupes de nouveaux détergents Type1 et Type2, mais que cette blancheur est nettement supérieure à celle du groupe avec un détergent conventionnel (Controle). b. Était-ce justifié d’utiliser des blocs? Justifiez votre réponse. Réponse Oui, l’utilisation des blocs était justifiée puisque le terme Bloc expliquait une partie importante de la variance de la variable réponse, tel qu’indiqué par un \\(F_{4, 8}\\) de 10 et un \\(P = 0.0033\\). De plus, le dispositif a été créé en bloc et c’est toujours une bonne idée d’inclure ce genre de facteur dans l’analyse pour refléter la structure du dispositif. "],["élaboration-et-optimisation-de-plans-déchantillonnage.html", "Module 9 Élaboration et optimisation de plans d’échantillonnage", " Module 9 Élaboration et optimisation de plans d’échantillonnage La leçon précédente a illustré les concepts d’effets fixes et d’effets aléatoires ainsi que de modèle mixte. L’ANOVA en blocs complets aléatoires a été présentée comme étant le cas le plus simple de modèle mixte et comme l’extension du test \\(t\\) pour les données appariées. Le dispositif en blocs complets aléatoires comporte des avantages importants, notamment sur le plan logistique comparé aux dispositifs complètement aléatoires (p. ex., ANOVA à un critère avec répétitions, ANOVA à deux critères avec répétitions). Dans la présente leçon, nous discuterons de façon plus générale de l’élaboration de dispositifs expérimentaux. Nous fournirons plusieurs conseils afin d’arriver à des conclusions solides. "],["théories-8.html", "9.1 Théories", " 9.1 Théories 9.1.1 Plans d’échantillonnage Dans ce texte, nous parcourrons différents éléments qui sont importants dans l’élaboration de plans d’échantillonnage aussi appelés plans d’expérience ou dispositifs expérimentaux (experimental design, sampling design) afin de tirer le maximum d’information de votre expérience et d’arriver à des conclusions solides. Avant d’aller plus loin, nous allons définir quelques termes qui reviendront au cours de la leçon. Certains de ces termes ont été présentés dans des leçons antérieures, mais nous tenons à les rappeler ici. 9.1.1.1 Expérience vs étude d’observation Une étude d’observation consiste à prendre des mesures sur des unités d’échantillonnage sélectionnées aléatoirement, mais où les traitements n’ont pas nécessairement été attribués aléatoirement. Par exemple, dans une étude sur les impacts de la pauvreté et des inégalités sociales sur la santé des populations on cherche à déterminer l’effet de la classe socio-économique sur les risques d’avoir une certaine maladie. Bien qu’on puisse sélectionner aléatoirement des personnes appartenant à chacune des classes qui nous intéressent, nous ne pouvons pas assigner aléatoirement une classe aux personnes participant à l’étude. Les résultats ne sont pas aussi clairs que ceux d’une vraie expérience aussi appelée expérience contrôlée. En effet, il est difficile de contrôler plusieurs facteurs dans une étude d’observation. À l’opposé, l’expérience consiste à prendre des mesures sur des unités expérimentales sélectionnées aléatoirement, auxquelles on a assigné aléatoirement un certain traitement. Ce genre de dispositif expérimental est celui que nous avons considéré dans le cas de l’ANOVA à un et à deux critères, par exemple. Dans l’expérience, nous contrôlons tous les facteurs et nous ne faisons varier que ceux qui sont d’intérêt, ce qui nous permet d’arriver à des conclusions solides. Bien que l’expérience soit souvent l’approche idéale, elle manque parfois de réalisme, et l’étude d’observation peut procurer des informations supplémentaires à propos de l’effet potentiel de certaines variables sur le système d’intérêt. 9.1.1.2 Témoin Le témoin ou contrôle (control) d’une expérience est un traitement qui consiste en une absence de traitement. Autrement dit, c’est un traitement qui présente des conditions de référence. Par exemple, dans une expérience qui compare l’effet de deux médicaments sur des patients, on pourrait considérer un troisième groupe de patients auxquels on n’administre pas de médicament. C’est le goupe témoin ou le témoin. L’utilisation d’un témoin est primordiale afin d’obtenir une moyenne de référence que l’on peut comparer aux moyennes de groupes avec traitements. Le témoin contrôle les changements temporaux ou les effets associés à des procédures ou à des manipulations liées à certains traitements. 9.1.1.3 Placébo Le placébo (placebo) est un traitement témoin qui consiste à administrer une substance neutre à des individus afin de contrôler l’effet de la manipulation. Par exemple, imaginons une expérience visant à déterminer l’effet d’un nouveau médicament. On administre le médicament sous forme de comprimé à un premier groupe tandis que le deuxième groupe reçoit un comprimé de sucre (placébo) et un troisième groupe ne subit aucun traitement (témoin). Comme autre exemple, dans une expérience qui vise à étudier l’activité d’animaux en présence de prédateurs, on soumet un premier groupe d’animaux à la présence d’un prédateur vivant maintenu dans un enclos. On soumet un deuxième groupe à la présence d’un prédateur empaillé (placébo), tandis qu’un troisième groupe n’est exposé à aucun prédateur (témoin). Le placébo est important lorsqu’il existe un effet de manipulation (p. ex., administrer un comprimé, la présence d’un prédateur immobile dans l’enclos). La comparaison d’un groupe placébo (avec manipulation) à un groupe témoin (sans manipulation) permet de quantifier l’effet de la manipulation sur les individus utilisés dans l’expérience. La comparaison d’un groupe placébo à un groupe qui a reçu un traitement permet de conclure qu’il y a bel et bien un effet du traitement en tenant compte de l’effet de manipulation. 9.1.1.4 Prise de données en aveugle Un moyen parfois utilisé afin d’éviter les biais associé à la prise de données est la prise de données en aveugle aussi appelée prise de données à l’insu (blinding). Pour ce faire, les personnes qui récoltent les données ou les sujets de l’expérience ignorent le traitement qui a été appliqué. Ceci permet d’éviter le cas où un technicien trop zélé introduit consciemment ou inconsciemment un biais dans les données en favorisant un des traitements. Le même phénomène peut survenir lorsqu’un patient sachant qu’il reçoit un certain traitement influence sa réponse. Autrement dit, un patient à qui on annonce qu’il reçoit le médicament peut avoir une réponse plus forte au traitement qu’un patient qui ne connaît pas la nature du traitement. La prise de données en aveugle peut régler ce problème. La prise de données en double aveugle ou prise de données à double insu (double blinding) implique que les personnes récoltant les données ainsi que les participants à l’expérience ignorent les traitements qui ont été administrés. 9.1.1.5 Répétition, randomisation et répartition spatiale La répétition des traitements ou des combinaisons des traitements contrôle la variabilité due au hasard aussi appelée stochasticité lors d’une expérience. En d’autres mots, la présence de répétitions peut protéger contre la présence d’éléments perturbateurs qui agissent sur les unités expérimentales (p. ex., un arbre tombe sur un des bassins contenant des têtards dans une expérience sur la compétition, un prédateur se nourrit des animaux dans un des enclos d’une expérience sur la sélection d’habitat, un patient testant un nouveau médicament contracte la grippe, une voiture a un bris mécanique affectant sa consommation d’essence alors qu’on testait sa performance en fonction de différents types d’essence, etc.): si certaines des unités sont contaminées, celles qui restent pourront tout de même être utilisées pour mener à bien l’expérience. La randomisation consiste en l’application d’un processus aléatoire sur l’attribution d’une condition aux unités d’échantillonnage (voir la leçon 2). Elle contrôle le biais potentiel de la sélection, de l’emplacement, et de l’attribution des traitements aux unités expérimentales. La randomisation aide à assurer l’indépendance des observations (ou des erreurs), une condition essentielle à l’utilisation de plusieurs analyses statistiques. La répartition spatiale ou temporelle (selon le cas) contrôle la variation spatiale ou temporelle des propriétés des unités expérimentales. Autrement dit, les conditions de l’emplacement où se trouve une unité expérimentale peuvent être très différentes des conditions à un autre emplacement. En disposant les parcelles à différents endroits sur le site d’étude, on obtient une meilleure estimation de la variabilité naturelle des unités expérimentales. Ceci permet de tirer des conclusions plus générales. De plus, établir les unités expérimentales à une distance suffisante les unes des autres assure aussi l’indépendance des observations. Le prochain exemple illustre certains problèmes liés à la répétition, la randomisation et la répartition spatiale qui peuvent survenir lors de l’élaboration d’un plan d’échantillonnage. On s’intéresse à l’effet des arbres sur les îlots de chaleurs en ville lors des périodes de canicule. On émet l’hypothèse scientifique que la température est plus basse dans les zones boisées que dans les zones adjacentes. Pour tester notre hypothèse, on attend une canicule et on mesure la température moyenne de l’air au milieu d’une zone boisée et à 50 m à l’extérieur de cette même zone. Si chaque thermomètre fournit une seule mesure de température, le nombre de répétitions sera insuffisant et on ne pourra pas tester d’hypothèse. On devra donc modifier le plan d’échantillonnage afin d’incorporer plus de répétitions. On reprend l’expérience, cette fois-ci en augmentant la répétition et en disposant 10 thermomètres dans la même zone boisée et 10 à l’extérieur de cette zone au même endroit. Malgré la présence de répétitions dans la zone boisée et son extérieur, un problème est toujours évident : le dispositif modifié permettra uniquement de déterminer s’il y a des différences entre l’intérieur et l’extérieur de la zone boisée à cet endroit précis de la ville. On ne pourra pas tirer de conclusion sur la ville en entier. En effet, les conditions (p. ex., couloir de vent, zone très bétonnée) à un point donné de la ville peuvent être différentes de celles à d’autres endroits de la même ville. Afin de pouvoir décrire l’effet des arbres sur les îlots de chaleur pour l’ensemble de la ville, il est nécessaire de répartir les thermomètres à différents endroits de la ville (c.-à-d., répartition spatiale) de façon aléatoire afin d’obtenir une meilleure estimation de la variabilité de cet effet. Dans le but d’améliorer la répartition spatiale, on reprend (encore) l’expérience en disposant 10 thermomètres au milieu d’une zone boisée et 10 thermomètres à l’extérieur d’une seconde zone boisée. Avec un tel dispositif, il sera impossible de distinguer l’effet des arbres de celui de l’effet de l’emplacement des thermomètres dans la ville. En d’autres mots, l’emplacement dans la ville et « intérieur vs. extérieur de la zone boisée » seront deux variables confondues l’une avec l’autre. Il se peut que les conditions ne soient pas homogènes entre les deux emplacements, ce qui rendra difficile la comparaison. Si on trouve une différence, il n’y aucun moyen de prouver que cela est dû uniquement à la présence d’arbres, l’emplacement dans la ville pourrait aussi expliquer cette différence. La solution ultime (et le meilleur plan d’échantillonnage) consistera à disposer chaque thermomètre à un endroit de la ville différent (déterminé aléatoirement) à l’intérieur ou l’extérieur d’une zone boisée, afin d’obtenir 20 thermomètres à 20 endroits différents de la ville (10 thermomètre à l’intérieur de 10 zones boisées et 10 thermomètres à l’extérieur de 10 autres zones boisées), ce qui correspondrait à un dispositif complètement aléatoire. Contrairement aux plans d’échantillonnage présentés plus haut, cette dernière approche permettra d’obtenir une mesure de la température à l’intérieur ou à l’extérieur des zones boisées sans ambiguïté. Dans cet exemple, aucun témoin n’a été présenté. L’inclusion d’un témoin améliorerait potentiellement le dispositif. Un témoin dans le contexte de cette expérience pourrait s’avérer être un groupe de thermomètre à la limite de la ville où les îlots de chaleurs sont moins importants, ou très en hauteur où il n’y a pas d’effet des arbres, ni du béton accumulant la chaleur. Le cas échéant, les thermomètres témoins devraient être répartis aléatoirement aux limites de la ville ou dans les hauteurs. Ceci permettrait de tester efficacement l’effet des arbres sur la température de l’air en ville en contrôlant la variabilité des endroits dans cette ville (répartition spatiale), et en intégrant la randomisation et la répétition. Un dispositif alternatif consisterait à utiliser des blocs complets aléatoires en sélectionnant aléatoirement 10 endroits dans la ville et en disposant un trio de thermomètres à chaque endroit (1 thermomètre à l’intérieur de la zone boisée, 1 thermomètre à l’extérieur et 1 thermomètre très en hauteur). 9.1.2 Nos meilleurs alliés dans l’élaboration d’un plan d’échantillonnage Lors de l’élaboration d’un plan d’échantillonnage, nos meilleurs alliés sont : une hypothèse scientifique claire et bien définie; l’échantillonnage aléatoire des observations parmi la population statistique d’intérêt; l’assignation aléatoire des traitements aux unités expérimentales; l’utilisation de témoins; l’utilisation de blocs (complets aléatoires) lorsque le milieu est hétérogène; la stratification lorsque des conditions différentes sont présentes dans la population (p. ex., différents habitats, différentes régions géographiques, différentes conditions économiques, différents accès à des services publics, différents historiques médicaux); l’utilisation de variables additionnelles (p. ex., pH et type de sol dans des études environnementales, âge du patient dans une étude médicale, revenu familiale dans une étude sociologique) si les conditions hétérogènes sont susceptibles d’influencer la variable réponse et qu’on veut quantifier cet effet; la répétition, la répétition et la répétition (une bonne taille d’échantillon peut pallier plusieurs problèmes); l’équilibre du nombre d’unités expérimentales par traitement (dispositif équilibré). Avant de réaliser une expérience et de récolter les données, il est très important de répondre à certaines questions, notamment : Quelle est l’hypothèse scientifique à tester? Quel est le meilleur dispositif qui permettra de tester simplement et efficacement l’hypothèse? Est-ce une expérience (avec manipulations) ou une étude d’observation (sans manipulations)? Quelle est la variable réponse et quelles sont les variables explicatives? Le dispositif que j’ai choisi est-il un dispositif complètement aléatoire ou y a-t-il des unités expérimentales avec une structure particulière (p. ex., observations groupées entre elles)? Y a-t-il de l’hétérogénéité environnementale ou temporelle? Le cas échéant, comment cette hétérogénéité a-t-elle été prise en compte ou mesurée? Les traitements ont-ils été attribués aléatoirement aux unités expérimentales? Mon dispositif est-il balancé (équilibré) et ai-je suffisamment de répétitions? Quelle analyse statistique permettra de tester le plus simplement et efficacement l’hypothèse scientifique? Pour l’analyse statistique choisie, quelles sont les suppositions à respecter afin que les conclusions soient valides? Mes observations sont-elles réellement indépendantes? Si elles ne le sont pas, comment l’analyse tient-elle compte de la non-indépendance des données? Y a-t-il des variables confondantes (counfounding variable, counfounding effect) qui rendront potentiellement difficile l’interprétation des résultats? On entend par variable confondante, une variable qui est intimement liée à une autre et qui rend difficile voire impossible de distinguer l’effet des deux variables sur la variable réponse. Par exemple, considérons une expérience où l’on a trois sites auxquels on assigne un traitement différent à chacun. Si on trouve une différence entre les traitements au stade de l’analyse, cette différence peut aussi bien être due à l’effet du traitement ou à l’effet du site car chaque site a reçu un traitement différent. On dira donc que l’effet du traitement et du site sont confondus. Sans plus d’information (répétitions des sites) ou l’utilisation de blocs (p. ex., l’application de tous les traitements à chaque site), on ne peut rien conclure et on aura dépensé inutilement du temps et de l’argent. 9.1.3 Problèmes associés à certains plan d’échantillonnage 9.1.3.1 L’absence de réelles répétitions : la pseudoréplication Au milieu des années 1980, un terme est apparu pour désigner des problèmes liés à la répétition de traitements d’une expérience. Hurlbert (198429) a introduit le terme pseudoréplication dans un article qui a été énormément cité en écologie. En gros, la pseudoréplication désigne une expérience mal élaborée qui ne possède pas de réelles répétitions. Trois différents types de pseudoréplication ont été présentés dans l’article original, soit la pseudoréplication simple, la pseudoréplication sacrificielle et la pseudoréplication temporelle. Nous décrivons chaque type dans les prochaines lignes. 9.1.3.1.1 Pseudoréplication simple La pseudoréplication simple (simple pseudoreplication) est le cas où on a plusieurs observations, mais seulement une vraie unité expérimentale par traitement (absence de réelles répétitions). On peut avoir plusieurs mesures dans deux parcelles, mais les observations ne sont pas indépendantes. Par exemple, si on s’intéresse à l’effet d’interventions par une (ou un) orthopédagogue pour la réussite d’élèves ayant des difficultés d’apprentissage, on pourrait engager une (ou un) orthopédagogue pour aider les élèves en difficulté dans une école et comparer avec les élèves dans une école sans orthopédagogue. On pourrait sélectionner aléatoirement des élèves en difficulté dans chaque école et comparer leur réussite scolaire. Même avec 50 élèves choisis aléatoirement dans chaque école, on a seulement deux réelles unités expérimentales : les deux écoles. Effectivement, les élèves dans la même école ne sont pas indépendants. Si on détecte une différence entre les deux écoles, il se peut que ce soit dû à une différence des caractéristiques de ces deux écoles (quartier riche ou défavorisé, autre aide à l’apprentissage, meilleurs enseignants) ou à l’aide amenée par l’orthopédagogue : les deux effets sont confondus. Dans cet exemple, il est nécessaire d’augmenter le nombre d’unités expérimentales plutôt que les observations sur une même unité. Pour ce faire, il faut sélectionner aléatoirement les élèves en difficulté d’apprentissage dans différentes écoles et assigner aléatoirement le traitement (interventions d’un orthopédagogue ou non) à chacun pour obtenir un dispositif complètement aléatoire (1 élève dans 1 école = 1 unité expérimentale). Si ceci est logistiquement impossible, on pourrait sélectionner aléatoirement 8 écoles assez près les unes des autres pour permettre les interventions du même orthopédagogue, en choisissant aléatoirement quelles écoles auraient droit à ses services et en suivant plusieurs élèves dans chaque école. Le cas échéant, on aurait 8 unités expérimentales puisque les élèves provenant de la même école ne seraient pas considérés comme indépendants. Toutefois, avec cette structure, il faudrait utiliser les moyennes de chaque école dans les analyses ou utiliser une analyse qui peut tenir compte de structures plus complexes. 9.1.3.1.2 Pseudoréplication sacrificielle La pseudoréplication sacrificielle (sacrificial pseudoreplication) survient lorsqu’on a plusieurs observations sur les mêmes unités expérimentales et que l’on combine ces observations dans la même analyse sans tenir compte de leur origine. Ici, nous avons une répétition des traitements (plusieurs unités par traitement), mais on considère à tort que chaque observation est une vraie unité expérimentale. Cette erreur est souvent justifiée par des énoncés tels que “Les sites ne différaient pas entre eux et nous avons combiné les données de chacun dans la même analyse …” La réalité, en milieu naturel et dans d’autres systèmes complexes, est qu’il est pratiquement impossible d’observer deux sites identiques et il est incorrect de simplement regrouper toutes les observations ensembles. Généralement, les observations qui proviennent d’une même unité expérimentale (p. ex., phénomènes observés sur un même site ou un même quadrat, mesures prises sur un même arbre, les personnes d’une même famille, ou les élèves d’une même école) sont plus similaires entre elles que deux observations provenant de deux unités expérimentales différentes. Comme solution, on peut appliquer les mêmes remèdes utilisés pour faire face à la pseudoréplication simple, soit augmenter le nombre de réelles unités expérimentales, travailler sur les moyennes des unités expérimentales (qui elles sont indépendantes les unes des autres), ou encore utiliser une analyse statistique qui permet d’incorporer les observations répétées dans chaque unité expérimentale. 9.1.3.1.3 Pseudoréplication temporelle La pseudoréplication temporelle (temporal pseudoreplication) se produit lorsqu’on prend des mesures successives sur les mêmes unités expérimentales étalées dans le temps, et que l’on considère ces mesures à différentes périodes comme des observations indépendantes. Les observations qui proviennent d’une même unité expérimentale à différentes périodes ne sont pas indépendantes. Par exemple, des mesures de l’état de santé du même patient à travers le temps ou des mesures de l’abondance d’une espèce à un site, mais à différentes périodes, ne constituent pas des observations indépendantes. Une solution à ce problème consiste à analyser chaque période séparément ou à utiliser une analyse qui tient compte des mesures répétées (p. ex., ANOVA pour mesures répétées, modèles mixtes). 9.1.3.2 Contrôler la variabilité temporelle Un type particulier de dispositif expérimental concerne l’application de traitements dans le temps, et la prise de mesure typiquement avant et après le traitement (before and after controlled impact design (BACI)). Par exemple, dans une expérience sur l’effet des tentes à faible taux d’oxygène sur la performance d’athlète de haut niveau (simulation d’altitude), on sélectionne aléatoirement 12 athlètes et on mesure leurs taux sanguins en globules rouges. On applique ensuite l’un des deux traitements suivants : un mois à dormir dans une tente avec une concentration d’oxygène X et un mois avec une concentration Y. Bien que chaque athlète puisse agir comme son propre témoin (conditions avant vs après), l’application d’un traitement dans le temps présente un problème: il est impossible de distinguer l’effet du traitement d’un effet temporel si toutes les unités reçoivent un traitement. Toute expérience ou étude d’observation s’échelonnant sur le temps nécessite un témoin temporel. En d’autres mots, on a besoin d’unités expérimentales qui n’ont reçu aucun traitement dans le temps afin de procurer une mesure de référence temporelle (c.-à-d., ce qui se passe en l’absence de traitement entre les périodes de mesures). Dans l’exemple ci-haut sur la performance des athlètes, il serait important d’inclure un traitement témoin (tente avec une concentration normale d’oxygène), afin d’obtenir quatre répétitions de chaque traitement. Cette modification du dispositif expérimental permettrait de distinguer les différences avant et après l’application de chaque traitement, en tenant compte des différences dues uniquement au temps écoulé entre deux mesures (le témoin avant et après). 9.1.3.3 Sources de confusion Lorsqu’on effectue une expérience ou une étude d’observation, certains éléments peuvent influencer la qualité des conclusions, ce que Hurlbert (1984) désigna comme des sources de confusion. Parmi celles-ci, on note les changements temporels qui surviennent pendant l’expérience modifiant les propriétés des unités expérimentales. L’utilisation de témoins permet de pallier ce problème. Les effets liés aux procédures, comme la manipulation d’animaux ou la perturbation du sol lors de la prise d’une mesure, sont d’autres sources de confusion. Ici encore, l’utilisation de témoins atténue ce problème. Le biais associé à l’expérimentateur est une autre source de confusion qui peut se manifester, par exemple, lorsqu’on est plus minutieux dans la prise de mesure de certains traitements. Cette dernière source de confusion peut être minimisée en assignant aléatoirement les traitements aux unités expérimentales, en randomisant la séquence de prise de mesure, ou en prenant les mesures en aveugle (p. ex., le traitement est inconnu de la personne prenant la mesure). La variabilité de l’expérimentateur (erreur aléatoire) est une source de confusion associée naturellement à la prise de mesure en tant que telle. On peut réduire cette source de confusion en augmentant le nombre de répétitions. La variabilité des unités expérimentales est une autre source de confusion découlant du fait que chaque unité expérimentale a des caractéristiques qui lui sont propres et que cette variabilité reflète la variabilité à l’intérieur de la population. La répétition des traitements, une bonne répartition spatiale des unités expérimentales, ainsi que la récolte de variables secondaires (p. ex., pH, humidité) pour décrire plus directement les caractéristiques de chaque unité expérimentale peuvent prévenir les problèmes associés à cette source de confusion. Certaines sources de confusion sont difficiles ou impossibles à contrôler, notamment l’effet d’événements aléatoires sur l’expérience. Par exemple, lors d’une expérience sur le terrain, le passage d’un ouragan sur les sites d’études peut modifier la variable réponse que l’on mesurera plus tard. Il en va de même avec les pannes électriques qui mettent hors service les systèmes de filtration ou d’éclairage dans des chambres de croissance ou des aquariums. Si l’événement influence toutes les unités expérimentales, le problème est moins grave. Toutefois, si seulement quelques unités expérimentales sont touchées, les observations provenant des unités affectées seront difficiles à interpréter et à comparer au reste des observations: à la limite, certaines données devront être écartées de l’analyse. Le meilleur moyen de réduire cette source de confusion est d’augmenter le nombre de répétitions et de maintenir une bonne répartition spatiale des unités expérimentales. Malgré toutes ces précautions, certaines sources de confusion demeurent problématiques, comme le cas d’un agent extérieur qui intervient délibérément sur les unités expérimentales d’un traitement particulier et épargne celles des autres traitements. Pour illustrer, considérons le cas d’un site d’étude sur lequel on a disposé des enclos d’animaux où l’on estime la survie de petits mammifères selon différentes densités (nombre d’individus/m\\(^2\\)). Ce site est fréquenté par un prédateur qui vise spécifiquement les enclos d’animaux avec les plus grandes densités. Si cette prédation passe inaperçue, on arrivera à des conclusions erronées. Un mauvais plan d’échantillonnage, comme la présence d’effets confondants ou un gradient qui n’a pas été mesuré, correspond aussi à une source de confusion qui ne permet pas de tester de façon efficace une hypothèse scientifique. Dans de tels cas, le facteur qui agit délibérément sur les unités expérimentales, c’est l’expérimentateur. Il faut donc rester vigilant dans l’élaboration des plans d’échantillonnage. 9.1.4 Conclusion Dans ce texte, nous avons vu plusieurs éléments importants à considérer lors de l’élaboration de plans d’échantillonnage, certains problèmes incluant la pseudoréplication et des sources de confusion atténuant ou invalidant les conclusions, ainsi que des solutions. La planification du plan d’échantillonnage est la partie la plus importante de votre projet et requiert une bonne réflexion. Cette étape détermine si l’analyse sera simple ou plus complexe. Un bon dispositif permet de tester l’hypothèse scientifique simplement et facilite énormément l’analyse. On peut pallier certains problèmes de dispositif expérimental avec des analyses sophistiquées, mais, dans certains cas, l’analyse des données s’avère impossible. Avec une analyse inappropriée réalisée à partir d’un plan d’échantillonnage solide, on peut toujours réanalyser les données, mais avec un mauvais dispositif, il faut refaire l’expérience. Hurlbert, S. H. 1984. Pseudoreplication and the design of ecological field experiments. Ecological Monographs 54:187-211.↩︎ "],["laboratoire-2.html", "9.2 Laboratoire", " 9.2 Laboratoire 9.2.1 Instructions Ce laboratoire est obligatoire et s’échelonne sur deux leçons. Ce laboratoire devra être remis selon les échéanciers de votre feuille de route. Le but de ce laboratoire est d’élaborer un plan d’échantillonnage afin de tester une hypothèse scientifique développée à partir de l’un des thèmes que vous trouverez ci-dessous. Vous devrez choisir un thème, formuler une hypothèse à tester, élaborer un plan d’échantillonnage pour la collecte de données réelles, choisir l’analyse statistique appropriée à partir de celles présentées dans le cours et réaliser cette analyse statistique sur les données. Prenez note que vous devez vous-même collecter ces données. Elles ne peuvent donc pas être simulées ou provenir de données ayant déjà fait l’objet d’analyses statistiques. Votre laboratoire devra obligatoirement inclure les éléments suivants : Introduction: quelques lignes sur l’hypothèse scientifique que vous désirez tester et l’hypothèse statistique que vous utiliserez. Méthodes: une description détaillée de votre plan d’échantillonnage permettant de répéter l’expérience exactement comme vous l’avez exécutée (matériel utilisé, identification des unités expérimentales, nombre de répétitions, identification de la variable réponse et de son unité de mesure, le ou les facteurs de l’expérience). Analyse: une justification du ou des tests statistiques que vous avez choisis pour analyser les données et une énumération des suppositions de cette analyse. Un tableau présentant les données brutes que vous avez récoltées. Le jeu de données en format texte sauvegardé dans un fichier séparé, tel que vous avez importé dans R pour réaliser votre analyse. Résultats: l’exécution de l’analyse, la vérification des suppositions, le summary( ) de votre analyse si pertinent. Graphiques: un ou deux graphiques illustrant les conclusions de votre étude. Discussion: quelques lignes sur l’interprétation des résultats en relation avec votre hypothèse scientifique de départ. Votre laboratoire doit comprendre tous les éléments identifiés plus haut, ainsi que le code R utilisé pour importer les données, exécuter l’analyse, vérifier les hypothèse, et réaliser les graphiques tels qu’ils apparaissent dans votre laboratoire. Afin de bien distinguer le code R et les sorties (output) du reste du texte, vous devez utiliser une police monospace comme celle-ci qui attribue le même espace à tous les caractères (p. ex., police de type Courier New) – ceci préserve l’alignement des éléments des sorties. La structure du laboratoire à remettre doit s’inspirer de la forme des solutions des autoévaluations. 9.2.2 Suggestions de thèmes à étudier La germination de graines (quelques possibilités ou d’autres si vous êtes inspirés)30 : le temps de germination de trois espèces (p. ex., haricots, lentilles, luzerne, oignons, radis); le succès de germination d’une espèce à pH neutre (eau distillée) et à pH acide (gouttes de jus de citron); le succès de germination à différentes températures (\\(4\\,^{\\circ}\\mathrm{C}\\)) et température de la pièce (\\(20\\,^{\\circ}\\mathrm{C}\\)); le succès de germination dans différents substrats (p. ex., terre noire, papier absorbant, aucun substrat); la longueur des racines de graines germées dans différents substrats. \\end{itemize} La moisissure (parmi les exemples suivants)31 : la présence de moisissure sur des carrés de pain de trois types (pain biologique, pain blanc générique, pain blé entier); le temps qu’il faut pour qu’apparaissent des moisissures sur des carrés de pain de trois types (pain biologique, pain blanc générique, pain blé entier); le temps qu’il faut pour qu’apparaissent des moisissures sur des carrés de pain d’un type de pain à différentes températures; le temps qu’il faut pour qu’apparaissent des moisissures sur des pêches soumises à différentes températures. \\end{itemize} La vitesse de frappe au clavier: la vitesse d’écriture entre hommes et femmes pour le même texte; la vitesse d’écriture entre hommes et femmes selon le type de texte (nouvelle du sport vs nouvelle des arts et spectacles). \\end{itemize} L’occurrence de la pratique de jeux vidéos chez les hommes et les femmes de différentes classes d’âge (15 – 20 ans, 21 – 25 ans, &gt; 25 ans). Le nombre de minutes passées à écouter des émissions en baladodiffusion par semaine chez les hommes et les femmes de différentes classes d’âge. Le nombre de minutes passées sur internet par semaine entre les hommes et les femmes de différentes classes d’âge. La fréquence d’utilisation de sacs réutilisables par les clients de différents types de commerces (épicerie, SAQ, quincaillerie, boutique de vêtements). La proportion de véhicules domestiques (GM, Chrysler, Ford) par rapport aux véhicules d’origine étrangère dans deux quartiers de votre ville. La capacité à distinguer des bananes biologiques vs non biologiques en dégustation en aveugle chez un groupe de gens. La préférence entre le vin et la bière selon le sexe et la classe d’âge. La fréquence de certaines couleurs de bonbons dans des sacs de bonbons (p. ex., jujubes, fèves en gelée) selon différentes compagnies (Dare, Choix du Président, Nos Compliments, etc.). La différence entre le volume moyen de raisins secs de différentes compagnie (Sun-Maid, Choix du Président, Nos Compliments, Sans Le nombre moyen de pépins dans les poivrons verts biologiques et les poivrons verts non biologiques. La préférence de chats pour trois types de nourriture (e.g., croquettes sèches, nourriture en boîte de conserve, nourriture de table). Le nombre d’hommes et de femmes qui accompagnent leur chien au parc. Le temps que mettent différents types de boissons gazeuses dans un verre (avec ou sans glace) à perdre tout leur CO\\(_2\\) à la température de la pièce. La longueur du texte de nouvelles internationales, d’environnement et de chroniques culinaires dans différents journaux. La fréquence d’un mot ou d’un groupe de mots dans différents journaux selon le sujet abordé ou depuis l’avènement d’un événement important (p. ex., nomination d’un nouveau premier ministre)32. Le nombre de publicités dans différents types de quotidiens en semaine vs en fin de semaine. Le prix de l’essence à différentes stations-service au Québec, en Ontario et au Manitoba ((http://www.gasbuddy.com)). La fréquence de personnes portant des écouteurs dans différents types de transport en commun (autobus, métro, train de banlieue). Le temps de germination de certaines espèces peut être relativement long. Assurez-vous que le calendrier du cours vous permet de réaliser votre expérience dans les délais prescrits.↩︎ Le temps d’apparition de moisissures peut être relativement long pour certains substrats. Assurez-vous que le calendrier du cours vous permet de réaliser votre expérience dans les délais prescrits.↩︎ La fréquence d’un mot ou d’un groupe de mots peut être évaluée grâce à des logiciels d’analyse de texte, tels que TextSTAT http://neon.niederlandistik.fu-berlin.de/en/textstat/ et Textalyser http://textalyser.net/.↩︎ "],["régression-linéaire-simple-et-corrélation.html", "Module 10 Régression linéaire simple et corrélation", " Module 10 Régression linéaire simple et corrélation Dans le texte de la leçon prédécédente, nous avons vu illustré de nombreux éléments importants à considérer lors de l’élaboration de plans d’échantillonnage. Certains problèmes liés à l’échantillonnage ont été présentés, notamment la pseudoréplication et des sources de confusion. Ces conseils s’appliquent aussi à d’autres types d’analyses. Dans la présente leçon, nous introduirons la régression linéaire simple et la corrélation. Les conditions d’utilisation et les tests d’hypothèses qui y sont associés seront présentés. Dans les dernières leçons, la variable réponse était numérique et l’objectif était de déterminer comment cette variable changeait selon les niveaux d’un ou de plusieurs facteurs (p. ex., test \\(t\\), ANOVA). Les facteurs définissaient des groupes et les moyennes des groupes étaient comparées entre elles. Nous allons maintenant voir des applications où on veut déterminer la relation entre une variable réponse et une variable explicative numérique (au lieu d’un facteur). Ici, la variable explicative est mesurée sur une échelle numérique (p. ex., âge) au lieu d’être exprimée en niveaux (p. ex., classes d’âge). "],["théorie.html", "10.1 Théorie", " 10.1 Théorie 10.1.1 La régression linéaire La régression linéaire simple (simple linear regression) est le cas où une variable réponse, aussi appelée variable dépendante (dependent variable), varie en fonction d’une variable explicative, aussi appelée variable indépendante (independent variable) ou prédicteur (predictor). On utilise l’appellation régression linéaire puisque la relation entre les deux variables est linéaire: on trace une droite linéaire pour représenter la relation entre les deux variables. La régression linéaire implique une relation de cause à effet. Les variations des observations de la variable réponse (variable dépendante) sont causées, du moins en partie, par une variable explicative (variable indépendante). 10.1.1.1 Équation de régression On peut représenter l’équation de la droite de régression linéaire comme suit : \\[y_i = a + bx_i + \\epsilon_i\\] où \\(y_i\\) correspond à l’observation \\(i\\) de la variable réponse \\(y\\), \\(a\\) correspond à l’ordonnée à l’origine (intercept). L’ordonnée à l’origine représente le point sur l’axe des y’s qui est croisé par la droite de régression. Le terme \\(b\\) correspond à l’estimation de la pente, qui indique le taux d’accroissement de \\(y\\) avec une augmentation d’une unité de la variable explicative \\(x\\). Le terme \\(x_i\\) correspond à l’observation \\(i\\) de la variable explicative \\(x\\), et \\(\\epsilon_i\\) correspond au terme d’erreur associé à l’estimation de l’observation \\(y_i\\). Une notation équivalente utilise des termes \\(\\beta\\), tels que : \\[y_i = \\beta_0 + \\beta_{x} x_i + \\epsilon_i\\] où \\(\\beta_0\\) représente l’ordonnée à l’origine et \\(\\beta_{x}\\) indique la pente de la variable explicative \\(x\\). C’est cette dernière notation que nous utiliserons dans cette leçon. De manière générale, on emploie le terme générique coefficient pour désigner \\(\\beta_0\\), \\(\\beta_{x}\\), \\(a\\) et \\(b\\). Ces coefficients représentent l’estimation de paramètres de la population (parameter estimate) pour désigner la valeur numérique de l’ordonnée à l’origine et de la pente. Le prochain exemple présente un jeu de données typique sur lequel on peut réaliser une régression linéaire. Nous nous intéressons au pouvoir des campagnes publicitaires lorsqu’elles utilisent principalement des “influenceurs” ou “influenceuses” du web, c.-à-d. des personnes très influentes sur les réseaux sociaux qui font du placement de produit à travers leurs vidéos ou leurs photos. Plus spécifiquement, nous voulons décrire le facteur d’augmentation des ventes de divers produits cosmétiques suite à une campagne publicitaire faisant appel à un nombre plus ou moins grands d’influenceurs ayant chacun au moins 20 000 abonnés sur ses plateformes. Le jeu de données est contenu dans le fichier influenceurs.csv. ##on importe les données influenceurs &lt;- read.table(file = &quot;Module_10/data/influenceurs.csv&quot;, sep = &#39;\\t&#39;, header = TRUE) head(influenceurs) ## Nombre Facteur ## 1 3 1.4 ## 2 4 3.1 ## 3 5 2.2 ## 4 6 2.4 ## 5 8 3.6 ## 6 9 3.2 La figure 10.1 illustre les données. On remarque déjà un patron linéaire et la droite de régression décrira formellement cette relation. Figure 10.1: Augmentation des ventes de divers produits cosmétiques en fonction du nombre d’influenceurs participant à la campagne publicitaire de ces nouveaux produits. 10.1.1.2 Estimation La régression linéaire minimise la somme des carrés des erreurs (\\(SSE\\))33 par la méthode des moindres carrés pour obtenir les estimations de l’ordonnée à l’origine et de la pente. Autrement dit, pour un jeu de données, l’équation de régression est l’équation qui donne la plus faible distance verticale entre chaque point (\\(y_i\\)) et la droite de régression (\\(\\hat{y}_i\\)). Pour des problèmes plus complexes, comme la régression multiple que nous ne verrons pas dans le cours, les paramètres sont estimés à l’aide d’algèbre matricielle. On peut calculer différentes sommes des carrés à partir des données de la régression linéaire, un peu comme nous l’avons effectué pour l’ANOVA. Les prochaines lignes montrent les sommes des carrés qui nous permettront d’obtenir les estimations des paramètres (variance résiduelle et coefficients de régression) à l’aide de la méthode des moindres carrés. La somme des carrés des erreurs (\\(SSE\\)) donne la somme des carrés de la déviation entre les valeurs observées de la variable dépendante (\\(y_i\\)) et les valeurs prédites (\\(\\hat{y}_i\\)) par l’équation de régression : \\[ SSE = \\sum_{i=1}^n (y_i - \\hat{y_i})^2\\] La somme des carrés totale de \\(Y\\) (\\(SSY\\)) donne la somme des carrés de la déviation entre les valeurs observées de la variable dépendante (\\(y_i\\)) et la moyenne de cette variable (\\(\\bar{y}\\)) \\[SSY = \\sum_{i=1}^n (y_i - \\bar{y})^2\\] La somme des carrés totale de \\(X\\) (\\(SSX\\)) donne la somme des carrés de la déviation entre les valeurs observées de la variable explicative (\\(x_i\\)) et la moyenne de cette variable (\\(\\bar{x}\\)) : \\[SSX = \\sum_{i=1}^n (x_i - \\bar{x})^2\\] La somme des produits croisés (\\(SSXY\\), sum of cross products) donne la somme du produit des déviations entre la variable dépendante et sa moyenne et des déviations entre la variable explicative et sa moyenne : \\[SSXY = \\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})\\] À partir de ces valeurs, nous pouvons obtenir la somme des carrés de la régression (\\(SS_{\\mathrm{r\\acute{e}gression}}\\)) : \\[SS_{\\mathrm{r\\acute{e}gression}} = \\frac{SSXY^2}{SSX}\\] La somme des carrés totale de \\(Y\\) (\\(SSY\\)), la somme des carrés de la régression (\\(SS_{\\mathrm{r\\acute{e}gression}}\\)) et la somme des carrés des erreurs (\\(SSE\\)) sont reliées par la relation suivante : \\[SSE = SSY - SS_{\\mathrm{r\\acute{e}gression}}\\] La pente (\\(b\\), \\(\\beta_{x}\\)) s’obtient : \\[\\beta_{x} = \\frac{SSXY}{SSX}\\] Puisque la droite de régression passe par le point \\(\\bar{y}\\) et \\(\\bar{x}\\), on peut trouver l’ordonnée à l’origine (\\(a\\), \\(\\beta_0\\)) : \\[\\bar{y} = \\beta_0 + \\beta_{x}\\bar{x} \\] \\[\\bar{y} - \\beta_{x}\\bar{x} = \\beta_0 \\] \\[\\beta_0 = \\bar{y} - \\beta_{x}\\bar{x} \\] Tout comme avec l’ANOVA, on peut estimer la variance résiduelle (\\(\\sigma^2\\)) à l’aide du carré moyen des erreurs (\\(MSE\\)), où la variance résiduelle correspond à la partie de la variance des observations de \\(y\\) qui n’est pas expliquée par \\(x\\): \\[MSE = \\frac{SSE}{n - 2}\\] Appliquons maintenant ces calculs à l’exemple du placement de produits par des influenceurs. Nous allons estimer le facteur d’augmentation des ventes à partir des équations vues dans la section précédente. La somme des carrés de \\(Y\\): \\[SSY = \\sum_{i=1}^n (y_i - \\bar{y})^2\\] \\[SSY = (1.4 - 3.4)^2 + \\ldots + (4.5 - 3.4)^2\\] \\[SSY = 13\\] La somme des carrés de \\(X\\): \\[SSX = \\sum_{i=1}^n (x_i - \\bar{x})^2\\] \\[SSX = (3 - 10)^2 + \\ldots + (17 - 10)^2\\] \\[SSX = 262\\] La somme des produits croisés : \\[SSXY = \\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})\\] \\[SSXY = (1.4 - 3.4)(3 - 10) + \\ldots + (4.5 - 3.4)(17 - 10)\\] \\[SSXY = 51.1\\] La somme des carrés de la régression: \\[SS_{\\mathrm{r\\acute{e}gression}} = \\frac{SSXY^2}{SSX}\\] \\[SS_{\\mathrm{r\\acute{e}gression}} = \\frac{51.1^2}{262}\\] \\[SS_{\\mathrm{r\\acute{e}gression}} = 10\\] La somme des carrés des erreurs: \\[SSE = SSY - SS_{r\\acute{e}gression}\\] \\[SSE = 13 - 10\\] \\[SSE = 3\\] Nous pouvons ensuite calculer la pente de la droite de régression: \\[\\beta_x = \\frac{SSXY}{SSX}\\] \\[\\beta_x = \\frac{51.1}{262}\\] \\[\\beta_x = 0.2\\] L’ordonnée à l’origine s’obtient comme suit: \\[\\beta_0 = \\bar{y} - \\beta_x\\bar{x}\\] \\[\\beta_0 = 3.4 - 0.2 * 10\\] \\[\\beta_0 = 1.5\\] Et la variance résiduelle est donnée par: \\[MSE = \\frac{SSE}{n - 2}\\] \\[MSE = \\frac{3.1}{13 - 2}\\] \\[MSE = 0.3\\] Nous pouvons réaliser rapidement tous ces calculs en appliquant les formules dans R ou encore en utilisant directement la fonction lm( ) : ##on crée des objets pour stocker chaque variable nombre &lt;- influenceurs$Nombre facteur &lt;- influenceurs$Facteur ##SSY SSY &lt;- sum((facteur - mean(facteur))^2) SSY ## [1] 13.04769 ##SSX SSX &lt;- sum((nombre - mean(nombre))^2) SSX ## [1] 262 ##SSXY SSXY &lt;- sum((facteur - mean(facteur)) * (nombre - mean(nombre))) SSXY ## [1] 51.1 ##SSReg SSReg &lt;- (SSXY^2)/SSX SSReg ## [1] 9.96645 ##SSE SSE &lt;- SSY - SSReg SSE ## [1] 3.081242 ##pente beta.x &lt;- SSXY/SSX beta.x ## [1] 0.1950382 ##ordonnée à l&#39;origine beta.0 &lt;- mean(facteur) - beta.x * mean(nombre) beta.0 ## [1] 1.480388 ##variance résiduelle MSE &lt;- SSE/(length(facteur) - 2) MSE ## [1] 0.2801129 ##régression linéaire avec lm( ) m1 &lt;- lm(Facteur ~ Nombre, data = influenceurs) m1 ## ## Call: ## lm(formula = Facteur ~ Nombre, data = influenceurs) ## ## Coefficients: ## (Intercept) Nombre ## 1.480 0.195 ##SSE à partir des valeurs prédites ##fitted() utilise l&#39;objet m1 (résultat du modèle) ##et retourne les valeurs prédites pour chaque ##ligne du jeu de données influenceurs SSE &lt;- sum((facteur - fitted(m1))^2) SSE ## [1] 3.081242 En assemblant les coefficients de régression, nous obtenons l’équation de régression linéaire suivante: \\[y_i = \\beta_0 + \\beta_xx_i + \\epsilon_i\\] \\[y_i = 1.5 + 0.2Nombre_i + \\epsilon_i\\] À noter que les valeurs prédites (\\(\\hat{y}_i\\)) sont obtenues avec \\(1.5 + 0.2Nombre_i\\). Par exemple, nous avons observé un facteur d’augmentation des ventes de 1.5 avec 3 influenceurs participant à une campagne publicitaire. Nous pouvons calculer le facteur d’augmentation prédit par la régression (c.-à-d., \\(\\hat{y}_i\\) avec 3 influenceurs) : \\[\\hat{y}_1 = 1.5 + 0.2 \\cdot 3 \\\\ = 2.1 \\] On peut procéder de la même façon pour calculer les autres valeurs prédites. Finalement, on ajoute la droite de régression sur le graphique présentant le facteur d’augmentation des ventes en fonction du nombre d’influenceurs (Figure 10.2a). par(mfrow = c(1, 2), cex = 1.2) ##graphique 1 plot(influenceurs$Facteur ~ influenceurs$Nombre, ylab = &quot;Facteur d&#39;augmentation des ventes&quot;, xlab = &quot;Nombre d&#39;influenceurs&quot;) ##ajout de la droite de régression abline(m1) text(x = 4, y = 5, labels = &quot;a&quot;) ##graphique 2 plot(influenceurs$Facteur ~ influenceurs$Nombre, ylab = &quot;Facteur d&#39;augmentation des ventes&quot;, xlab = &quot;Nombre d&#39;influenceurs&quot;) abline(m1) segments(x0 = nombre, y0 = facteur, x1 = nombre, y1 = fitted(m1), col = &quot;blue&quot;) text(x = 4, y = 5, labels = &quot;b&quot;) Figure 10.2: Droite de régression linéaire du facteur d’augmentation des ventes en fonction du nombre d’influenceurs (a) et distance entre les observations (\\(y_i\\)) et la valeur prédite correspondante (\\(\\hat{y_i}\\)) (b). Puisque les erreurs (\\(\\epsilon_i\\)) correspondent aux résidus, on peut réécrire: \\[y_i = \\hat{y_i} + \\epsilon_i\\] \\[y_i - \\hat{y_i} = \\epsilon_i \\] \\[\\epsilon_i = y_i - \\hat{y_i}\\] La droite de régression minimise la somme des carrés des erreurs qui dépend directement de la distance (verticale) entre chaque \\(y_i\\) et sa valeur prédite \\(\\hat{y_i}\\) correspondante (Figure 10.2b). Dans R, on peut extraire les résidus du modèle de régression à l’aide de la fonction residuals( ) en fournissant comme argument l’objet qui contient le résultat du modèle (c.-à-d., m1 dans notre exemple), alors que la fonction fitted( ) extrait les valeurs prédites du modèle. Nous utiliserons ces deux éléments pour vérifier les suppositions de la régression linéaire. 10.1.1.3 Suppositions La régression linéaire comporte les suppositions suivantes : existence: à chaque valeur de \\(x\\), il existe une distribution de valeurs de \\(y\\) dans la population. Cette supposition ne peut être vérifiée formellement, mais implique qu’il y a une série de valeurs de \\(y\\) possibles à chaque valeur de \\(x\\); normalité: les erreurs proviennent d’une distribution normale. Cette supposition peut être vérifiée à l’aide d’un graphique quantile-quantile à partir des résidus; homoscédasticité: la variance de \\(y\\) (et des erreurs) est la même pour chaque \\(x\\). Le graphique des résidus en fonction des valeurs prédites permet de diagnostiquer des problèmes associés à l’hétérogénéité de la variance; indépendance: es valeurs de \\(y\\) (et les erreurs) sont indépendantes les unes des autres. L’échantillonnage aléatoire des observations assure l’indépendance des observations et des erreurs; linéarité: la relation entre \\(y\\) et \\(x\\) est linéaire. La régression linéaire implique une relation linéaire: un graphique des valeurs observées (\\(y\\)) en fonction de la variable explicative (\\(x\\)) permet de vérifier la plausibilité de cette supposition. Si la relation n’a pas une forme linéaire (une relation exponentielle par exemple), on peut considérer une transformation afin de linéariser la relation34. mesure de \\(x\\) sans erreur: les valeurs de \\(x\\) sont mesurées sans erreur. Un assouplissement de cette supposition consiste à supposer que l’erreur sur \\(x\\) est très faible par rapport à l’erreur de mesure de \\(y\\). La prévention et la rigueur dans la prise de mesure permet de satisfaire à cette condition. Poursuivons le développement de l’exemple sur l’augmentation des ventes suite à une campagne publicitaire avec des influenceurs du web en vérifiant les suppositions pour la régression linéaire. Nous pouvons vérifier l’homogénéité des variances et la normalité à l’aide des mêmes outils que pour l’ANOVA. par(mfrow = c(1, 2), cex = 1.2) ##homogénéité de la variance plot(residuals(m1) ~ fitted(m1), ylab = &quot;Résidus&quot;, xlab = &quot;Valeurs prédites&quot;, main = &quot;Homogénéité des variances&quot;) ##normalité text(&quot;a&quot;, x= 2.3, y = 0.7, cex = 1.2) qqnorm(residuals(m1), ylab = &quot;Quantiles observés&quot;, xlab = &quot;Quantiles théoriques&quot;, main = &quot;Normalité des résidus&quot;) qqline(residuals(m1)) text(&quot;b&quot;, x= -1.4, y = 0.7, cex = 1.2) Figure 10.3: Homogénéité des variances (a) et normalité des résidus (b) obtenus à partir de la droite de régression linéaire du facteur d’augmentation des ventes en fonction du nombre d’influenceurs faisant campagne pour le produit. La figure 10.3a montre que les variances sont homogènes. Comme pour l’ANOVA, la suppositon d’homogénéité des variances doit être respectée. Bien que quelques résidus dévient de la normalité (10.3b), la régression linéaire est appropriée. Nous pouvons procéder à l’interprétation de la régression. 10.1.1.4 Valeurs extrêmes Une observation qui se démarque du reste des observations est une observation extrême (outlier). Cette caractéristique d’une observation peut entraîner des problèmes d’estimation. La présence d’une valeur extrême dans une variable peut fortement influencer la moyenne arithmétique de cette même variable. Par exemple, dans un vecteur avec les 5 valeurs suivantes \\(0.1, 0.1, 0.2, 0.2, 0.3\\), nous obtenons une moyenne arithmétique de 0.18. Si on remplace la valeur de 0.3 par 30, la moyenne deviendra 6.12 et très différente de la moyenne originale. Le même phénomène se produit avec la régression. Figure 10.4: Régression sans valeur extrême (a) où la pente est de 0.195 et (b) régression avec les mêmes données sauf pour une observation extrême identifiée en bleu (\\(x\\) = 1, \\(y\\) = 5) qui modifie la pente à la valeur -0.021. 10.1.1.4.1 Résidus de Student Les résidus de Student (Student residuals) permettent d’identifier des valeurs extrêmes dans une régression. Pour ce faire, chaque observation est comparée aux autres au moyen d’une variable binaire qui prend la valeur de 1 pour coder cette observation et de 0 pour les autres valeurs. Cette nouvelle variable est ajoutée à la régression et le test \\(t\\) associé à ce coefficient donne le résidu de Student pour cette observation. La fonction rstudent( ) extrait directement ces valeurs. Le code ci-dessous montre l’extraction des résidus de Student pour les deux premières observations. ##on crée une variable binaire pour obs1 influenceurs$obs1 &lt;- ifelse(rownames(influenceurs) == &quot;1&quot;, 1, 0) ##on crée une variable binaire pour obs2 influenceurs$obs2 &lt;- ifelse(rownames(influenceurs) == &quot;2&quot;, 1, 0) ##on ajoute cette variable à la régression m1.obs1 &lt;- lm(Facteur ~ Nombre + obs1, data = influenceurs) m1.obs2 &lt;- lm(Facteur ~ Nombre + obs2, data = influenceurs) summary(m1.obs1)$coef ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.7915038 0.3919162 4.571140 0.0010244634 ## Nombre 0.1708815 0.0344507 4.960176 0.0005698837 ## obs1 -0.9041484 0.5804032 -1.557794 0.1503394339 summary(m1.obs2)$coef ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.1535127 0.35523771 3.247157 8.762565e-03 ## Nombre 0.2195067 0.03129716 7.013631 3.655048e-05 ## obs2 1.0684604 0.52727435 2.026384 7.022757e-02 ##extraction des deux premiers résidus de Student du modèle m1 rstudent(m1)[1:2] ## 1 2 ## -1.557794 2.026384 plot(rstudent(m1) ~ fitted(m1), ylab = &quot;Résidus de Student&quot;, xlab = &quot;Valeurs prédites&quot;) Figure 10.5: Résidus de Student en fonction des valeurs prédites de la régression de l’augmentation des ventes en fonction du nombre d’influenceurs du web participant à la campagne publicitaire. Puisque les résidus de Student sont sur l’échelle du \\(t\\) de Student, des valeurs supérieures à 4 ou inférieures à -4 indiquent des valeurs qui se démarquent nettement des autres (voir la leçon 2). On peut visualiser rapidement les résidus de Student en fonction des valeurs prédites (fig. 10.5). 10.1.1.5 Tests d’hypothèses Nous pouvons tester une série d’hypothèses statistiques à partir de la régression linéaire, notamment des tests d’hypothèses sur les coefficients ainsi que sur la régression. La fonction summary( ) permet d’obtenir un bref résumé de l’analyse et le résultat de ces tests d’hypothèses. Nous utilisons cette fonction afin d’aller chercher le résumé de l’analyse réalisée plus tôt sur l’augmentation des ventes suite à du placement de produit fait par des influenceurs sur le web : summary(m1) ## ## Call: ## lm(formula = Facteur ~ Nombre, data = influenceurs) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.91092 -0.25558 -0.03573 0.27915 0.83946 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.4804 0.3584 4.130 0.00167 ** ## Nombre 0.1950 0.0327 5.965 9.39e-05 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.5293 on 11 degrees of freedom ## Multiple R-squared: 0.7638, Adjusted R-squared: 0.7424 ## F-statistic: 35.58 on 1 and 11 DF, p-value: 9.388e-05 Nous remarquons différents éléments dans les résultats de summary( ), incluant l’appel à la fonction (Call:), les quantiles des résidus (Residuals), suivi des coefficients de la régression (Coefficients). Pour chaque coefficient de régression, on retrouve quatre colonnes. La première colonne présente l’estimation (Estimate) et la deuxième donne l’erreur-type de l’estimation (Std. Error). La troisième colonne correspond à un test d’hypothèse sur l’estimation, à savoir, si sa valeur diffère significativement de 0 (\\(H_0: \\beta = 0\\)). Cette hypothèse est testée à l’aide d’un test \\(t\\) de Wald, un test qui est formé de l’estimation divisée par son erreur-type (\\(t \\: \\mathrm{de} \\: \\mathrm{Wald} = \\frac{\\beta}{SE_{\\beta}}\\)). La quatrième colonne correspond à la probabilité cumulative associée à un test \\(t\\) pour l’hypothèse nulle sur l’estimation et aux degrés de liberté résiduels du modèle (\\(P(t_{df} \\geq |t_{obs, df}|)\\)). À noter qu’on teste rarement si l’ordonnée à l’origine diffère de 0 (1ère rangée), puisque l’ordonnée à l’origine diffère la plupart du temps de 0 (2e rangée). C’est pourquoi on se limite généralement à tester la pente. La dernière portion des résultats indique l’erreur-type résiduelle (\\(\\sqrt{MSE}\\)), les degrés de liberté résiduels du modèle de régression, ainsi le coefficient de détermination (\\(R^2\\)) qui est une mesure de la force de la régression que nous verrons dans la prochaine section. La dernière ligne nous présente le \\(F\\) sur l’ANOVA de la régression et la probabilité cumulative de cette statistique. Ce test détermine si globalement la portion de la variance expliquée par la variable explicative est supérieure à la portion inexpliquée (variance résiduelle, \\(MSE\\)). %Ce test amène peu d’information additionnelle pour la régression linéaire simple, puisque le test-\\(t\\) de Wald nous informe rapidement de l’importance du coefficient. C’est avec la régression multiple que ce résultat amène le plus d’information. Une alternative aux tests d’hypothèses classiques s’avère l’usage d’intervalles de confiance afin d’évaluer l’effet de l’estimation de la pente. La construction d’un intervalle de confiance autour d’une estimation est similaire à celle autour d’une moyenne d’un échantillon. On calculera l’intervalle de confiance à \\(1 - \\alpha \\: \\%\\) comme suit : \\[IC \\: \\text{à} \\: 1 - \\alpha \\: \\%: \\\\ P(\\beta_{x} - t_{\\alpha/2, \\: \\mathrm{degr\\acute{e}s \\: de \\: libert\\acute{e} \\: r\\acute{e}siduels}} \\cdot SE_{\\beta_x} \\leq \\mu \\leq \\beta_{x} + t_{\\alpha/2, \\: \\mathrm{degr\\acute{e}s \\: de \\: libert\\acute{e} \\: r\\acute{e}siduels}} \\cdot SE_{\\beta_x}) = (1 - \\alpha)\\] où \\(\\beta_x\\) représente l’estimation de la pente, \\(SE_{\\beta_x}\\) donne la précision de cette estimation et \\(t_{\\alpha/2, \\: \\mathrm{degr\\acute{e}s \\: de \\: libert\\acute{e} \\: r\\acute{e}siduels}}\\) correspond à la statistique du \\(t\\) de Student associée à la portion \\(\\alpha/2\\) % de la courbe et aux degrés de libertés résiduels. Ainsi, si l’intervalle de confiance exclut 0, nous déclarerons que l’estimation du coefficient diffère de 0. En poursuivant l’exemple de régression linéaire l’augmentation des ventes de produits avec une campagne publicitaire d’influenceurs du web, nous obtenons un intervalle de confiance à 95 % autour de la pente de \\([-0.264, -1.486]\\) et nous concluons que l’estimation diffère de 0. ##extraction de pente pente &lt;- coef(m1)[2] ##extraction de SE de la pente ##à partir du summary() du modèle m1 SE.pente &lt;- summary(m1)$coefficient[2,2] ##df résiduels df.res &lt;- m1$df.residual ##limites de IC inf95 &lt;- pente + qt(p = 0.025, df = df.res) * SE.pente sup95 &lt;- pente - qt(p = 0.025, df = df.res) * SE.pente ##IC c(inf95, sup95) ## Nombre Nombre ## 0.1230712 0.2670051 10.1.1.6 Évaluer le pouvoir prédictif Le coefficient de détermination ou \\(\\boldsymbol{R^2}\\) est une mesure de la force de la régression. Elle indique si les observations sont près de la droite de régression. Le coefficient de détermination peut prendre des valeurs entre 0 (la régression n’explique aucune partie de la variance de \\(y\\)) et 1 (la régression explique complètement la variance de \\(y\\)). En multipliant le coefficient de détermination par 100, on peut l’interpréter comme un pourcentage de variance de la variable réponse (\\(y\\)) expliquée par la variable explicative (\\(x\\)). Le coefficient de détermination peut se calculer: \\[ R^2 = \\frac{SSReg}{SSY}\\] où \\(SSReg\\) correspond à la somme des carrés de la régression et \\(SSY\\) correspond à la somme des carrés totale de \\(y\\). Le \\(R^2\\) donne la fraction de la variance totale de \\(y\\) expliquée par la régression. Pour l’exemple sur l’augmentation des ventes suite à une campagne publicitaire, nous obtenons \\(R^2 = \\frac{9.97}{13.05} = 0.76\\). Nous pourrons conclure que 76 % de la variabilité de \\(y\\) est expliquée par la régression, ce qui est très élevé. Avec certains jeux de données en sciences, nous nous réjouissons souvent d’avoir un \\(R^2\\) de 0.30. Bien que les tests d’hypothèses puissent être informatifs, il faut être conscient que dans certains cas, le rejet d’une hypothèse nulle n’entraîne pas nécessairement un résultat significatif du point de vue pratique. Avec un nombre d’observations suffisamment élevé, on peut rejeter pratiquement n’importe quelle hypothèse nulle statistique. C’est pourquoi l’utilisation de mesures de précision autour de la pente, telles que l’erreur-type ou l’intervalle de confiance, permettent de nous aider à évaluer la relation. Le coefficient de détermination aide à conclure sur la force de la relation et de la distance des points par rapport à la droite de régression – même avec un \\(P &lt; 0.0001\\), il est possible d’avoir un \\(R^2\\) très près de 0. Le cas échéant, nous modulerons notre interprétation des résultats. 10.1.1.7 Prédiction Un des objectifs de la régression linéaire est souvent de faire des prédictions. Pour ce faire, nous pouvons utiliser l’équation de régression et substituer la valeur de \\(x\\) pour laquelle nous désirons une valeur prédite. Dans l’exemple sur l’augmentation des ventes en fonction du nombre d’influenceurs, l’équation de la droite de régression est : \\[\\hat{y_i} = \\beta_0 + \\beta_xx_i \\hat{y_i} = 1.5 + 0.2 \\cdot Nombre_i\\] Il est possible de faire des prédictions à l’intérieur de l’étendue des valeurs de la variable explicative. Ici, le nombre d’influenceurs promouvant un même produit varie de 3 à 17. En d’autres mots, on ne peut pas prédire le facteur d’augmentation des ventes pour un nombre d’influenceurs du web moindre que 3 ou plus grand que 17. Si nous voulons obtenir le facteur d’augmentation des ventes d’un campagne ayant 13 influenceurs du web y participant (notez que 13 fait partie de l’intervalle, mais qu’aucune observation n’avait été mesurée avec ce nombre d’influenceurs dans le jeu de données), nous n’avons qu’à substituer cette valeur dans l’équation qui donnera: \\[\\hat{y_i} = 1.5 + 0.2 \\cdot 13 \\hat{y_i} = 4\\] Nous conclurons qu’une campagne publicitaire utilisant 13 influenceurs du web aura un facteur d’augmentation des ventes de 4. Cette réponse est seulement partielle pour l’instant. Bien que nous ayons une valeur prédite, cette valeur est difficile à interpréter sans mesure de précision. N’importe quel modèle peut nous produire une valeur prédite, mais les bons modèles fourniront des prédictions avec une bonne précision. On peut calculer l’erreur-type de la prédiction avec l’algèbre matricielle, mais la fonction dans permet d’extraire ces valeurs facilement. Par la suite, on peut calculer un intervalle de confiance autour de la valeur prédite selon la méthode habituelle: \\[IC \\: \\text{à} \\: (1 - \\alpha) \\: \\%: \\\\ P(\\hat{y_i} - t_{\\alpha/2, \\: \\mathrm{degr\\acute{e}s \\: de \\: libert\\acute{e} \\: r\\acute{e}siduels}} \\cdot SE_{\\hat{y_i}} \\leq \\mu \\leq \\hat{y_i} + t_{\\alpha/2, \\: \\mathrm{degr\\acute{e}s \\: de \\: libert\\acute{e} \\: r\\acute{e}siduels}} \\cdot SE_{\\hat{y_i}} = (1 - \\alpha)\\] où \\(\\hat{y_i}\\) représente l’estimation de la valeur prédite par l’équation de régression, \\(SE_{\\hat{y_i}}\\) donne la précision de cette estimation et \\(t_{\\alpha/2, \\: \\mathrm{degr\\acute{e}s \\: de \\: libert\\acute{e} \\: r\\acute{e}siduels}}\\) correspond à la statistique du \\(t\\) de Student associée à la portion \\((\\alpha * 100)/2 \\: \\%\\) de la courbe et degrés de libertés résiduels. Afin d’utiliser predict( ), il faut fournir à l’argument newdata un jeu de données à partir duquel on fera les prédictions : ##jeu de données à partir duquel on fait des prédictions jeu.pred &lt;- data.frame(Nombre = 13) ##on effectue la prédiction avec SE pred &lt;- predict(m1, newdata = jeu.pred, se.fit = TRUE) ##on calcule IC à 95 % inf95 &lt;- pred$fit + qt(p = 0.025, df = m1$df.residual) * pred$se.fit sup95 &lt;- pred$fit - qt(p = 0.025, df = m1$df.residual) * pred$se.fit c(inf95, sup95) ## 1 1 ## 3.627303 4.404464 Nous concluons qu’une campagne publicitaire engageant 13 influenceurs du web aura un facteur d’augmentation des ventes de 4 avec un intervalle de confiance à 95 %: (\\(3.6, 4.4\\)). On peut utiliser la même stratégie afin d’ajouter des intervalles de confiance autour de la droite de régression (Figure 10.6). par(cex = 1.2) ##jeu de données à partir duquel on fait des prédictions jeu.pred &lt;- data.frame(Nombre = seq(from = min(influenceurs$Nombre), to = max(influenceurs$Nombre), by = 1)) ##on effectue la prédiction avec SE pred &lt;- predict(m1, newdata = jeu.pred, se.fit = TRUE) ##ajout à jeu.pred jeu.pred$fit &lt;- pred$fit jeu.pred$se.fit &lt;- pred$se.fit ##on calcule IC à 95 % jeu.pred$inf95 &lt;- jeu.pred$fit + qt(p = 0.025, df = m1$df.residual) * jeu.pred$se.fit jeu.pred$sup95 &lt;- jeu.pred$fit - qt(p = 0.025, df = m1$df.residual) * jeu.pred$se.fit ##graphique avec points originaux plot(influenceurs$Facteur ~ influenceurs$Nombre, ylab = &quot;Facteur d&#39;augmentation des ventes&quot;, xlab = &quot;Nombre d&#39;influenceurs&quot;, ylim = c(min(jeu.pred$inf95), max(jeu.pred$sup95)), col = &quot;blue&quot;) ##ajoute droite lines(y = jeu.pred$fit, x = jeu.pred$Nombre) ##ajoute limites de confiance lines(y = jeu.pred$inf95, x = jeu.pred$Nombre, lty = &quot;dotted&quot;) lines(y = jeu.pred$sup95, x = jeu.pred$Nombre, lty = &quot;dotted&quot;) Figure 10.6: Droite de régression avec limites de confiance à 95 % en pointillés et valeurs observées. Dans le reste du document, nous développons un autre exemple complet de régression linéaire afin d’appliquer les notions que nous avons apprises. Nous voulons déterminer si le prix de tablettes de chocolat (en dollars australiens) dépend de la masse en grammes de la tablette de chocolat. Une série de différentes tablettes de chocolat, toutes de différentes marques, ont été sélectionnées. Le fichier chocolats.txt contient ces données. ##importation chocs &lt;- read.table(&quot;Module_10/data/chocolats.txt&quot;, header = TRUE) head(chocs) ## Masse Prix ## Dark.Bounty 50 0.88 ## Bounty 50 0.88 ## Milo.Bar 40 1.15 ## Viking 80 1.54 ## KitKat.White 45 1.15 ## KitKat.Chunky 78 1.40 ##graphique des données par(cex = 1.2) plot(chocs$Prix ~ chocs$Masse, ylab = &quot;Prix (dollars australiens)&quot;, xlab = &quot;Masse (g)&quot;) Figure 10.7: Graphique du prix de tablettes de chocolat australiennes en fonction de leur masse. Nous voyons qu’une relation linéaire entre ces deux variables est plausible (Figure 10.7). Nous pouvons essayer d’ajuster la régression linéaire aux données. m.chocs &lt;- lm(Prix ~ Masse, data = chocs) ##vérifications des suppositions par(mfrow = c(1, 2), cex = 1.2) plot(rstudent(m.chocs) ~ fitted(m.chocs), ylab = &quot;Résidus de Student&quot;, xlab = &quot;Valeurs prédites&quot;, main = &quot;Homogénéité des variances&quot;) text(y = 1.5, x = 1.025, labels = &quot;a&quot;) qqnorm(rstudent(m.chocs), ylab = &quot;Quantiles observés&quot;, xlab = &quot;Quantiles théoriques&quot;, main = &quot;Normalité des résidus&quot;) qqline(rstudent(m.chocs)) text(y = 1.5, x = -1.9, labels = &quot;b&quot;) Figure 10.8: Diagnostics des suppositions d’homogénéité des variances (a) et de normalité des résidus (b). Les graphiques diagnostiques montrent que les variances sont sensiblement homogènes et que les résidus suivent approximativement une distribution normale (Figure 10.8). Puisque nous avons utilisé les résidus de Student, nous avons pu confirmer du même coup l’absence de valeurs extrêmes dans la figure 10.8a). La fonction summary( ) indique : summary(m.chocs) ## ## Call: ## lm(formula = Prix ~ Masse, data = chocs) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.28031 -0.14368 0.07184 0.12546 0.29969 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.574368 0.213738 2.687 0.01769 * ## Masse 0.011266 0.003768 2.989 0.00975 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.1891 on 14 degrees of freedom ## Multiple R-squared: 0.3896, Adjusted R-squared: 0.346 ## F-statistic: 8.937 on 1 and 14 DF, p-value: 0.009753 Nous concluons que la masse augmente significativement le prix de tablettes de chocolat en Australie avec une pente de 0.011 et un intervalle de confiance à 95 % de \\((0.003, 0.019)\\). La masse explique 39 % de la variance du prix, ce qui est satisfaisant avec ce type de données. Dans R, nous pouvons déterminer ces quantités de la façon suivante : ##extraction de la pente et du degré de liberté ##des résidus à partir de l&#39;objet m.chocs pente &lt;- m.chocs$coefficients[2] df.res &lt;- m.chocs$df.residual ##extraction du SE de la pente à partir de l&#39;objet ##summary(m.chocs) SE.pente &lt;- summary(m.chocs)$coefficient[2,2] ##Calcul de l&#39;IC de la pente à 95% inf95 &lt;- pente + qt(p = 0.025, df = df.res) * SE.pente sup95 &lt;- pente - qt(p = 0.025, df = df.res) * SE.pente c(inf95, sup95) ## Masse Masse ## 0.003183133 0.019348192 ##Calcul du R carré summary(m.chocs)$r.squared ## [1] 0.3896299 Nous pouvons présenter la droite de régression avec un intervalle de confiance à 95 % (Figure 10.9). ##jeu de données à partir duquel on fait des prédictions jeu.pred &lt;- data.frame(Masse = seq(from = min(chocs$Masse), to = max(chocs$Masse), by = 1)) ##on effectue la prédiction avec SE pred &lt;- predict(m.chocs, newdata = jeu.pred, se.fit = TRUE) ##ajout à jeu.pred jeu.pred$fit &lt;- pred$fit jeu.pred$se.fit &lt;- pred$se.fit ##on calcule IC à 95 % jeu.pred$inf95 &lt;- jeu.pred$fit + qt(p = 0.025, df = m.chocs$df.residual) * jeu.pred$se.fit jeu.pred$sup95 &lt;- jeu.pred$fit - qt(p = 0.025, df = m.chocs$df.residual) * jeu.pred$se.fit ##graphique avec points originaux par(cex = 1.2) plot(chocs$Prix ~ chocs$Masse, ylab = &quot;Prix (dollars australiens)&quot;, xlab = &quot;Masse (g)&quot;, col = &quot;blue&quot;, ylim = c(min(jeu.pred$inf95), max(jeu.pred$sup95))) ##on ajoute la droite lines(y = jeu.pred$fit, x = jeu.pred$Masse) ##on ajoute les limites de confiance lines(y = jeu.pred$inf95, x = jeu.pred$Masse, lty = &quot;dotted&quot;) lines(y = jeu.pred$sup95, x = jeu.pred$Masse, lty = &quot;dotted&quot;) Figure 10.9: Droite de régression et limites de confiance à 95 % (pointillés) à partir de l’analyse du prix en fonction de la masse de tablettes de chocolat australiennes et valeurs observées. 10.1.2 Corrélation La corrélation est une relation d’association entre deux variables. Contrairement à la régression, il n’y a aucune allusion à une relation de cause à effet avec la corrélation. En d’autres mots, on ne distingue pas entre variable dépendante (\\(y\\)) et indépendante (\\(x\\)) – les deux variables varient en même temps et une troisième variable peut potentiellement agir sur les deux. Pour distinguer entre la régression et la corrélation, considérons un exemple où on veut expliquer le nombre moyen de verres cassés par jour dans des restaurants d’une ville selon le nombre de clients. Si le nombre de clients influence le nombre de verres cassés, nous aurons un cas de régression linéaire. À l’opposé, si on mesure le nombre moyen de verres cassés par jour dans des restaurants d’une ville et la quantité de déchets dans les rues autour de ces restaurants, la relation entre les deux variables serait plutôt une corrélation, et cette corrélation pourrait être expliquée par une troisième variable qui influencent les deux premières variables, c.-à-d. la densité de population ou le nombre de clients. 10.1.2.1 Corrélation de Pearson Le coefficient de corrélation de Pearson (Pearson’s product-moment correlation coefficient) nous donne la corrélation entre deux variables35: \\[r = \\frac{SSXY}{\\sqrt{SSX \\cdot SSY}}\\] où \\(SSXY\\), \\(SSX\\) et \\(SSY\\) sont les mêmes termes que l’on calcule dans la régression. La figure 10.10 présente différents degrés de corrélation positive, alors que la figure 10.11 illustre des corrélations négatives. Figure 10.10: Différents degrés de corrélation positive (\\(r &gt; 0\\)) entre deux variables. Figure 10.11: Différents degrés de corrélation négative (\\(r &lt; 0\\)) entre deux variables. Dans R, on peut obtenir la corrélation de Pearson entre deux variables à l’aide de la fonction cor( ). Nous pouvons aussi effectuer un test d’hypothèse sur le coefficient de corrélation, à savoir s’il diffère de 0 (\\(H_0: \\rho = 0\\)) avec la fonction cor.test( ). Néanmoins, il faut demeurer vigilant car une valeur de \\(P\\) faible n’est pas une garantie que la corrélation est forte. Une meilleure mesure s’avère le \\(r^2\\) (c.-à-d., le coefficient de Pearson au carré) et est en fait le coefficient de détermination. Dans le cas d’une corrélation, il indique le pourcentage de la variation d’une variable qui est associée à la deuxième variable. Dans ce petit exemple, nous montrons le coefficient de corrélation entre deux variables aléatoires ainsi que le test d’hypothèse sur ce coefficient. ##on crée deux variables aléatoires set.seed(seed = 10) x1 &lt;- rnorm(1000) set.seed(seed = 11) x2 &lt;- rnorm(1000) ##corrélation cor(x1, x2) ## [1] -0.0710729 ##test d&#39;hypothèse cor.test(x1, x2) ## ## Pearson&#39;s product-moment correlation ## ## data: x1 and x2 ## t = -2.251, df = 998, p-value = 0.0246 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## -0.132482263 -0.009120001 ## sample estimates: ## cor ## -0.0710729 ##r^2 cor(x1, x2)^2 ## [1] 0.005051357 Chaque variable comporte 1000 valeurs et la corrélation de Pearson donne -0.071. Malgré cette faible corrélation, le test d’hypothèse rejette l’hypothèse nulle principalement en raison de l’énorme taille d’échantillon. Le \\(r^2\\) est aussi très faible (0.005), indiquant que seulement 0.5% de la variabilité de \\(x1\\) est associée à \\(x2\\). Nous conclurons que la corrélation est très faible et qu’à toute fin pratique, il n’y a pas d’association entre les deux variables (pas d’effet pratique). 10.1.3 Corrélation vs régression La régression est souvent utilisée pour des études d’observation. Dans de telles conditions, une relation de cause à effet peut difficilement être établie sans avoir contrôlé les autres variables ou avoir mesuré toutes les variables pertinentes. Il faut faire attention à l’interprétation et les résultats peuvent suggérer le besoin de réaliser des expériences plus formelles. Les mêmes conseils de plan d’échantillonnage de l’ANOVA s’appliquent à la régression (e.g., randomisation, sélection aléatoire des unités expérimentales, répartition spatiale). La corrélation est surtout utilisée pour déterminer l’association entre deux variables sans être une relation de cause à effet. La corrélation est utile surtout dans la phase de préanalyse pour déterminer les relations entre différentes variables explicatives. 10.1.4 Conclusion Cette leçon a présenté la régression linéaire simple, les suppositions et conditions nécessaires à son application, l’interprétation et la présentation des résultats. Nous avons vu les tests d’hypothèses associés aux coefficients de la régression ainsi que l’utilisation d’intervalles de confiance comme mesure de précision autour des coefficients et des valeurs prédites. Nous avons également présenté la corrélation linéaire et nous l’avons comparé à la régression linéaire simple. Rappel: la somme des carrés des erreurs s’obtient avec l’équation \\(SSE = \\sum_{i=1}^n (y_i - \\hat{y}_i)^2\\), où \\(y_i\\) correspond aux observations de la variable réponse et \\(\\hat{y}_i\\) représente les valeurs prédites par l’équation de régression.↩︎ Dans le cas de certaines relations comme une relation exponentielle, transformer la variable réponse à l’échelle log peut linéariser la relation. Si le patron est plutôt quadratique (p. ex., une courbe avec un optimum), on peut aussi considérer un terme quadratique dans l’équation, c’est-à-dire, utiliser une régression polynomiale avec variable \\(x + x^2\\)↩︎ Il existe aussi des analogues non-paramétriques de la corrélation, tels que la corrélation de Spearman ou de Kendall.↩︎ "],["exercices-7.html", "10.2 Exercices", " 10.2 Exercices 10.2.1 Question 1 Importez le fichier Pression_sanguine.txt qui présente les données relatives à la pression sanguine en mm de Hg chez des patients de différents âges. Réponse Nous importons le jeu de données : sang &lt;- read.table(file = &quot;Module_10/data/Pression_sanguine.txt&quot;, header = TRUE) head(sang) ## Age Pression ## 1 30 108 ## 2 30 110 ## 3 30 106 ## 4 40 125 ## 5 40 120 ## 6 40 118 str(sang) ## &#39;data.frame&#39;: 20 obs. of 2 variables: ## $ Age : int 30 30 30 40 40 40 40 50 50 50 ... ## $ Pression: int 108 110 106 125 120 118 119 132 137 134 ... a. Effectuez une régression linéaire simple afin de déterminer l’effet de l’âge sur la pression sanguine. Réponse m.sang &lt;- lm(Pression ~ Age, data = sang) summary(m.sang) ## ## Call: ## lm(formula = Pression ~ Age, data = sang) ## ## Residuals: ## Min 1Q Median 3Q Max ## -4.0050 -1.9186 -0.4421 2.0264 4.0893 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 68.78491 2.21607 31.04 &lt;2e-16 *** ## Age 1.30314 0.04077 31.97 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.57 on 18 degrees of freedom ## Multiple R-squared: 0.9827, Adjusted R-squared: 0.9817 ## F-statistic: 1022 on 1 and 18 DF, p-value: &lt; 2.2e-16 b. Vérifiez les suppositions de la régression linéaire simple. Apportez des transformations si nécessaire. La régression linéaire est-elle justifiée ici? Réponse Les suppositions de linéarité, d’homogénéité des variances et de normalité des résidus sont respectées (Figure 10.12). La régression est donc appropriée avec ces données. par(mfrow = c(2, 2)) ##linéarité plot(sang$Pression ~ sang$Age, main = &quot;Linéarité&quot;) ##homogénéité des variances plot(rstudent(m.sang) ~ fitted(m.sang), main = &quot;Homoscédasticité&quot;) ##normalité des résidus qqnorm(rstudent(m.sang), main = &quot;Normalité des résidus&quot;) qqline(rstudent(m.sang)) Figure 10.12: Diagnostics de la régression linéaire effectuée sur les données de pression sanguine en fonction de l’âge de patients. c. Interprétez les résultats et commentez la valeur des coefficients ainsi que le pouvoir prédictif de la régression. Réponse summary(m.sang) ## ## Call: ## lm(formula = Pression ~ Age, data = sang) ## ## Residuals: ## Min 1Q Median 3Q Max ## -4.0050 -1.9186 -0.4421 2.0264 4.0893 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 68.78491 2.21607 31.04 &lt;2e-16 *** ## Age 1.30314 0.04077 31.97 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.57 on 18 degrees of freedom ## Multiple R-squared: 0.9827, Adjusted R-squared: 0.9817 ## F-statistic: 1022 on 1 and 18 DF, p-value: &lt; 2.2e-16 Nous remarquons que l’effet de l’âge est très positif avec une pente de 1.3 et une excellente précision (erreur-type très faible relative à la valeur du coefficient de la pente: 0.04). De plus, le \\(R^2\\) indique que l’âge explique 98.3 % de la variabilité de la pression sanguine. La régression décrit très bien les données. d. Utilisez l’équation de régression pour prédire la pression sanguine d’un patient de 55 ans. Construisez un intervalle de confiance autour de la prédiction. Réponse L’équation de régression est : \\[ \\hat{y}_i = 68.78 + 1.3 \\cdot Age_i \\\\ \\hat{y}_i = 68.78 + 1.3 \\cdot 55 \\\\ \\hat{y}_i = 140.46 \\] Nous pouvons calculer la valeur prédite et un intervalle de confiance autour de cette valeur : ##jeu de données à partir duquel on fait des prédictions jeu.pred &lt;- data.frame(Age = 55) ##on effectue la prédiction avec SE pred &lt;- predict(m.sang, newdata = jeu.pred, se.fit = TRUE) ##ajout à jeu.pred jeu.pred$fit &lt;- pred$fit jeu.pred$se.fit &lt;- pred$se.fit ##on calcule IC à 95% jeu.pred$inf95 &lt;- jeu.pred$fit + qt(p = 0.025, df = m.sang$df.residual) * jeu.pred$se.fit jeu.pred$sup95 &lt;- jeu.pred$fit - qt(p = 0.025, df = m.sang$df.residual) * jeu.pred$se.fit jeu.pred ## Age fit se.fit inf95 sup95 ## 1 55 140.4579 0.5836904 139.2316 141.6841 Nous concluons qu’un patient âgé de 55 ans aura une pression de 140.5 mm de Hg avec un intervalle de confiance à 95 %: (139.2, 141.7). e. Présentez la droite de régression sous forme graphique. Ajoutez les limites de confiance autour de la droite. Réponse Calculons les valeurs prédites et leurs intervalles de confiance respectifs pour chacune des valeurs observées d’âge : ##jeu de données à partir duquel on fait des prédictions jeu.pred &lt;- data.frame(Age = seq(from = min(sang$Age), to = max(sang$Age), by = 1)) ##on effectue la prédiction avec SE pred &lt;- predict(m.sang, newdata = jeu.pred, se.fit = TRUE) ##ajout à jeu.pred jeu.pred$fit &lt;- pred$fit jeu.pred$se.fit &lt;- pred$se.fit ##on calcule IC à 95% jeu.pred$inf95 &lt;- jeu.pred$fit + qt(p = 0.025, df = m.sang$df.residual) * jeu.pred$se.fit jeu.pred$sup95 &lt;- jeu.pred$fit - qt(p = 0.025, df = m.sang$df.residual) * jeu.pred$se.fit La figure 10.13 illustre la droite de prédiction ainsi que les intervalles de confiance autour des valeurs prédites. Nous remarquons l’excellente précision dans les prédictions à partir de la régression, tel qu’indiqué par les intervalles de confiance très étroits autour des valeurs prédites. ##graphique vide par(cex = 1.2) plot(jeu.pred$fit ~ jeu.pred$Age, type = &quot;n&quot;, ylab = &quot;Pression sanguine (mm de Hg)&quot;, xlab = &quot;Âge (années)&quot;, ylim = c(min(jeu.pred$inf95), max(jeu.pred$sup95))) ##ajoute droite lines(y = jeu.pred$fit, x = jeu.pred$Age) ##ajoute limites de confiance lines(y = jeu.pred$inf95, x = jeu.pred$Age, lty = &quot;dotted&quot;) lines(y = jeu.pred$sup95, x = jeu.pred$Age, lty = &quot;dotted&quot;) Figure 10.13: Pression sanguine en fonction de l’âge des patients. Les pointillés représentent les limites de confiance à 95 % autour des valeurs prédites obtenues à partir de la régression linéaire. 10.2.2 Question 2 Dans une étude d’observation, on s’intéresse au aux taux annuels de noyade dans les grandes villes nord-américaines. Pour ce faire, nous sélectionnons aléatoirement 50 villes dans notre aire d’étude et récoltons les données auprès des autorités de ces villes. Lors de cette même étude d’observation, nous avons également récolté les taux d’homicide dans ces différentes villes. a. Quelle analyse sera la plus approriée pour décrire la relation entre le taux de noyade et le taux d’homicide dans les grande villes nord-américaines? Justifiez votre réponse. Réponse Une corrélation serait plus appropriée pour décrire la relation entre les deux variables. On ne s’attend pas à une relation de cause à effet entre le taux de noyade et le taux d’homicide. Il est peu probable que le taux d’homicide influence directement le taux de noyade dans les grandes villes. Même si on trouve une corrélation entre ces deux variables, il est possible qu’une troisième variable vienne influencer les taux de noyade et d’homicide. Par exemple, la température pourrait augmenter le nombre de baigneurs ainsi que l’occurrence de crimes violents36, affectant ainsi nos deux variables à l’étude. Tiihonen, J., Halonen, P., Tiihonen, L., Kautiainen, H., Storvik, M., et Callaway, J.(2017). The Association of Ambient Temperature and Violent Crime. Scientific Reports, 7(1), 6543.↩︎ "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
